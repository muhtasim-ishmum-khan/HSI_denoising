{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa40afb-7e89-4ee0-af31-4788e71285ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trial 13\n",
    "# fixes applied\n",
    "# Random val split\n",
    "# increases training and val pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0236464-7454-4cf4-b71c-7f52f3535a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SOTA-Enhanced Memory-Optimized HSI Denoising ===\n",
      "4-stage SST Transformer with hybrid attention and deep supervision\n",
      "Target: PSNR > 40 dB with efficient memory usage\n",
      "Device: cuda\n",
      "GPU Memory: 12.5 GB\n",
      "\n",
      "=== DATA DISCOVERY ===\n",
      "Looking for HSI data in: /home/habib/Documents/workspace/icvl_part/train\n",
      "Found 100 .mat files in directory\n",
      "ICVL Split: 90 training files, 10 validation files\n",
      "\n",
      "=== Initializing TRAINING Dataset ===\n",
      "Noise Level (σ): 10 -> 0.0392 (0-1 scale)\n",
      "Center crop size: 1024x1024\n",
      "Patch size: 64x64\n",
      "Multi-scale strides: [64, 32, 32]\n",
      "Attempting to load 90 files...\n",
      "  [1/90] Loading: prk_0328-1034.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [2/90] Loading: lst_0408-1012.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [3/90] Loading: eve_0331-1601.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [4/90] Loading: mor_0328-1209-2.mat\n",
      "    ✓ Shape: (31, 1392, 1027), Key: 'rad' (HDF5)\n",
      "  [5/90] Loading: bgu_0403-1439.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [6/90] Loading: peppers_0503-1330.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [7/90] Loading: prk_0328-1025.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [8/90] Loading: eve_0331-1633.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [9/90] Loading: ramot_0325-1322.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [10/90] Loading: hill_0325-1242.mat\n",
      "    ✓ Shape: (31, 1392, 1191), Key: 'rad' (HDF5)\n",
      "  [11/90] Loading: omer_0331-1055.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [12/90] Loading: eve_0331-1656.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [13/90] Loading: omer_0331-1119.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [14/90] Loading: omer_0331-1150.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [15/90] Loading: bguCAMP_0514-1718.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [16/90] Loading: plt_0411-1046.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [17/90] Loading: omer_0331-1130.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [18/90] Loading: sami_0331-1019.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [19/90] Loading: maz_0326-1048.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [20/90] Loading: plt_0411-1200-1.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [21/90] Loading: eve_0331-1705.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [22/90] Loading: omer_0331-1118.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [23/90] Loading: bguCAMP_0514-1659.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [24/90] Loading: bguCAMP_0514-1712.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [25/90] Loading: Master20150112_f2_colorchecker.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [26/90] Loading: BGU_0522-1211.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [27/90] Loading: bguCAMP_0514-1711.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [28/90] Loading: rsh2_0406-1505.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [29/90] Loading: omer_0331-1102.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [30/90] Loading: plt_0411-1116.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [31/90] Loading: pepper_0503-1229.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [32/90] Loading: sat_0406-1157-1.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [33/90] Loading: 4cam_0411-1640-1.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [34/90] Loading: prk_0328-1045.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [35/90] Loading: bgu_0403-1511.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [36/90] Loading: BGU_0403-1419-1.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [37/90] Loading: BGU_0522-1201.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [38/90] Loading: eve_0331-1606.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [39/90] Loading: plt_0411-1207.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [40/90] Loading: Ramot0325-1364.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [41/90] Loading: rsh_0406-1356.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [42/90] Loading: BGU_0522-1217.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [43/90] Loading: bguCAMP_0514-1724.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [44/90] Loading: plt_0411-1037.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [45/90] Loading: lst_0408-1004.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [46/90] Loading: BGU_0522-1127.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [47/90] Loading: sat_0406-1129.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [48/90] Loading: omer_0331-1159.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [49/90] Loading: pepper_0503-1236.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [50/90] Loading: peppers_0503-1308.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [51/90] Loading: eve_0331-1549.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [52/90] Loading: pepper_0503-1228.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [53/90] Loading: Flower_0325-1336.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [54/90] Loading: hill_0325-1235.mat\n",
      "    ✓ Shape: (31, 1392, 1194), Key: 'rad' (HDF5)\n",
      "  [55/90] Loading: BGU_0522-1136.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [56/90] Loading: eve_0331-1632.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [57/90] Loading: eve_0331-1657.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [58/90] Loading: bguCAMP_0514-1723.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [59/90] Loading: rsh_0406-1441-1.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [60/90] Loading: eve_0331-1702.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [61/90] Loading: hill_0325-1219.mat\n",
      "    ✓ Shape: (31, 1392, 1202), Key: 'rad' (HDF5)\n",
      "  [62/90] Loading: rsh_0406-1413.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [63/90] Loading: grf_0328-0949.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [64/90] Loading: strt_0331-1027.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [65/90] Loading: hill_0325-1228.mat\n",
      "    ✓ Shape: (31, 1392, 1196), Key: 'rad' (HDF5)\n",
      "  [66/90] Loading: nachal_0823-1110.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [67/90] Loading: 4cam_0411-1648.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [68/90] Loading: peppers_0503-1315.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [69/90] Loading: eve_0331-1551.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [70/90] Loading: Lehavim_0910-1630.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [71/90] Loading: bgu_0403-1523.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [72/90] Loading: prk_0328-0945.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [73/90] Loading: BGU_0522-1216.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [74/90] Loading: plt_0411-1211.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [75/90] Loading: plt_0411-1210.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [76/90] Loading: eve_0331-1646.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [77/90] Loading: sat_0406-1130.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [78/90] Loading: peppers_0503-1332.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [79/90] Loading: bgu_0403-1525.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [80/90] Loading: prk_0328-1031.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [81/90] Loading: BGU_0522-1203.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [82/90] Loading: sat_0406-1107.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [83/90] Loading: eve_0331-1647.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [84/90] Loading: peppers_0503-1311.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [85/90] Loading: bgu_0403-1444.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [86/90] Loading: prk_0328-1037.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [87/90] Loading: plt_0411-1232-1.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [88/90] Loading: omer_0331-1135.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [89/90] Loading: lst_0408-0950.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [90/90] Loading: rsh_0406-1343.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "\n",
      "TRAINING Dataset Summary:\n",
      "  ✓ Successfully loaded: 90 files\n",
      "  → Patches per file: 15\n",
      "  → Total iterations per epoch: 1350\n",
      "\n",
      "=== Initializing VALIDATION_SYNTHETIC Dataset ===\n",
      "Noise Level (σ): 10 -> 0.0392 (0-1 scale)\n",
      "Center crop size: 1024x1024\n",
      "Patch size: 64x64\n",
      "Multi-scale strides: [64]\n",
      "Attempting to load 10 files...\n",
      "  [1/10] Loading: rsh_0406-1443.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [2/10] Loading: rsh_0406-1427.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [3/10] Loading: eve_0331-1602.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [4/10] Loading: plt_0411-1155.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [5/10] Loading: omer_0331-1131.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [6/10] Loading: Maz0326-1038.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [7/10] Loading: omer_0331-1104.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [8/10] Loading: eve_0331-1618.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [9/10] Loading: bgu_0403-1459.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "  [10/10] Loading: BGU_0522-1113-1.mat\n",
      "    ✓ Shape: (31, 1392, 1300), Key: 'rad' (HDF5)\n",
      "\n",
      "VALIDATION_SYNTHETIC Dataset Summary:\n",
      "  ✓ Successfully loaded: 10 files\n",
      "  → Patches per file: 10\n",
      "  → Total iterations per epoch: 100\n",
      "\n",
      "=== PRE-TRAINING DATA VERIFICATION ===\n",
      "Testing first few training samples...\n",
      "Sample 1: Noisy shape: torch.Size([2, 31, 64, 64]), Clean shape: torch.Size([2, 31, 64, 64])\n",
      "Sample 2: Noisy shape: torch.Size([2, 31, 64, 64]), Clean shape: torch.Size([2, 31, 64, 64])\n",
      "Sample 3: Noisy shape: torch.Size([2, 31, 64, 64]), Clean shape: torch.Size([2, 31, 64, 64])\n",
      "✓ Successfully verified 6 samples from 90 real data files\n",
      "==================================================\n",
      "Model parameters: 53.67M\n",
      "Architecture: 4-stage SST Transformer + Hybrid Attention\n",
      "EMA initialized with decay=0.999\n",
      "Scheduler: Warmup (10 epochs) + Cosine Annealing\n",
      "Using gradient accumulation: 8 steps (effective batch size: 16)\n",
      "Starting training for 300 epochs...\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch    1 | LR: 1.50e-05 | Train Loss: 0.062039\n",
      "Epoch    2 | LR: 3.00e-05 | Train Loss: 0.020886\n",
      "Epoch    3 | LR: 4.50e-05 | Train Loss: 0.018849\n",
      "Epoch    4 | LR: 6.00e-05 | Train Loss: 0.016348\n",
      "  *** New best model saved! PSNR: 37.4137 dB ***\n",
      "Epoch    5 | LR: 7.50e-05 | Train Loss: 0.016228 | Val Loss: 0.012029 | Val PSNR: 37.4137 | Val SSIM: 0.9484 | Val SAM: 0.0457 | Best: 37.4137\n",
      "Epoch    6 | LR: 9.00e-05 | Train Loss: 0.013492\n",
      "Epoch    7 | LR: 1.05e-04 | Train Loss: 0.012718\n",
      "Epoch    8 | LR: 1.20e-04 | Train Loss: 0.015804\n",
      "Epoch    9 | LR: 1.35e-04 | Train Loss: 0.017461\n",
      "  *** New best model saved! PSNR: 38.5421 dB ***\n",
      "Epoch   10 | LR: 1.50e-04 | Train Loss: 0.015543 | Val Loss: 0.010471 | Val PSNR: 38.5421 | Val SSIM: 0.9567 | Val SAM: 0.0388 | Best: 38.5421\n",
      "Epoch   11 | LR: 1.50e-04 | Train Loss: 0.010775\n",
      "Epoch   12 | LR: 1.50e-04 | Train Loss: 0.011163\n",
      "Epoch   13 | LR: 1.50e-04 | Train Loss: 0.011940\n",
      "Epoch   14 | LR: 1.50e-04 | Train Loss: 0.009496\n",
      "  *** New best model saved! PSNR: 40.0382 dB ***\n",
      "Epoch   15 | LR: 1.50e-04 | Train Loss: 0.009928 | Val Loss: 0.008628 | Val PSNR: 40.0382 | Val SSIM: 0.9618 | Val SAM: 0.0331 | Best: 40.0382\n",
      "Epoch   16 | LR: 1.50e-04 | Train Loss: 0.013524\n",
      "Epoch   17 | LR: 1.50e-04 | Train Loss: 0.008825\n",
      "Epoch   18 | LR: 1.50e-04 | Train Loss: 0.009046\n",
      "Epoch   19 | LR: 1.50e-04 | Train Loss: 0.012812\n",
      "Epoch   20 | LR: 1.50e-04 | Train Loss: 0.008491 | Val Loss: 0.009558 | Val PSNR: 39.8242 | Val SSIM: 0.9627 | Val SAM: 0.0316 | Best: 40.0382\n",
      "Epoch   21 | LR: 1.50e-04 | Train Loss: 0.008382\n",
      "Epoch   22 | LR: 1.50e-04 | Train Loss: 0.008348\n",
      "Epoch   23 | LR: 1.50e-04 | Train Loss: 0.008425\n",
      "Epoch   24 | LR: 1.50e-04 | Train Loss: 0.008562\n",
      "  *** New best model saved! PSNR: 40.5683 dB ***\n",
      "Epoch   25 | LR: 1.50e-04 | Train Loss: 0.007966 | Val Loss: 0.008123 | Val PSNR: 40.5683 | Val SSIM: 0.9725 | Val SAM: 0.0313 | Best: 40.5683\n",
      "Epoch   26 | LR: 1.50e-04 | Train Loss: 0.008170\n",
      "Epoch   27 | LR: 1.50e-04 | Train Loss: 0.009908\n",
      "Epoch   28 | LR: 1.50e-04 | Train Loss: 0.009208\n",
      "Epoch   29 | LR: 1.50e-04 | Train Loss: 0.008059\n",
      "  *** New best model saved! PSNR: 40.9329 dB ***\n",
      "Epoch   30 | LR: 1.50e-04 | Train Loss: 0.007720 | Val Loss: 0.007685 | Val PSNR: 40.9329 | Val SSIM: 0.9705 | Val SAM: 0.0278 | Best: 40.9329\n",
      "Epoch   31 | LR: 1.50e-04 | Train Loss: 0.007507\n",
      "Epoch   32 | LR: 1.50e-04 | Train Loss: 0.007606\n",
      "Epoch   33 | LR: 1.50e-04 | Train Loss: 0.008739\n",
      "Epoch   34 | LR: 1.50e-04 | Train Loss: 0.007936\n",
      "  *** New best model saved! PSNR: 41.0020 dB ***\n",
      "Epoch   35 | LR: 1.50e-04 | Train Loss: 0.007217 | Val Loss: 0.007899 | Val PSNR: 41.0020 | Val SSIM: 0.9720 | Val SAM: 0.0294 | Best: 41.0020\n",
      "Epoch   36 | LR: 1.50e-04 | Train Loss: 0.007282\n",
      "Epoch   37 | LR: 1.50e-04 | Train Loss: 0.007353\n",
      "Epoch   38 | LR: 1.50e-04 | Train Loss: 0.007207\n",
      "Epoch   39 | LR: 1.50e-04 | Train Loss: 0.009145\n",
      "Epoch   40 | LR: 1.50e-04 | Train Loss: 0.008341 | Val Loss: 0.008317 | Val PSNR: 40.9550 | Val SSIM: 0.9668 | Val SAM: 0.0347 | Best: 41.0020\n",
      "Epoch   41 | LR: 1.50e-04 | Train Loss: 0.007138\n",
      "Epoch   42 | LR: 1.50e-04 | Train Loss: 0.007020\n",
      "Epoch   43 | LR: 1.50e-04 | Train Loss: 0.007149\n",
      "Epoch   44 | LR: 1.50e-04 | Train Loss: 0.006817\n",
      "  *** New best model saved! PSNR: 41.9697 dB ***\n",
      "Epoch   45 | LR: 1.50e-04 | Train Loss: 0.010807 | Val Loss: 0.006945 | Val PSNR: 41.9697 | Val SSIM: 0.9759 | Val SAM: 0.0252 | Best: 41.9697\n",
      "Epoch   46 | LR: 1.50e-04 | Train Loss: 0.007233\n",
      "Epoch   47 | LR: 1.50e-04 | Train Loss: 0.006631\n",
      "Epoch   48 | LR: 1.50e-04 | Train Loss: 0.006441\n",
      "Epoch   49 | LR: 1.50e-04 | Train Loss: 0.007089\n",
      "  *** New best model saved! PSNR: 42.2905 dB ***\n",
      "Epoch   50 | LR: 1.50e-04 | Train Loss: 0.006423 | Val Loss: 0.006720 | Val PSNR: 42.2905 | Val SSIM: 0.9785 | Val SAM: 0.0273 | Best: 42.2905\n",
      "Epoch   51 | LR: 1.50e-04 | Train Loss: 0.009648\n",
      "Epoch   52 | LR: 1.50e-04 | Train Loss: 0.007425\n",
      "Epoch   53 | LR: 1.50e-04 | Train Loss: 0.006284\n",
      "Epoch   54 | LR: 1.50e-04 | Train Loss: 0.006285\n",
      "Epoch   55 | LR: 1.50e-04 | Train Loss: 0.006240 | Val Loss: 0.006977 | Val PSNR: 42.2258 | Val SSIM: 0.9778 | Val SAM: 0.0260 | Best: 42.2905\n",
      "Epoch   56 | LR: 1.50e-04 | Train Loss: 0.006418\n",
      "Epoch   57 | LR: 1.50e-04 | Train Loss: 0.006580\n",
      "Epoch   58 | LR: 1.50e-04 | Train Loss: 0.006160\n",
      "Epoch   59 | LR: 1.50e-04 | Train Loss: 0.006064\n",
      "  *** New best model saved! PSNR: 42.6100 dB ***\n",
      "Epoch   60 | LR: 1.49e-04 | Train Loss: 0.007229 | Val Loss: 0.006909 | Val PSNR: 42.6100 | Val SSIM: 0.9768 | Val SAM: 0.0241 | Best: 42.6100\n",
      "Epoch   61 | LR: 1.49e-04 | Train Loss: 0.006184\n",
      "Epoch   62 | LR: 1.49e-04 | Train Loss: 0.006297\n",
      "Epoch   63 | LR: 1.49e-04 | Train Loss: 0.006606\n",
      "Epoch   64 | LR: 1.49e-04 | Train Loss: 0.006050\n",
      "  *** New best model saved! PSNR: 42.6163 dB ***\n",
      "Epoch   65 | LR: 1.49e-04 | Train Loss: 0.006208 | Val Loss: 0.006411 | Val PSNR: 42.6163 | Val SSIM: 0.9808 | Val SAM: 0.0232 | Best: 42.6163\n",
      "Epoch   66 | LR: 1.48e-04 | Train Loss: 0.006345\n",
      "Epoch   67 | LR: 1.48e-04 | Train Loss: 0.007805\n",
      "Epoch   68 | LR: 1.48e-04 | Train Loss: 0.006647\n",
      "Epoch   69 | LR: 1.48e-04 | Train Loss: 0.005990\n",
      "  *** New best model saved! PSNR: 42.8002 dB ***\n",
      "Epoch   70 | LR: 1.48e-04 | Train Loss: 0.005856 | Val Loss: 0.006629 | Val PSNR: 42.8002 | Val SSIM: 0.9788 | Val SAM: 0.0227 | Best: 42.8002\n",
      "Epoch   71 | LR: 1.47e-04 | Train Loss: 0.006051\n",
      "Epoch   72 | LR: 1.47e-04 | Train Loss: 0.006046\n",
      "Epoch   73 | LR: 1.47e-04 | Train Loss: 0.005911\n",
      "Epoch   74 | LR: 1.47e-04 | Train Loss: 0.005979\n",
      "  *** New best model saved! PSNR: 43.1963 dB ***\n",
      "Epoch   75 | LR: 1.46e-04 | Train Loss: 0.006700 | Val Loss: 0.006046 | Val PSNR: 43.1963 | Val SSIM: 0.9821 | Val SAM: 0.0211 | Best: 43.1963\n",
      "Epoch   76 | LR: 1.46e-04 | Train Loss: 0.007176\n",
      "Epoch   77 | LR: 1.46e-04 | Train Loss: 0.005951\n",
      "Epoch   78 | LR: 1.45e-04 | Train Loss: 0.005736\n",
      "Epoch   79 | LR: 1.45e-04 | Train Loss: 0.005870\n",
      "  *** New best model saved! PSNR: 43.4462 dB ***\n",
      "Epoch   80 | LR: 1.45e-04 | Train Loss: 0.006003 | Val Loss: 0.006036 | Val PSNR: 43.4462 | Val SSIM: 0.9817 | Val SAM: 0.0230 | Best: 43.4462\n",
      "Epoch   81 | LR: 1.44e-04 | Train Loss: 0.006697\n",
      "Epoch   82 | LR: 1.44e-04 | Train Loss: 0.006702\n",
      "Epoch   83 | LR: 1.44e-04 | Train Loss: 0.006952\n",
      "Epoch   84 | LR: 1.43e-04 | Train Loss: 0.005797\n",
      "Epoch   85 | LR: 1.43e-04 | Train Loss: 0.005792 | Val Loss: 0.005904 | Val PSNR: 43.3419 | Val SSIM: 0.9833 | Val SAM: 0.0240 | Best: 43.4462\n",
      "Epoch   86 | LR: 1.43e-04 | Train Loss: 0.005819\n",
      "Epoch   87 | LR: 1.42e-04 | Train Loss: 0.005943\n",
      "Epoch   88 | LR: 1.42e-04 | Train Loss: 0.005763\n",
      "Epoch   89 | LR: 1.41e-04 | Train Loss: 0.005814\n",
      "Epoch   90 | LR: 1.41e-04 | Train Loss: 0.005961 | Val Loss: 0.005946 | Val PSNR: 43.3740 | Val SSIM: 0.9820 | Val SAM: 0.0215 | Best: 43.4462\n",
      "Epoch   91 | LR: 1.40e-04 | Train Loss: 0.006063\n",
      "Epoch   92 | LR: 1.40e-04 | Train Loss: 0.005705\n",
      "Epoch   93 | LR: 1.39e-04 | Train Loss: 0.005790\n",
      "Epoch   94 | LR: 1.39e-04 | Train Loss: 0.006857\n",
      "Epoch   95 | LR: 1.38e-04 | Train Loss: 0.005766 | Val Loss: 0.006164 | Val PSNR: 43.1778 | Val SSIM: 0.9809 | Val SAM: 0.0214 | Best: 43.4462\n",
      "Epoch   96 | LR: 1.38e-04 | Train Loss: 0.005577\n",
      "Epoch   97 | LR: 1.37e-04 | Train Loss: 0.005729\n",
      "Epoch   98 | LR: 1.37e-04 | Train Loss: 0.005663\n",
      "Epoch   99 | LR: 1.36e-04 | Train Loss: 0.006521\n"
     ]
    }
   ],
   "source": [
    "# Complete Memory-Optimized Enhanced HSI Denoising Pipeline for PSNR > 40 dB\n",
    "# Comprehensive training with PSNR, SSIM, SAM metrics and visualization\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import json\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from torchvision.models import vgg19\n",
    "from torchvision.models import VGG19_Weights\n",
    "import torch.fft as fft\n",
    "import h5py\n",
    "try:\n",
    "    import scipy.io as sio\n",
    "except ImportError:\n",
    "    sio = None\n",
    "import scipy.ndimage\n",
    "# ----------------------------\n",
    "# Memory-Efficient Utilities\n",
    "# ----------------------------\n",
    "class LayerNormChannel3d(nn.Module):\n",
    "    \"\"\"Lightweight channel normalization\"\"\"\n",
    "    def __init__(self, num_channels: int = None, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.num_channels = num_channels\n",
    "        self.gn = None\n",
    "        if num_channels is not None:\n",
    "            self.gn = nn.GroupNorm(1, num_channels, eps=eps, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        C = x.shape[1]\n",
    "        if self.gn is None or C != self.num_channels:\n",
    "            self.num_channels = C\n",
    "            self.gn = nn.GroupNorm(1, C, eps=self.eps, affine=True).to(x.device)\n",
    "        return self.gn(x)\n",
    "\n",
    "def depthwise_conv3d(channels: int, kernel_size: int = 3, stride: int = 1, padding: int = 1, dilation: int = 1):\n",
    "    return nn.Conv3d(channels, channels, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, groups=channels, bias=True, dilation=dilation)\n",
    "\n",
    "class EfficientChannelAttention(nn.Module):\n",
    "    \"\"\"Memory-efficient channel attention\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        hidden = max(4, channels // reduction)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, hidden, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        y = self.avg_pool(x).view(B, C)\n",
    "        y = self.fc(y).view(B, C, 1, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class AdaptiveDropout3d(nn.Module):\n",
    "    \"\"\"Adaptive dropout that adjusts rate based on a factor (e.g., layer depth)\"\"\"\n",
    "    def __init__(self, base_drop=0.25, factor=1.0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout3d(base_drop * factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x)\n",
    "\n",
    "# ----------------------------\n",
    "# Memory-Optimized Blocks\n",
    "# ----------------------------\n",
    "class PatchMerging3D(nn.Module):\n",
    "    \"\"\"\n",
    "    3D Patch Merging Layer (downsampling)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(8 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, D, H, W)\n",
    "        \"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "\n",
    "        # Pad if needed\n",
    "        pad_d = (2 - D % 2) % 2\n",
    "        pad_h = (2 - H % 2) % 2\n",
    "        pad_w = (2 - W % 2) % 2\n",
    "\n",
    "        if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h, 0, pad_d))\n",
    "            D, H, W = x.shape[2:]\n",
    "\n",
    "        # Convert to (B, D, H, W, C)\n",
    "        x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
    "\n",
    "        # Downsample by merging 2x2x2 patches\n",
    "        x0 = x[:, 0::2, 0::2, 0::2, :]  # (B, D/2, H/2, W/2, C)\n",
    "        x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, 0::2, :]\n",
    "        x4 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x5 = x[:, 1::2, 0::2, 1::2, :]\n",
    "        x6 = x[:, 0::2, 1::2, 1::2, :]\n",
    "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "\n",
    "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)  # (B, D/2, H/2, W/2, 8*C)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)  # (B, D/2, H/2, W/2, 2*C)\n",
    "\n",
    "        # Convert back to (B, C, D, H, W)\n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "class PatchMerging3D(nn.Module):\n",
    "    \"\"\"3D Patch Merging with better handling\"\"\"\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(8 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Pad to multiples of 2\n",
    "        pad_d = (2 - D % 2) % 2\n",
    "        pad_h = (2 - H % 2) % 2\n",
    "        pad_w = (2 - W % 2) % 2\n",
    "        \n",
    "        if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h, 0, pad_d))\n",
    "            D, H, W = x.shape[2:]\n",
    "\n",
    "        x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        \n",
    "        x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, 0::2, :]\n",
    "        x4 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x5 = x[:, 1::2, 0::2, 1::2, :]\n",
    "        x6 = x[:, 0::2, 1::2, 1::2, :]\n",
    "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "        \n",
    "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        \n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "class PatchExpanding3D(nn.Module):\n",
    "    \"\"\"3D Patch Expanding for decoder\"\"\"\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.expand = nn.Linear(dim, 4 * dim, bias=False)\n",
    "        self.norm = norm_layer(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        x = self.norm(x)\n",
    "        x = self.expand(x)\n",
    "        \n",
    "        x = rearrange(x, 'b d h w (p1 p2 p3 c) -> b (d p1) (h p2) (w p3) c', \n",
    "                     p1=2, p2=2, p3=2, c=C//2)\n",
    "        \n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "class SpectralAttentionModule(nn.Module):\n",
    "    \"\"\"Dedicated spectral attention for bottleneck - HYBRID ATTENTION\"\"\"\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Conv3d(dim, dim * 3, 1)\n",
    "        self.proj = nn.Conv3d(dim, dim, 1)\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        qkv = self.qkv(x_norm)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=1)\n",
    "        \n",
    "        # Reshape for spectral attention: (B*H*W, num_heads, D, head_dim)\n",
    "        q = q.permute(0, 3, 4, 2, 1).reshape(B * H * W, D, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.permute(0, 3, 4, 2, 1).reshape(B * H * W, D, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 3, 4, 2, 1).reshape(B * H * W, D, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Spectral attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = (attn @ v).permute(0, 2, 1, 3).reshape(B, H, W, D, C)\n",
    "        out = out.permute(0, 4, 3, 1, 2).contiguous()\n",
    "        \n",
    "        out = self.proj(out)\n",
    "        return x + out\n",
    "\n",
    "# ----------------------------\n",
    "# SST Blocks\n",
    "# ----------------------------\n",
    "\n",
    "class SpectralSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Spectral Self-Attention: Attends along the spectral dimension (bands)\n",
    "    Treats each spatial position independently and attends across all bands\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_bands, num_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.num_bands = num_bands  # Expected bands from config\n",
    "            self.num_heads = num_heads\n",
    "            self.head_dim = dim // num_heads\n",
    "            self.scale = self.head_dim ** -0.5\n",
    "            \n",
    "            assert dim % num_heads == 0, f\"dim {dim} must be divisible by num_heads {num_heads}\"\n",
    "            \n",
    "            # Linear projections for Q, K, V\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "            self.attn_drop = nn.Dropout(attn_drop)\n",
    "            self.proj = nn.Linear(dim, dim)\n",
    "            self.proj_drop = nn.Dropout(proj_drop)\n",
    "            \n",
    "            # FIXED: Initialize buffer with expected bands, will expand automatically if needed\n",
    "            spectral_pos_embed = torch.zeros(1, num_bands, dim)\n",
    "            nn.init.trunc_normal_(spectral_pos_embed, std=0.02)\n",
    "            self.register_buffer('spectral_pos_embed', spectral_pos_embed)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, D, H, W) - Input feature map\n",
    "        Returns:\n",
    "            x: (B, C, D, H, W) - Output feature map\n",
    "        \"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Reshape to (B, H, W, D, C) for spectral attention\n",
    "        x = x.permute(0, 3, 4, 2, 1).contiguous()  # (B, H, W, D, C)\n",
    "        \n",
    "        # Flatten spatial dimensions: (B*H*W, D, C)\n",
    "        x_flat = x.reshape(B * H * W, D, C)\n",
    "        \n",
    "        # FIXED: Dynamically expand buffer if we encounter more bands than initialized\n",
    "        if D > self.spectral_pos_embed.shape[1]:\n",
    "            # This happens rarely (only when encountering new max bands)\n",
    "            old_size = self.spectral_pos_embed.shape[1]\n",
    "            new_size = D\n",
    "            \n",
    "            # Create expanded buffer on same device\n",
    "            expanded = torch.zeros(1, new_size, self.dim, \n",
    "                                  device=self.spectral_pos_embed.device,\n",
    "                                  dtype=self.spectral_pos_embed.dtype)\n",
    "            \n",
    "            # Copy existing learned embeddings\n",
    "            expanded[:, :old_size, :] = self.spectral_pos_embed\n",
    "            \n",
    "            # Initialize new bands with small random values\n",
    "            nn.init.trunc_normal_(expanded[:, old_size:, :], std=0.02)\n",
    "            \n",
    "            # Update the buffer in-place\n",
    "            self.spectral_pos_embed.resize_(expanded.shape)\n",
    "            self.spectral_pos_embed.copy_(expanded)\n",
    "            \n",
    "            #print(f\"[SpectralPosEmbed] Auto-expanded from {old_size} to {new_size} bands\")\n",
    "        \n",
    "        # Use only the bands we need (safe slicing - buffer is always >= D now)\n",
    "        pos_embed = self.spectral_pos_embed[:, :D, :]  # (1, D, dim)\n",
    "        \n",
    "        # Add spectral positional encoding\n",
    "        x_flat = x_flat + pos_embed\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x_flat).reshape(B * H * W, D, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each: (B*H*W, num_heads, D, head_dim)\n",
    "        \n",
    "        # Scaled dot-product attention across spectral dimension\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))  # (B*H*W, num_heads, D, D)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x_attn = (attn @ v).transpose(1, 2).reshape(B * H * W, D, C)  # (B*H*W, D, C)\n",
    "        \n",
    "        # Project and reshape back\n",
    "        x_attn = self.proj(x_attn)\n",
    "        x_attn = self.proj_drop(x_attn)\n",
    "        \n",
    "        # Reshape back to (B, H, W, D, C)\n",
    "        x_attn = x_attn.reshape(B, H, W, D, C)\n",
    "        \n",
    "        # Reshape to original format (B, C, D, H, W)\n",
    "        x_attn = x_attn.permute(0, 4, 3, 1, 2).contiguous()\n",
    "        \n",
    "        return x_attn\n",
    "\n",
    "class SpatialSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    FIXED Spatial Self-Attention - SAME NAME, IMPROVED IMPLEMENTATION\n",
    "    Drop-in replacement - no API changes, just better internals\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, window_size=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        assert dim % num_heads == 0, f\"dim {dim} must be divisible by num_heads {num_heads}\"\n",
    "        \n",
    "        # FIXED: Full 3D convolution instead of depthwise\n",
    "        # OLD: self.dwconv = depthwise_conv3d(dim, kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
    "        # NEW: Full conv for cross-spectral information flow\n",
    "        self.dwconv = nn.Conv3d(dim, dim, kernel_size=(1, 3, 3), padding=(0, 1, 1), bias=True)\n",
    "        \n",
    "        # Keep same API as before\n",
    "        self.qkv = nn.Conv3d(dim, dim * 3, kernel_size=1, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Conv3d(dim, dim, kernel_size=1)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "        # NEW: Relative position bias (optional, improves performance)\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads)\n",
    "        )\n",
    "        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        \n",
    "        coords_h = torch.arange(window_size)\n",
    "        coords_w = torch.arange(window_size)\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += window_size - 1\n",
    "        relative_coords[:, :, 1] += window_size - 1\n",
    "        relative_coords[:, :, 0] *= 2 * window_size - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        SAME API: (B, C, D, H, W) -> (B, C, D, H, W)\n",
    "        Just improved internals\n",
    "        \"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Apply full 3D conv (cross-spectral enabled)\n",
    "        x_local = self.dwconv(x)\n",
    "        \n",
    "        qkv = self.qkv(x_local)\n",
    "        \n",
    "        if H * W > self.window_size ** 2:\n",
    "            # Efficient attention for large spatial dims\n",
    "            qkv = rearrange(qkv, 'b (three head c) d h w -> three b d head c (h w)', \n",
    "                           three=3, head=self.num_heads)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "            \n",
    "            q_flat = q.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            k_flat = k.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            v_flat = v.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            \n",
    "            k_global = k_flat.mean(dim=-1, keepdim=True)\n",
    "            v_global = v_flat.mean(dim=-1, keepdim=True)\n",
    "            \n",
    "            q_flat = q_flat * self.scale\n",
    "            attn = torch.bmm(q_flat.transpose(1, 2), k_global)\n",
    "            attn = F.softmax(attn, dim=1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            \n",
    "            x_attn = v_global * attn.transpose(1, 2)\n",
    "            x_attn = x_attn.reshape(B, D, self.num_heads, self.head_dim, H * W)\n",
    "            \n",
    "        else:\n",
    "            # Full attention for small spatial dims\n",
    "            qkv = rearrange(qkv, 'b (three head c) d h w -> three b d head c (h w)', \n",
    "                           three=3, head=self.num_heads)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "            \n",
    "            q_flat = q.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            k_flat = k.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            v_flat = v.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            \n",
    "            q_flat = q_flat * self.scale\n",
    "            attn = torch.bmm(q_flat.transpose(1, 2), k_flat)\n",
    "            \n",
    "            if H == self.window_size and W == self.window_size:\n",
    "                relative_position_bias = self.relative_position_bias_table[\n",
    "                    self.relative_position_index.view(-1)\n",
    "                ].view(self.window_size * self.window_size, self.window_size * self.window_size, -1)\n",
    "                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "                attn = attn.view(B * D, self.num_heads, H * W, H * W) + relative_position_bias.unsqueeze(0)\n",
    "                attn = attn.view(B * D * self.num_heads, H * W, H * W)\n",
    "            \n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            \n",
    "            x_attn = torch.bmm(attn, v_flat.transpose(1, 2))\n",
    "            x_attn = x_attn.transpose(1, 2)\n",
    "            x_attn = x_attn.reshape(B, D, self.num_heads, self.head_dim, H * W)\n",
    "        \n",
    "        x_attn = rearrange(x_attn, 'b d head c (h w) -> b (head c) d h w', \n",
    "                          head=self.num_heads, h=H, w=W)\n",
    "        \n",
    "        x_attn = self.proj(x_attn)\n",
    "        x_attn = self.proj_drop(x_attn)\n",
    "        \n",
    "        return x_attn\n",
    "\n",
    "\n",
    "class SSTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Spectral-Spatial Transformer Block\n",
    "    Combines spectral and spatial self-attention with feed-forward network\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_bands, num_heads=8, window_size=8, \n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., \n",
    "                 drop_path=0., norm_layer=None):\n",
    "        super().__init__()\n",
    "        norm_layer = norm_layer or LayerNormChannel3d\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_bands = num_bands\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        \n",
    "        # Normalization layers\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.norm3 = norm_layer(dim)\n",
    "        \n",
    "        # Spectral attention\n",
    "        self.spectral_attn = SpectralSelfAttention(\n",
    "            dim=dim,\n",
    "            num_bands=num_bands,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop\n",
    "        )\n",
    "        \n",
    "        # Spatial attention\n",
    "        self.spatial_attn = SpatialSelfAttention(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            window_size=window_size,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop\n",
    "        )\n",
    "        \n",
    "        # Drop path for stochastic depth\n",
    "        self.drop_path = nn.Dropout(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        \n",
    "        # Feed-forward network (reuse existing GDFN)\n",
    "        self.ffn = GDFN(dim, ffn_expansion_factor=mlp_ratio, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, D, H, W)\n",
    "        Returns:\n",
    "            x: (B, C, D, H, W)\n",
    "        \"\"\"\n",
    "        # Spectral attention with residual\n",
    "        x = x + self.drop_path(self.spectral_attn(self.norm1(x)))\n",
    "        \n",
    "        # Spatial attention with residual\n",
    "        x = x + self.drop_path(self.spatial_attn(self.norm2(x)))\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        x = x + self.drop_path(self.ffn(self.norm3(x)))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SSTStage(nn.Module):\n",
    "    \"\"\"\n",
    "    SST Stage: Multiple SST blocks with optional downsampling\n",
    "    Replaces SwinTransformerStage3D\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_bands, depth, num_heads=8, window_size=8,\n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n",
    "                 drop_path_rate=0., norm_layer=None, downsample=None):\n",
    "        super().__init__()\n",
    "        norm_layer = norm_layer or LayerNormChannel3d\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        \n",
    "        # Stochastic depth decay rule\n",
    "        dpr = [drop_path_rate * (i / (depth - 1)) if depth > 1 else drop_path_rate \n",
    "               for i in range(depth)]\n",
    "        \n",
    "        # Build SST blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SSTBlock(\n",
    "                dim=dim,\n",
    "                num_bands=num_bands,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Downsampling layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=nn.LayerNorm)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, D, H, W)\n",
    "        Returns:\n",
    "            x: (B, C, D, H, W) - features before downsampling\n",
    "            x_down: (B, 2*C, D/2, H/2, W/2) - downsampled features (if downsample exists)\n",
    "        \"\"\"\n",
    "        # Pass through SST blocks\n",
    "        for blk in self.blocks:\n",
    "            if self.training:\n",
    "                x = checkpoint(blk, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        \n",
    "        # Downsample if needed\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x)\n",
    "            return x, x_down\n",
    "        else:\n",
    "            return x, x\n",
    "\n",
    "# ----------------------------\n",
    "# SST Blocks\n",
    "# ----------------------------\n",
    "\n",
    "class SpectralSelfModulatingResidualBlock(nn.Module):\n",
    "    \"\"\"Spectral Self-Modulating Residual Block (SSMRB) for adaptive feature transformation.\"\"\"\n",
    "    def __init__(self, dim, ffn_expand=2, drop=0.25, drop_factor=1.0):\n",
    "        super().__init__()\n",
    "        hidden = dim * ffn_expand\n",
    "\n",
    "        # Main FFN path (vanilla-like)\n",
    "        self.ffn_pw1 = nn.Conv3d(dim, hidden * 2, kernel_size=1, bias=True)\n",
    "        self.ffn_dw = depthwise_conv3d(hidden * 2, kernel_size=3, padding=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.ffn_pw2 = nn.Conv3d(hidden, dim, kernel_size=1, bias=True)\n",
    "\n",
    "        # Self-modulation branch: Generate gamma (scale) and beta (shift) using spectral-adjacent conv\n",
    "        # Kernel (3,1,1) captures adjacent spectral bands; depthwise for efficiency\n",
    "        self.mod_gamma = nn.Sequential(\n",
    "            depthwise_conv3d(dim, kernel_size=(3,1,1), padding=(1,0,0)),\n",
    "            nn.Sigmoid()  # For scaling (0 to 1)\n",
    "        )\n",
    "        self.mod_beta = nn.Sequential(\n",
    "            depthwise_conv3d(dim, kernel_size=(3,1,1), padding=(1,0,0)),\n",
    "            nn.Tanh()  # For shifting (-1 to 1)\n",
    "        )\n",
    "\n",
    "        # Normalization and dropout\n",
    "        self.norm = LayerNormChannel3d(dim)\n",
    "        self.dropout = AdaptiveDropout3d(drop, factor=drop_factor)  # Adaptive dropout\n",
    "\n",
    "        # Layer scale for residual contribution\n",
    "        self.gamma_res = nn.Parameter(torch.ones(1, dim, 1, 1, 1) * 1e-4)\n",
    "\n",
    "    def _ffn_path(self, x):\n",
    "        \"\"\"Vanilla FFN computation.\"\"\"\n",
    "        x2 = self.ffn_pw1(x)\n",
    "        x2 = self.ffn_dw(x2)\n",
    "        a, b = torch.chunk(x2, 2, dim=1)\n",
    "        x2 = self.act(a) * b\n",
    "        x2 = self.ffn_pw2(x2)\n",
    "        return x2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize input\n",
    "        x_norm = self.norm(x)\n",
    "\n",
    "        # Compute main FFN path\n",
    "        ffn_out = self._ffn_path(x_norm)\n",
    "\n",
    "        # Compute self-modulation parameters from input (using adjacent spectral info)\n",
    "        gamma = self.mod_gamma(x_norm)  # Scale\n",
    "        beta = self.mod_beta(x_norm)    # Shift\n",
    "\n",
    "        # Apply modulation: gamma * ffn_out + beta\n",
    "        modulated = gamma * ffn_out + beta\n",
    "\n",
    "        # Apply dropout and scale\n",
    "        modulated = self.dropout(modulated) * self.gamma_res\n",
    "\n",
    "        # Residual connection: x + modulated\n",
    "        return x + modulated\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# New Classes Added (for Restormer Integration in Architecture)\n",
    "# ----------------------------\n",
    "class LayerNorm3d(nn.Module):\n",
    "    \"\"\"3D LayerNorm for channel normalization\"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(dim=1, keepdim=True)\n",
    "        var = x.var(dim=1, keepdim=True, unbiased=False)\n",
    "        inv_std = torch.rsqrt(var + self.eps)\n",
    "        out = (x - mu) * inv_std * self.weight.view(1, -1, 1, 1, 1) + self.bias.view(1, -1, 1, 1, 1)\n",
    "        return out\n",
    "\n",
    "class GDFN(nn.Module):\n",
    "    \"\"\"Gated-Dconv Feed-Forward Network adapted to 3D, focusing on spatial\"\"\"\n",
    "    def __init__(self, dim, ffn_expansion_factor=2.66, bias=False):\n",
    "        super().__init__()\n",
    "        hidden = int(dim * ffn_expansion_factor)\n",
    "        self.project_in = nn.Conv3d(dim, hidden * 2, kernel_size=1, bias=bias)\n",
    "        self.dwconv = nn.Conv3d(hidden * 2, hidden * 2, kernel_size=(1, 3, 3), stride=1, padding=(0, 1, 1), groups=hidden * 2, bias=bias)  # Spatial focus\n",
    "        self.project_out = nn.Conv3d(hidden, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x = F.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "class MDTA(nn.Module):\n",
    "    \"\"\"Multi-Dconv Head Transposed Attention adapted to 3D, focusing on spatial\"\"\"\n",
    "    def __init__(self, dim, num_heads, bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        self.qkv = nn.Conv3d(dim, dim * 3, kernel_size=1, bias=bias)\n",
    "        self.qkv_dwconv = nn.Conv3d(dim * 3, dim * 3, kernel_size=(1, 3, 3), stride=1, padding=(0, 1, 1), groups=dim * 3, bias=bias)  # Spatial focus\n",
    "        self.project_out = nn.Conv3d(dim, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.qkv_dwconv(self.qkv(x))\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        b, c, d, h, w = q.shape\n",
    "        q = rearrange(q, 'b (head cc) d h w -> b head cc (d h w)', head=self.num_heads, cc=c // self.num_heads)\n",
    "        k = rearrange(k, 'b (head cc) d h w -> b head cc (d h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head cc) d h w -> b head cc (d h w)', head=self.num_heads)\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v)\n",
    "        out = rearrange(out, 'b head cc (d h w) -> b (head cc) d h w', head=self.num_heads, d=d, h=h, w=w)\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "\n",
    "class RestormerBlock(nn.Module):\n",
    "    \"\"\"Restormer Transformer Block adapted to 3D\"\"\"\n",
    "    def __init__(self, dim, num_heads=4, ffn_expansion_factor=2.66, bias=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = LayerNorm3d(dim)\n",
    "        self.attn = MDTA(dim, num_heads, bias)\n",
    "        self.norm2 = LayerNorm3d(dim)\n",
    "        self.ffn = GDFN(dim, ffn_expansion_factor, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "# ----------------------------\n",
    "# New Classes Added (for Restormer Integration in Architecture)\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Modified FusedBottleneck (for Restormer Integration in Architecture)\n",
    "# ----------------------------\n",
    "# ============================================================\n",
    "# INSERT THESE NEW CLASSES BEFORE FusedBottleneck (around line ~680)\n",
    "# These are ADDITIONS, not replacements\n",
    "# ============================================================\n",
    "\n",
    "class PositionalEncoding3D(nn.Module):\n",
    "    \"\"\"\n",
    "    FIXED: Lightweight 3D Positional Encoding\n",
    "    \n",
    "    Instead of storing full (C, D, H, W) tensor, we use:\n",
    "    1. Separate 1D embeddings for each dimension (much smaller)\n",
    "    2. Broadcast and combine at runtime\n",
    "    \n",
    "    Memory: O(C*D + C*H + C*W) instead of O(C*D*H*W)\n",
    "    Example: 64*128 + 64*256 + 64*256 = 41K params vs 537M params!\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, max_d=128, max_h=256, max_w=256):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.max_d = max_d\n",
    "        self.max_h = max_h\n",
    "        self.max_w = max_w\n",
    "        \n",
    "        # Separate 1D positional embeddings for each dimension\n",
    "        # These will be broadcast and combined\n",
    "        self.pos_embed_d = nn.Parameter(torch.zeros(1, channels, max_d, 1, 1))\n",
    "        self.pos_embed_h = nn.Parameter(torch.zeros(1, channels, 1, max_h, 1))\n",
    "        self.pos_embed_w = nn.Parameter(torch.zeros(1, channels, 1, 1, max_w))\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        nn.init.trunc_normal_(self.pos_embed_d, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed_h, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed_w, std=0.02)\n",
    "        \n",
    "        # Learnable scaling factors for each dimension\n",
    "        self.scale_d = nn.Parameter(torch.ones(1))\n",
    "        self.scale_h = nn.Parameter(torch.ones(1))\n",
    "        self.scale_w = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, D, H, W)\n",
    "        Returns:\n",
    "            x + positional encoding: (B, C, D, H, W)\n",
    "        \"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Slice and broadcast each dimension\n",
    "        pe_d = self.pos_embed_d[:, :, :D, :, :] * self.scale_d\n",
    "        pe_h = self.pos_embed_h[:, :, :, :H, :] * self.scale_h\n",
    "        pe_w = self.pos_embed_w[:, :, :, :, :W] * self.scale_w\n",
    "        \n",
    "        # Combine positional encodings (broadcasting happens automatically)\n",
    "        pe = pe_d + pe_h + pe_w  # Shape: (1, C, D, H, W)\n",
    "        \n",
    "        return x + pe\n",
    "\n",
    "\n",
    "class CrossSpectralSpatialAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    NEW CLASS: Cross-attention between spectral and spatial features\n",
    "    Enables joint spectral-spatial modeling instead of separate processing\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Separate projections for cross-attention paths\n",
    "        self.q_spectral = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv_spatial = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.q_spatial = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv_spectral = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_spectral = nn.Linear(dim, dim)\n",
    "        self.proj_spatial = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "        # Gated fusion\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"(B, C, D, H, W) -> (B, C, D, H, W)\"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Path 1: Spectral features with spatial context\n",
    "        x_spectral = x.permute(0, 3, 4, 2, 1).reshape(B * H * W, D, C)\n",
    "        q_spec = self.q_spectral(x_spectral)\n",
    "        q_spec = q_spec.reshape(B * H * W, D, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        q_spec_scaled = q_spec * self.scale\n",
    "        attn_spec = torch.matmul(q_spec_scaled, q_spec_scaled.transpose(-2, -1))\n",
    "        attn_spec = F.softmax(attn_spec, dim=-1)\n",
    "        attn_spec = self.attn_drop(attn_spec)\n",
    "        \n",
    "        out_spec = torch.matmul(attn_spec, q_spec)\n",
    "        out_spec = out_spec.transpose(1, 2).reshape(B * H * W, D, C)\n",
    "        out_spec = self.proj_spectral(out_spec)\n",
    "        out_spec = self.proj_drop(out_spec)\n",
    "        out_spectral = out_spec.reshape(B, H, W, D, C).permute(0, 4, 3, 1, 2)\n",
    "        \n",
    "        # Path 2: Spatial features with spectral context\n",
    "        x_spatial_q = x.permute(0, 2, 3, 4, 1).reshape(B * D, H * W, C)\n",
    "        q_spat = self.q_spatial(x_spatial_q)\n",
    "        q_spat = q_spat.reshape(B * D, H * W, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        q_spat_scaled = q_spat * self.scale\n",
    "        attn_spat = torch.matmul(q_spat_scaled, q_spat_scaled.transpose(-2, -1))\n",
    "        attn_spat = F.softmax(attn_spat, dim=-1)\n",
    "        attn_spat = self.attn_drop(attn_spat)\n",
    "        \n",
    "        out_spat = torch.matmul(attn_spat, q_spat)\n",
    "        out_spat = out_spat.transpose(1, 2).reshape(B * D, H * W, C)\n",
    "        out_spat = self.proj_spatial(out_spat)\n",
    "        out_spat = self.proj_drop(out_spat)\n",
    "        out_spatial = out_spat.reshape(B, D, H, W, C).permute(0, 4, 1, 2, 3)\n",
    "        \n",
    "        # Gated fusion\n",
    "        concat_features = torch.cat([out_spectral, out_spatial], dim=1)\n",
    "        gate_input = F.adaptive_avg_pool3d(concat_features, 1).squeeze(-1).squeeze(-1).squeeze(-1)\n",
    "        gate = self.gate(gate_input).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        fused = gate * out_spectral + (1 - gate) * out_spatial\n",
    "        return fused\n",
    "\n",
    "\n",
    "class EnhancedBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    NEW CLASS: SOTA-level bottleneck with cross-attention and joint modeling\n",
    "    Will be used in the main architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.norm1 = LayerNormChannel3d(dim)\n",
    "        self.norm2 = LayerNormChannel3d(dim)\n",
    "        self.norm3 = LayerNormChannel3d(dim)\n",
    "        \n",
    "        # Cross-attention for spectral-spatial joint modeling\n",
    "        self.cross_attn = CrossSpectralSpatialAttention(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=True,\n",
    "            attn_drop=0.1,\n",
    "            proj_drop=0.1\n",
    "        )\n",
    "        \n",
    "        # Non-local attention for global context\n",
    "        self.non_local = nn.Sequential(\n",
    "            nn.Conv3d(dim, dim // 2, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv3d(dim // 2, dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Keep using existing GDFN and SSMRB\n",
    "        self.ffn = GDFN(dim, ffn_expansion_factor=mlp_ratio, bias=False)\n",
    "        self.spectral_refine = SpectralSelfModulatingResidualBlock(\n",
    "            dim, ffn_expand=2, drop=0.1, drop_factor=1.0\n",
    "        )\n",
    "        \n",
    "        # Gated fusion\n",
    "        self.fusion_gate = nn.Sequential(\n",
    "            nn.Conv3d(dim * 3, dim, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv3d(dim, dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"(B, C, D, H, W) -> (B, C, D, H, W)\"\"\"\n",
    "        identity = x\n",
    "        \n",
    "        # Path 1: Cross-attention\n",
    "        cross_out = self.cross_attn(self.norm1(x))\n",
    "        x = x + cross_out\n",
    "        \n",
    "        # Path 2: Non-local\n",
    "        non_local_weight = self.non_local(self.norm2(x))\n",
    "        non_local_out = x * non_local_weight\n",
    "        x = x + non_local_out\n",
    "        \n",
    "        # Path 3: FFN + Spectral refinement\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        spectral_out = self.spectral_refine(x + ffn_out)\n",
    "        \n",
    "        # Gated fusion\n",
    "        fusion_input = torch.cat([cross_out, non_local_out, spectral_out], dim=1)\n",
    "        fusion_weight = self.fusion_gate(fusion_input)\n",
    "        \n",
    "        out = identity + fusion_weight * (cross_out + non_local_out + spectral_out) / 3.0\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# Modified FusedBottleneck (for Restormer Integration in Architecture)\n",
    "# ----------------------------\n",
    "        \n",
    "class FusedBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    IMPROVED FusedBottleneck - SAME NAME, better implementation\n",
    "    Now uses stacked EnhancedBottleneck blocks for SOTA performance\n",
    "    Drop-in replacement - same API, just calls different internals\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dim, window_sizes=[2, 4]):\n",
    "        super().__init__()\n",
    "        # Calculate actual dim from base_dim (maintains compatibility)\n",
    "        # Your original: dim = base_dim * 4\n",
    "        # But you call it with base_dim * 2, so actual dim is base_dim * 8\n",
    "        dim = base_dim * 4  # This gives base_dim * 8 when called with base_dim * 2\n",
    "        \n",
    "        # Use stacked enhanced bottleneck blocks instead of old approach\n",
    "        self.blocks = nn.ModuleList([\n",
    "            EnhancedBottleneck(dim, num_heads=8, mlp_ratio=4.),\n",
    "            #EnhancedBottleneck(dim, num_heads=8, mlp_ratio=4.)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        SAME API: (B, C, D, H, W) -> (B, C, D, H, W)\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Modified FusedBottleneck (for Restormer Integration in Architecture)\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Efficient Loss Function\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Modified MemoryEfficientLoss (for Spatial-Focused Loss Improvements)\n",
    "# ----------------------------\n",
    "class MemoryEfficientLoss(nn.Module):\n",
    "    \"\"\"Lightweight but effective loss function with FIXED weights and tensor handling\"\"\"\n",
    "    def __init__(self, device='cuda', mse_weight=1.0, l1_weight=1.0, sam_weight=0.5, edge_weight=0.2):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.device = device\n",
    "        self.mse_weight = mse_weight\n",
    "        self.l1_weight = l1_weight\n",
    "        self.sam_weight = sam_weight\n",
    "        self.edge_weight = edge_weight\n",
    "\n",
    "    def forward(self, pred, target, epoch=None):\n",
    "        # FIXED: Ensure both tensors have same shape\n",
    "        if pred.shape != target.shape:\n",
    "            # If shapes don't match, interpolate pred to match target\n",
    "            if pred.dim() == 5 and target.dim() == 5:\n",
    "                pred = F.interpolate(pred, size=target.shape[2:], mode='trilinear', align_corners=False)\n",
    "            elif pred.dim() == 4 and target.dim() == 4:\n",
    "                pred = F.interpolate(pred, size=target.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Main losses\n",
    "        mse_loss = self.mse(pred, target)\n",
    "        l1_loss = self.l1(pred, target)\n",
    "\n",
    "        # FIXED: SAM calculation with proper tensor handling\n",
    "        eps = 1e-8\n",
    "\n",
    "        # Handle both 4D and 5D tensors\n",
    "        if pred.dim() == 5:  # (B, C, D, H, W)\n",
    "            B, C, D, H, W = pred.shape\n",
    "            pred_flat = pred.reshape(B, C, D * H * W)  # (B, C, D*H*W)\n",
    "            target_flat = target.reshape(B, C, D * H * W)  # (B, C, D*H*W)\n",
    "\n",
    "            # Normalize along channel dimension (spectral bands)\n",
    "            pred_norm = F.normalize(pred_flat, dim=1, eps=eps)\n",
    "            target_norm = F.normalize(target_flat, dim=1, eps=eps)\n",
    "\n",
    "            # Compute cosine similarity along spectral dimension\n",
    "            cos_sim = torch.sum(pred_norm * target_norm, dim=1)  # (B, D*H*W)\n",
    "\n",
    "        elif pred.dim() == 4:  # (B, D, H, W) - spectral first\n",
    "            B, D, H, W = pred.shape\n",
    "            pred_flat = pred.reshape(B, D, H * W)  # (B, D, H*W)\n",
    "            target_flat = target.reshape(B, D, H * W)  # (B, D, H*W)\n",
    "\n",
    "            # Normalize along spectral dimension\n",
    "            pred_norm = F.normalize(pred_flat, dim=1, eps=eps)\n",
    "            target_norm = F.normalize(target_flat, dim=1, eps=eps)\n",
    "\n",
    "            # Compute cosine similarity along spectral dimension\n",
    "            cos_sim = torch.sum(pred_norm * target_norm, dim=1)  # (B, H*W)\n",
    "\n",
    "        else:\n",
    "            # Fallback for other dimensions\n",
    "            pred_flat = pred.flatten(start_dim=1)\n",
    "            target_flat = target.flatten(start_dim=1)\n",
    "            pred_norm = F.normalize(pred_flat, dim=1, eps=eps)\n",
    "            target_norm = F.normalize(target_flat, dim=1, eps=eps)\n",
    "            cos_sim = torch.sum(pred_norm * target_norm, dim=1)\n",
    "\n",
    "        cos_sim = torch.clamp(cos_sim, -1 + eps, 1 - eps)\n",
    "        sam_loss = torch.mean(1 - cos_sim)\n",
    "\n",
    "        # FIXED: Edge loss with proper spatial dimension handling\n",
    "        def spatial_gradient(x):\n",
    "            if x.dim() == 5:  # (B, C, D, H, W)\n",
    "                grad_h = torch.abs(x[:, :, :, 1:, :] - x[:, :, :, :-1, :])\n",
    "                grad_w = torch.abs(x[:, :, :, :, 1:] - x[:, :, :, :, :-1])\n",
    "            elif x.dim() == 4:  # (B, D, H, W)\n",
    "                grad_h = torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :])\n",
    "                grad_w = torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1])\n",
    "            else:\n",
    "                return 0, 0\n",
    "            return grad_h.mean(), grad_w.mean()\n",
    "\n",
    "        pred_grad_h, pred_grad_w = spatial_gradient(pred)\n",
    "        target_grad_h, target_grad_w = spatial_gradient(target)\n",
    "        edge_loss = abs(pred_grad_h - target_grad_h) + abs(pred_grad_w - target_grad_w)\n",
    "\n",
    "        # Static combination\n",
    "        total_loss = (\n",
    "            self.mse_weight * mse_loss +\n",
    "            self.l1_weight * l1_loss +\n",
    "            self.sam_weight * sam_loss +\n",
    "            self.edge_weight * edge_loss\n",
    "        )\n",
    "\n",
    "        return total_loss\n",
    "# ----------------------------\n",
    "# Modified MemoryEfficientLoss (for Spatial-Focused Loss Improvements)\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Memory-Efficient U-Net\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Memory-Efficient U-Net\n",
    "# ----------------------------\n",
    "class MemoryOptimizedUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    SST-based U-Net for HSI Denoising (MODIFIED)\n",
    "    - 4 hierarchical stages with SST blocks instead of Swin\n",
    "    - Spectral-aware attention at all levels\n",
    "    - Deep supervision at each decoder stage\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, base_dim=48, window_sizes=[4, 8, 16], num_bands=64):\n",
    "        super().__init__()\n",
    "        self.base_dim = base_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.num_bands = num_bands\n",
    "        \n",
    "        # Initial projection\n",
    "        self.patch_embed = nn.Conv3d(in_channels, base_dim, kernel_size=3, padding=1)\n",
    "        self.pos_embed_init = PositionalEncoding3D(base_dim, 128, 256, 256)\n",
    "        \n",
    "        # ENCODER: 4 SST stages with [2, 2, 6, 2] depth\n",
    "        # Stage 1: base_dim, shallow features\n",
    "        self.enc_stage1 = SSTStage(\n",
    "            dim=base_dim,\n",
    "            num_bands=num_bands,\n",
    "            depth=2,\n",
    "            num_heads=8,\n",
    "            window_size=8,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.05,\n",
    "            downsample=PatchMerging3D\n",
    "        )\n",
    "        \n",
    "        # Stage 2: base_dim*2, intermediate features\n",
    "        self.enc_stage2 = SSTStage(\n",
    "            dim=base_dim * 2,\n",
    "            num_bands=num_bands // 2,  # Bands halved after merging\n",
    "            depth=2,\n",
    "            num_heads=8,\n",
    "            window_size=8,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.1,\n",
    "            downsample=PatchMerging3D\n",
    "        )\n",
    "        \n",
    "        # Stage 3: base_dim*4, deep features (6 blocks)\n",
    "        self.enc_stage3 = SSTStage(\n",
    "            dim=base_dim * 4,\n",
    "            num_bands=num_bands // 4,\n",
    "            depth=6,\n",
    "            num_heads=16,\n",
    "            window_size=4,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.15,\n",
    "            downsample=PatchMerging3D\n",
    "        )\n",
    "        \n",
    "        # Stage 4 (deepest): base_dim*8, bottleneck\n",
    "        self.enc_stage4 = SSTStage(\n",
    "            dim=base_dim * 8,\n",
    "            num_bands=num_bands // 8,\n",
    "            depth=2,\n",
    "            num_heads=16,\n",
    "            window_size=2,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.2,\n",
    "            downsample=None\n",
    "        )\n",
    "\n",
    "        self.pe_enc1 = PositionalEncoding3D(base_dim, 128, 256, 256)\n",
    "        self.pe_enc2 = PositionalEncoding3D(base_dim * 2, 64, 128, 128)\n",
    "        self.pe_enc3 = PositionalEncoding3D(base_dim * 4, 32, 64, 64)\n",
    "        self.pe_enc4 = PositionalEncoding3D(base_dim * 8, 16, 32, 32)\n",
    "        \n",
    "        # BOTTLENECK: Keep your custom FusedBottleneck (works well with SST)\n",
    "        self.spectral_attention = SpectralAttentionModule(base_dim * 8, num_heads=8)\n",
    "        self.bottleneck_fusion = FusedBottleneck(base_dim * 2, window_sizes=window_sizes)\n",
    "        \n",
    "        # DECODER: 4 SST stages matching encoder\n",
    "        # Stage 3 decoder\n",
    "        self.dec_stage3_up = PatchExpanding3D(dim=base_dim * 8)\n",
    "        self.dec_stage3 = SSTStage(\n",
    "            dim=base_dim * 4,\n",
    "            num_bands=num_bands // 4,\n",
    "            depth=6,\n",
    "            num_heads=16,\n",
    "            window_size=4,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.1,\n",
    "            downsample=None\n",
    "        )\n",
    "        \n",
    "        # Stage 2 decoder\n",
    "        self.dec_stage2_up = PatchExpanding3D(dim=base_dim * 4)\n",
    "        self.dec_stage2 = SSTStage(\n",
    "            dim=base_dim * 2,\n",
    "            num_bands=num_bands // 2,\n",
    "            depth=2,\n",
    "            num_heads=8,\n",
    "            window_size=8,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.08,\n",
    "            downsample=None\n",
    "        )\n",
    "        \n",
    "        # Stage 1 decoder\n",
    "        self.dec_stage1_up = PatchExpanding3D(dim=base_dim * 2)\n",
    "        self.dec_stage1 = SSTStage(\n",
    "            dim=base_dim,\n",
    "            num_bands=num_bands,\n",
    "            depth=2,\n",
    "            num_heads=8,\n",
    "            window_size=8,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.02,\n",
    "            downsample=None\n",
    "        )\n",
    "        \n",
    "        # DEEP SUPERVISION: Auxiliary outputs at each decoder stage\n",
    "        self.deep_sup3 = nn.Conv3d(base_dim * 4, in_channels, 1)\n",
    "        self.deep_sup2 = nn.Conv3d(base_dim * 2, in_channels, 1)\n",
    "        self.deep_sup1 = nn.Conv3d(base_dim, in_channels, 1)\n",
    "        \n",
    "        # Final reconstruction\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv3d(base_dim, base_dim // 2, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv3d(base_dim // 2, in_channels, 1),\n",
    "        )\n",
    "        \n",
    "        # Global residual\n",
    "        self.global_residual = nn.Conv3d(in_channels, in_channels, 1)\n",
    "        \n",
    "        # Deep supervision flag\n",
    "        self.use_deep_supervision = True\n",
    "\n",
    "    def _align_tensors(self, x, target_size):\n",
    "        if x.shape[2:] != target_size:\n",
    "            x = F.interpolate(x, size=target_size, mode='trilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, return_deep_sup=False):\n",
    "        # Handle input shape\n",
    "        original_was_4d = False\n",
    "        if x.dim() == 4:\n",
    "            original_was_4d = True\n",
    "            x = x.unsqueeze(1)\n",
    "        elif x.dim() == 5 and x.shape[1] != 1:\n",
    "            if x.shape[2] == 1:\n",
    "                x = x.transpose(1, 2)\n",
    "        \n",
    "        original_size = x.shape[2:]\n",
    "        input_residual = self.global_residual(x)\n",
    "        \n",
    "        # Initial embedding\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.pos_embed_init(x)\n",
    "        \n",
    "        # ENCODER (4 SST stages)\n",
    "        e1, e1_down = self.enc_stage1(self.pe_enc1(x))       # Skip 1\n",
    "        e2, e2_down = self.enc_stage2(self.pe_enc2(e1_down))  # Skip 2\n",
    "        e3, e3_down = self.enc_stage3(self.pe_enc3(e2_down))  # Skip 3\n",
    "        e4, _ = self.enc_stage4(self.pe_enc4(e3_down))        # Deepest features\n",
    "        \n",
    "        # BOTTLENECK: Hybrid attention\n",
    "        b = self.spectral_attention(e4)  # Add spectral attention\n",
    "        b = self.bottleneck_fusion(b)   # Your custom fusion\n",
    "        \n",
    "        # DECODER with deep supervision\n",
    "        deep_outputs = []\n",
    "        \n",
    "        # Decoder stage 3\n",
    "        d3 = self.dec_stage3_up(b)\n",
    "        d3 = self._align_tensors(d3, e3.shape[2:])\n",
    "        d3 = d3 + e3  # Skip connection\n",
    "        d3, _ = self.dec_stage3(d3)\n",
    "        if self.training and self.use_deep_supervision:\n",
    "            sup3 = self.deep_sup3(d3)\n",
    "            sup3 = self._align_tensors(sup3, original_size)\n",
    "            deep_outputs.append(sup3)\n",
    "        \n",
    "        # Decoder stage 2\n",
    "        d2 = self.dec_stage2_up(d3)\n",
    "        d2 = self._align_tensors(d2, e2.shape[2:])\n",
    "        d2 = d2 + e2\n",
    "        d2, _ = self.dec_stage2(d2)\n",
    "        if self.training and self.use_deep_supervision:\n",
    "            sup2 = self.deep_sup2(d2)\n",
    "            sup2 = self._align_tensors(sup2, original_size)\n",
    "            deep_outputs.append(sup2)\n",
    "        \n",
    "        # Decoder stage 1\n",
    "        d1 = self.dec_stage1_up(d2)\n",
    "        d1 = self._align_tensors(d1, e1.shape[2:])\n",
    "        d1 = d1 + e1\n",
    "        d1, _ = self.dec_stage1(d1)\n",
    "        if self.training and self.use_deep_supervision:\n",
    "            sup1 = self.deep_sup1(d1)\n",
    "            sup1 = self._align_tensors(sup1, original_size)\n",
    "            deep_outputs.append(sup1)\n",
    "        \n",
    "        # Final reconstruction\n",
    "        out = self.final_conv(d1)\n",
    "        out = self._align_tensors(out, original_size)\n",
    "        input_residual = self._align_tensors(input_residual, original_size)\n",
    "        out = out + input_residual\n",
    "        \n",
    "        # Return format handling\n",
    "        if original_was_4d and out.shape[1] == 1:\n",
    "            out = out.squeeze(1)\n",
    "            if self.training and self.use_deep_supervision:\n",
    "                deep_outputs = [o.squeeze(1) for o in deep_outputs]\n",
    "        \n",
    "        if return_deep_sup and self.training:\n",
    "            return out, deep_outputs\n",
    "        return out\n",
    "# ----------------------------\n",
    "# Efficient Data Loading\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# Modified domain_shift_augment (for Spatial Augmentations in Data)\n",
    "# ----------------------------\n",
    "def domain_shift_augment(cube):\n",
    "    \"\"\"Simulate sensor differences for better generalization\"\"\"\n",
    "    # Simulate sensor differences: random spectral scaling and offset\n",
    "    if random.random() < 0.5:\n",
    "        scale = random.uniform(0.9, 1.1)\n",
    "        offset = random.uniform(-0.05, 0.05)\n",
    "        cube = np.clip(cube * scale + offset, 0, 1)\n",
    "    # Simulate band misalignment: slight band shuffling with probability\n",
    "    if random.random() < 0.3:\n",
    "        perm = np.arange(cube.shape[0])\n",
    "        np.random.shuffle(perm)\n",
    "        shift = random.randint(1, 3)\n",
    "        perm = np.roll(perm, shift)\n",
    "        cube = cube[perm]\n",
    "    # Added spatial-heavy: rotate on spatial axes\n",
    "    if random.random() < 0.4:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        for i in range(cube.shape[0]):\n",
    "            cube[i] = scipy.ndimage.rotate(cube[i], angle, reshape=False, mode='reflect')\n",
    "    return cube\n",
    "\n",
    "# ----------------------------\n",
    "# Modified domain_shift_augment (for Spatial Augmentations in Data)\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Modified MemoryEfficientHSIDataset (for Spatial Augmentations and Curriculum in Data)\n",
    "# ----------------------------\n",
    "class MemoryEfficientHSIDataset(Dataset):\n",
    "    def __init__(self, files, patch_size, noise_level=30,\n",
    "                 patches_per_file=200, target_bands=None, augment=True, \n",
    "                 dataset_type=\"unknown\", train_crop_size=1024, scales=None):\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.patch_size = patch_size\n",
    "        self.noise_level = noise_level / 255.0\n",
    "        self.patches_per_file = patches_per_file\n",
    "        self.augment = augment\n",
    "        self.target_bands = target_bands if target_bands else 31  # ICVL default: 31 bands\n",
    "        self.dataset_type = dataset_type\n",
    "        self.synthetic_count = 0\n",
    "        self.real_data_count = 0\n",
    "        self.train_crop_size = train_crop_size  # ICVL: center crop size\n",
    "        self.scales = scales if scales else [64, 32, 32]  # ICVL: multi-scale strides\n",
    "\n",
    "        # Keep existing file loading logic\n",
    "        self.file_info = []\n",
    "        print(f\"\\n=== Initializing {dataset_type.upper()} Dataset ===\")\n",
    "        print(f\"Noise Level (σ): {noise_level} -> {self.noise_level:.4f} (0-1 scale)\")\n",
    "        print(f\"Center crop size: {train_crop_size}x{train_crop_size}\")\n",
    "        print(f\"Patch size: {patch_size}x{patch_size}\")\n",
    "        print(f\"Multi-scale strides: {self.scales}\")\n",
    "\n",
    "        if len(files) > 0 and files[0] != 'synthetic':\n",
    "            print(f\"Attempting to load {len(files)} files...\")\n",
    "            successful_files = []\n",
    "            failed_files = []\n",
    "\n",
    "            for i, file_path in enumerate(files):\n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        print(f\"  [{i+1}/{len(files)}] Loading: {os.path.basename(file_path)}\")\n",
    "\n",
    "                        # Try h5py first (for MATLAB v7.3)\n",
    "                        h5py_success = False\n",
    "                        try:\n",
    "                            with h5py.File(file_path, 'r') as f:\n",
    "                                # FIXED: Look for 'rad' key specifically for ICVL data\n",
    "                                if 'rad' in f:\n",
    "                                    key = 'rad'\n",
    "                                    data = f[key]\n",
    "                                    cube_shape = data.shape\n",
    "                                    print(f\"    ✓ Shape: {cube_shape}, Key: '{key}' (HDF5)\")\n",
    "                                    self.file_info.append({'path': file_path, 'key': key, 'shape': cube_shape, 'format': 'h5py'})\n",
    "                                    successful_files.append(os.path.basename(file_path))\n",
    "                                    h5py_success = True\n",
    "                                else:\n",
    "                                    raise Exception(\"No 'rad' key found in file\")\n",
    "                        except Exception as h5_error:\n",
    "                            # Fallback to scipy.io for older MAT files\n",
    "                            if sio and not h5py_success:\n",
    "                                try:\n",
    "                                    mat = sio.loadmat(file_path)\n",
    "                                    key = [k for k in mat.keys() if not k.startswith('__')][0]\n",
    "                                    cube_shape = mat[key].shape\n",
    "                                    print(f\"    ✓ Shape: {cube_shape}, Key: '{key}' (scipy)\")\n",
    "                                    self.file_info.append({'path': file_path, 'key': key, 'shape': cube_shape, 'format': 'scipy'})\n",
    "                                    successful_files.append(os.path.basename(file_path))\n",
    "                                except Exception as scipy_error:\n",
    "                                    raise Exception(f\"Both h5py ({str(h5_error)[:50]}) and scipy ({str(scipy_error)[:50]}) failed\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ✗ Failed: {e}\")\n",
    "                        failed_files.append(os.path.basename(file_path))\n",
    "\n",
    "            print(f\"\\n{dataset_type.upper()} Dataset Summary:\")\n",
    "            print(f\"  ✓ Successfully loaded: {len(successful_files)} files\")\n",
    "            print(f\"  → Patches per file: {patches_per_file}\")\n",
    "            print(f\"  → Total iterations per epoch: {len(successful_files) * patches_per_file}\")\n",
    "            if not self.file_info:\n",
    "                print(f\"  → Will use SYNTHETIC data for {dataset_type}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.file_info)) * self.patches_per_file\n",
    "\n",
    "    def _generate_synthetic_data(self):\n",
    "        \"\"\"Generate BETTER synthetic HSI data with proper spectral correlation\"\"\"\n",
    "        self.synthetic_count += 1\n",
    "    \n",
    "        D, H, W = self.target_bands, self.patch_size, self.patch_size\n",
    "    \n",
    "        clean = torch.zeros(D, H, W)\n",
    "    \n",
    "        # Create different material signatures\n",
    "        n_materials = 3\n",
    "        for mat in range(n_materials):\n",
    "            signature = torch.randn(D)\n",
    "            signature = torch.softmax(signature, dim=0)\n",
    "    \n",
    "            center_h, center_w = random.randint(H//4, 3*H//4), random.randint(W//4, 3*W//4)\n",
    "            for h in range(H):\n",
    "                for w in range(W):\n",
    "                    dist = ((h - center_h)**2 + (w - center_w)**2) ** 0.5\n",
    "                    weight = torch.exp(torch.tensor(-dist / (H/4)))  # FIX: Convert to tensor\n",
    "                    clean[:, h, w] += weight * signature * random.uniform(0.3, 1.0)\n",
    "    \n",
    "        # Normalize to [0, 1]\n",
    "        clean = (clean - clean.min()) / (clean.max() - clean.min() + 1e-8)\n",
    "    \n",
    "        # Add noise in [0,1] range\n",
    "        noise = torch.randn_like(clean) * self.noise_level\n",
    "        noisy = torch.clamp(clean + noise, 0, 1)\n",
    "\n",
    "        return noisy, clean\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # REMOVED: Synthetic data fallback - force real data only\n",
    "        if not self.file_info:\n",
    "            raise RuntimeError(\n",
    "                f\"CRITICAL ERROR: No files loaded in {self.dataset_type} dataset!\\n\"\n",
    "                f\"file_info is empty. Check __init__ file loading.\"\n",
    "            )\n",
    "    \n",
    "        file_idx = idx // self.patches_per_file\n",
    "        file_info = self.file_info[file_idx % len(self.file_info)]\n",
    "    \n",
    "        # Load based on file format\n",
    "        if file_info.get('format') == 'h5py':\n",
    "            with h5py.File(file_info['path'], 'r') as f:\n",
    "                cube = np.array(f[file_info['key']]).astype(np.float32)\n",
    "        else:\n",
    "            mat = sio.loadmat(file_info['path'])\n",
    "            cube = mat[file_info['key']].astype(np.float32)\n",
    "        \n",
    "        # Ensure proper shape (H, W, D)\n",
    "        if cube.ndim == 3:\n",
    "            # ICVL data is in (D, H, W) format, need to transpose to (H, W, D)\n",
    "            if cube.shape[0] < min(cube.shape[1:]):  # Shape is (D, H, W)\n",
    "                cube = cube.transpose(1, 2, 0)  # Convert to (H, W, D)\n",
    "            elif cube.shape[2] > min(cube.shape[:2]):  # Shape is likely (H, W, D) already\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"ERROR: Unexpected cube shape {cube.shape} from file {file_info['path']}\\n\"\n",
    "                    f\"Cannot determine if format is (D,H,W) or (H,W,D)\"\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"ERROR: cube has {cube.ndim} dimensions, expected 3\\n\"\n",
    "                f\"Shape: {cube.shape}, File: {file_info['path']}\"\n",
    "            )\n",
    "                \n",
    "        H, W, D = cube.shape\n",
    "    \n",
    "        # ICVL: Center crop to train_crop_size (1024x1024)\n",
    "        if H > self.train_crop_size or W > self.train_crop_size:\n",
    "            start_h = (H - self.train_crop_size) // 2\n",
    "            start_w = (W - self.train_crop_size) // 2\n",
    "            cube = cube[start_h:start_h+self.train_crop_size,\n",
    "                       start_w:start_w+self.train_crop_size, :]\n",
    "            H, W = self.train_crop_size, self.train_crop_size\n",
    "    \n",
    "        # Check minimum size\n",
    "        if H < self.patch_size or W < self.patch_size:\n",
    "            raise ValueError(\n",
    "                f\"ERROR: Image too small after cropping\\n\"\n",
    "                f\"Size: {H}x{W}, Required: {self.patch_size}x{self.patch_size}\\n\"\n",
    "                f\"File: {file_info['path']}\"\n",
    "            )\n",
    "        \n",
    "        if D < 4:\n",
    "            raise ValueError(\n",
    "                f\"ERROR: Too few spectral bands: {D}\\n\"\n",
    "                f\"File: {file_info['path']}\"\n",
    "            )\n",
    "    \n",
    "        # ICVL: Randomly choose stride from multi-scale strides\n",
    "        stride = random.choice(self.scales)\n",
    "        \n",
    "        # Random patch extraction\n",
    "        start_h = random.randint(0, H - self.patch_size)\n",
    "        start_w = random.randint(0, W - self.patch_size)\n",
    "        cube = cube[start_h:start_h+self.patch_size,\n",
    "                   start_w:start_w+self.patch_size,\n",
    "                   :min(self.target_bands, D)]\n",
    "    \n",
    "        # ICVL: Normalize to [0, 1]\n",
    "        cube_min = cube.min()\n",
    "        cube_max = cube.max()\n",
    "        if cube_max > cube_min:\n",
    "            cube = (cube - cube_min) / (cube_max - cube_min)\n",
    "        else:\n",
    "            cube = np.clip(cube / (np.max(cube) + 1e-8), 0, 1)\n",
    "    \n",
    "        # Convert to (D, H, W) format for model\n",
    "        cube = cube.transpose(2, 0, 1)\n",
    "    \n",
    "        # ============================================================\n",
    "        # AUGMENTATION PIPELINE (single coherent block)\n",
    "        # ============================================================\n",
    "        if self.augment:\n",
    "            # Geometric augmentations (on numpy arrays)\n",
    "            # Random rotation (90° increments)\n",
    "            if random.random() < 0.5:\n",
    "                k = random.choice([1, 2, 3])\n",
    "                cube = np.rot90(cube, k, axes=(1, 2)).copy()\n",
    "            \n",
    "            # Horizontal flip\n",
    "            if random.random() < 0.5:\n",
    "                cube = np.flip(cube, axis=1).copy()\n",
    "            \n",
    "            # Vertical flip\n",
    "            if random.random() < 0.5:\n",
    "                cube = np.flip(cube, axis=2).copy()\n",
    "            \n",
    "            # Gaussian blur (spatial only) - applied to numpy before tensor conversion\n",
    "            if random.random() < 0.3:\n",
    "                sigma = random.uniform(0.3, 0.8)\n",
    "                for i in range(cube.shape[0]):\n",
    "                    cube[i] = scipy.ndimage.gaussian_filter(cube[i], sigma=sigma)\n",
    "    \n",
    "        # Convert to tensor AFTER all numpy augmentations\n",
    "        clean_tensor = torch.from_numpy(cube.copy())  # Extra safety: ensure contiguous\n",
    "    \n",
    "        # Add noise in [0,1] range\n",
    "        noise = torch.randn_like(clean_tensor) * self.noise_level\n",
    "        noisy_tensor = torch.clamp(clean_tensor + noise, 0, 1)\n",
    "    \n",
    "        # Spectral augmentation (on tensors, applied to both clean and noisy)\n",
    "        if self.augment and random.random() < 0.4:\n",
    "            scale = torch.tensor(\n",
    "                np.random.uniform(0.95, 1.05, size=(clean_tensor.shape[0], 1, 1)), \n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            clean_tensor = torch.clamp(clean_tensor * scale, 0, 1)\n",
    "            noisy_tensor = torch.clamp(noisy_tensor * scale, 0, 1)\n",
    "    \n",
    "        self.real_data_count += 1\n",
    "        return noisy_tensor.float(), clean_tensor.float()\n",
    "\n",
    "    def get_usage_stats(self):\n",
    "        \"\"\"Return statistics about data usage\"\"\"\n",
    "        total = self.real_data_count + self.synthetic_count\n",
    "        if total == 0:\n",
    "            return \"No data accessed yet\"\n",
    "        real_percent = (self.real_data_count / total) * 100\n",
    "        synthetic_percent = (self.synthetic_count / total) * 100\n",
    "        return f\"Real data: {self.real_data_count} ({real_percent:.1f}%), Synthetic: {self.synthetic_count} ({synthetic_percent:.1f}%)\"\n",
    "\n",
    "# ----------------------------\n",
    "# Metric Calculation Functions\n",
    "# ----------------------------\n",
    "def calculate_psnr(pred, target):\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    return 10 * torch.log10(1.0 / (mse + 1e-8))\n",
    "\n",
    "def calculate_ssim(pred_np, target_np):\n",
    "    \"\"\"Calculate SSIM across all spectral bands\"\"\"\n",
    "    if pred_np.ndim == 3:  # (D, H, W)\n",
    "        D, H, W = pred_np.shape\n",
    "        ssim_vals = []\n",
    "        for d in range(D):\n",
    "            try:\n",
    "                ssim_val = compare_ssim(pred_np[d], target_np[d], data_range=1.0)\n",
    "                ssim_vals.append(ssim_val)\n",
    "            except Exception:\n",
    "                ssim_vals.append(0.5)  # Fallback\n",
    "        return np.mean(ssim_vals)\n",
    "    else:\n",
    "        return compare_ssim(pred_np, target_np, data_range=1.0)\n",
    "\n",
    "def calculate_sam(pred_np, target_np):\n",
    "    \"\"\"Calculate Spectral Angle Mapper\"\"\"\n",
    "    eps = 1e-8\n",
    "    if pred_np.ndim == 3:  # (D, H, W)\n",
    "        pred_flat = pred_np.reshape(pred_np.shape[0], -1)  # (D, H*W)\n",
    "        target_flat = target_np.reshape(target_np.shape[0], -1)\n",
    "\n",
    "        dot = np.sum(pred_flat * target_flat, axis=0)\n",
    "        norm_pred = np.linalg.norm(pred_flat, axis=0) + eps\n",
    "        norm_target = np.linalg.norm(target_flat, axis=0) + eps\n",
    "        cos_angle = np.clip(dot / (norm_pred * norm_target), -1, 1)\n",
    "        angles = np.arccos(cos_angle)\n",
    "        return np.mean(angles)\n",
    "    else:\n",
    "        dot = np.sum(pred_np * target_np)\n",
    "        norm_pred = np.linalg.norm(pred_np) + eps\n",
    "        norm_target = np.linalg.norm(target_np) + eps\n",
    "        cos_angle = np.clip(dot / (norm_pred * norm_target), -1, 1)\n",
    "        return np.arccos(cos_angle)\n",
    "\n",
    "# ----------------------------\n",
    "# Visualizations Functions\n",
    "# ----------------------------\n",
    "def create_training_visualizations(train_losses, val_losses, val_psnrs, val_ssims, val_sams, learning_rates, save_dir, best_psnr):\n",
    "    \"\"\"Create comprehensive training visualizations and save them\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f'HSI Denoising Training Progress (Best PSNR: {best_psnr:.4f} dB)', fontsize=16)\n",
    "\n",
    "    # Calculate validation epochs\n",
    "    val_epochs = [i*5 for i in range(1, len(val_losses)+1)] if val_losses else []\n",
    "\n",
    "    # 1. Training and Validation Loss\n",
    "    axes[0, 0].plot(range(1, len(train_losses)+1), train_losses, 'b-', label='Training Loss', alpha=0.8)\n",
    "    if val_losses:\n",
    "        axes[0, 0].plot(val_epochs, val_losses, 'r-', label='Validation Loss', marker='o', markersize=3)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "\n",
    "    # 2. PSNR over Epochs\n",
    "    if val_psnrs:\n",
    "        axes[0, 1].plot(val_epochs, val_psnrs, 'r-', label='PSNR', marker='o', markersize=4)\n",
    "        axes[0, 1].axhline(y=40, color='purple', linestyle='--', label='Target (40 dB)', alpha=0.7)\n",
    "        axes[0, 1].axhline(y=best_psnr, color='orange', linestyle=':', label=f'Best ({best_psnr:.2f} dB)', alpha=0.7)\n",
    "        axes[0, 1].legend()\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('PSNR (dB)')\n",
    "    axes[0, 1].set_title('PSNR over Epochs')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. SSIM over Epochs\n",
    "    if val_ssims:\n",
    "        axes[0, 2].plot(val_epochs, val_ssims, 'r-', label='SSIM', marker='o', markersize=4)\n",
    "        axes[0, 2].axhline(y=1.0, color='purple', linestyle='--', label='Perfect (1.0)', alpha=0.7)\n",
    "        axes[0, 2].legend()\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('SSIM')\n",
    "    axes[0, 2].set_title('SSIM over Epochs')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].set_ylim([0, 1.1])\n",
    "\n",
    "    # 4. SAM over Epochs\n",
    "    if val_sams:\n",
    "        axes[1, 0].plot(val_epochs, val_sams, 'r-', label='SAM', marker='o', markersize=4)\n",
    "        axes[1, 0].axhline(y=0, color='purple', linestyle='--', label='Perfect (0.0)', alpha=0.7)\n",
    "        axes[1, 0].legend()\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('SAM (radians)')\n",
    "    axes[1, 0].set_title('SAM over Epochs')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Learning Rate Changes\n",
    "    axes[1, 1].plot(range(1, len(learning_rates)+1), learning_rates, 'purple', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].set_title('Learning Rate Schedule')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_yscale('log')\n",
    "\n",
    "    # 6. Combined Metrics Summary\n",
    "    if val_psnrs and val_ssims and val_sams:\n",
    "        # Normalize metrics for combined view\n",
    "        norm_psnr = np.array(val_psnrs) / 50.0  # Normalize to ~1\n",
    "        norm_ssim = np.array(val_ssims)  # Already 0-1\n",
    "        norm_sam = 1 - np.array(val_sams)  # Invert so higher is better\n",
    "\n",
    "        axes[1, 2].plot(val_epochs, norm_psnr, 'b-', label='PSNR (normalized)', alpha=0.7, linewidth=2)\n",
    "        axes[1, 2].plot(val_epochs, norm_ssim, 'r-', label='SSIM', alpha=0.7, linewidth=2)\n",
    "        axes[1, 2].plot(val_epochs, norm_sam, 'g-', label='1-SAM', alpha=0.7, linewidth=2)\n",
    "        axes[1, 2].legend()\n",
    "\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Normalized Metrics')\n",
    "    axes[1, 2].set_title('Combined Metrics Overview')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].set_ylim([0, 1.1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(save_dir, 'training_progress.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Training plots saved to: {plot_path}\")\n",
    "\n",
    "    # Also save as PDF for high quality\n",
    "    plot_path_pdf = os.path.join(save_dir, 'training_progress.pdf')\n",
    "    plt.savefig(plot_path_pdf, bbox_inches='tight', facecolor='white')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def print_final_scores(val_psnrs, val_ssims, val_sams, best_psnr, train_files, val_files, save_dir):\n",
    "    \"\"\"Print comprehensive final scores\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Best Validation PSNR: {best_psnr:.4f} dB\")\n",
    "    print(\"Training finished. Models saved with comprehensive metadata.\")\n",
    "    print(f\"Results saved to: {save_dir}\")\n",
    "\n",
    "    print(\"\\n=== FINAL SCORES ===\")\n",
    "    if val_psnrs and val_ssims and val_sams:\n",
    "        # Handle single validation set\n",
    "        final_psnr = val_psnrs[-1]\n",
    "        final_ssim = val_ssims[-1]\n",
    "        final_sam = val_sams[-1]\n",
    "\n",
    "        print(f\"Final Validation PSNR: {final_psnr:.4f} dB\")\n",
    "        print(f\"Final Validation SSIM: {final_ssim:.4f}\")\n",
    "        print(f\"Final Validation SAM: {final_sam:.4f} radians\")\n",
    "        \n",
    "        # Additional statistics\n",
    "        print(f\"\\nValidation History:\")\n",
    "        print(f\"  Mean PSNR: {np.mean(val_psnrs):.4f} ± {np.std(val_psnrs):.4f} dB\")\n",
    "        print(f\"  Mean SSIM: {np.mean(val_ssims):.4f} ± {np.std(val_ssims):.4f}\")\n",
    "        print(f\"  Mean SAM:  {np.mean(val_sams):.4f} ± {np.std(val_sams):.4f} radians\")\n",
    "        print(f\"  Best PSNR: {max(val_psnrs):.4f} dB\")\n",
    "        print(f\"  Worst PSNR: {min(val_psnrs):.4f} dB\")\n",
    "\n",
    "    # Count actual files (filter out 'synthetic')\n",
    "    real_train_files = [f for f in train_files if f != 'synthetic']\n",
    "    real_val_files = [f for f in val_files if f != 'synthetic']\n",
    "\n",
    "    print(f\"\\nDataset Information:\")\n",
    "    print(f\"  Training files: {len(real_train_files)}\")\n",
    "    print(f\"  Validation files: {len(real_val_files)}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "#WarmupCosineScheduler\n",
    "class WarmupCosineScheduler:\n",
    "    \"\"\"\n",
    "    IMPROVED: LR scheduler with warmup + peak plateau + cosine annealing\n",
    "    Stays at peak LR longer for better exploration before decay\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_epochs, peak_epochs, total_epochs, lr_max, lr_min):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs      # Warmup period\n",
    "        self.peak_epochs = peak_epochs          # NEW: How long to stay at peak\n",
    "        self.total_epochs = total_epochs\n",
    "        self.lr_max = lr_max\n",
    "        self.lr_min = lr_min\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Update learning rate and return current LR\"\"\"\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "        if self.current_epoch <= self.warmup_epochs:\n",
    "            # Phase 1: Warmup (linear increase from 0 to lr_max)\n",
    "            lr = self.lr_max * (self.current_epoch / self.warmup_epochs)\n",
    "            \n",
    "        elif self.current_epoch <= self.peak_epochs:\n",
    "            # Phase 2: Peak plateau (stay at lr_max for exploration)\n",
    "            lr = self.lr_max\n",
    "            \n",
    "        else:\n",
    "            # Phase 3: Cosine annealing (smooth decay from lr_max to lr_min)\n",
    "            progress = (self.current_epoch - self.peak_epochs) / (self.total_epochs - self.peak_epochs)\n",
    "            lr = self.lr_min + (self.lr_max - self.lr_min) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        # Update optimizer learning rate\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        return lr\n",
    "    \n",
    "    def state_dict(self):\n",
    "        \"\"\"For checkpoint saving compatibility\"\"\"\n",
    "        return {\n",
    "            'current_epoch': self.current_epoch,\n",
    "            'warmup_epochs': self.warmup_epochs,\n",
    "            'peak_epochs': self.peak_epochs,  # Save peak_epochs\n",
    "            'total_epochs': self.total_epochs,\n",
    "            'lr_max': self.lr_max,\n",
    "            'lr_min': self.lr_min\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"For checkpoint loading compatibility\"\"\"\n",
    "        self.current_epoch = state_dict['current_epoch']\n",
    "        self.warmup_epochs = state_dict['warmup_epochs']\n",
    "        self.peak_epochs = state_dict.get('peak_epochs', self.warmup_epochs)  # Backward compatibility\n",
    "        self.total_epochs = state_dict['total_epochs']\n",
    "        self.lr_max = state_dict['lr_max']\n",
    "        self.lr_min = state_dict['lr_min']\n",
    "\n",
    "# ----------------------------\n",
    "# Memory-Optimized Main Function with Comprehensive Training\n",
    "# ----------------------------\n",
    "# Complete main() function with all modifications integrated\n",
    "# Keeps all original components, adds deep supervision and gradient accumulation\n",
    "\n",
    "def main():\n",
    "    print(\"=== SOTA-Enhanced Memory-Optimized HSI Denoising ===\")\n",
    "    print(\"4-stage SST Transformer with hybrid attention and deep supervision\")\n",
    "    print(\"Target: PSNR > 40 dB with efficient memory usage\")\n",
    "\n",
    "    # Enable TF32 for RTX 40-series (faster matrix operations)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Enable cuDNN benchmarking for consistent input sizes\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # UPDATED CONFIG for 4-stage\n",
    "    config = {\n",
    "        'patch_size': 64,           \n",
    "        'batch_size': 2,             \n",
    "        'base_dim': 64,              \n",
    "        'noise_level': 30,           \n",
    "        'lr_max': 1.5e-4,          \n",
    "        'lr_min': 1e-6,              \n",
    "        'total_epochs': 300,         \n",
    "        'patience': 20,             \n",
    "        'weight_decay': 2e-4,       \n",
    "        'val_split': 0.1,            \n",
    "        'seed': 42,                  \n",
    "        'target_bands': 31,\n",
    "        'gradient_accumulation': 8,   # Simulate batch_size= batch_size * gradient_accumulation\n",
    "        'train_crop_size': 1024,\n",
    "        'scales': [64, 32, 32], \n",
    "        'warmup_epochs': 10,\n",
    "    }\n",
    "\n",
    "    # Set seeds\n",
    "    torch.manual_seed(config['seed'])\n",
    "    random.seed(config['seed'])\n",
    "    np.random.seed(config['seed'])\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "    data_dir = '/workspace/icvl_part/train'\n",
    "    save_dir = './HSI_denoising_ICVL_resultsV15_noise30'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n=== DATA DISCOVERY ===\")\n",
    "    print(f\"Looking for HSI data in: {data_dir}\")\n",
    "\n",
    "    # Dataset preparation with detailed logging\n",
    "    if os.path.exists(data_dir):\n",
    "        try:\n",
    "            mat_files = glob.glob(os.path.join(data_dir, '*.mat'))\n",
    "            print(f\"Found {len(mat_files)} .mat files in directory\")\n",
    "\n",
    "            if len(mat_files) >= 10:\n",
    "                n_val = 10\n",
    "                train_files = mat_files[:-n_val]  # First 90 for training\n",
    "                val_files = mat_files[-n_val:]     # Last 10 for validation\n",
    "                print(f\"ICVL Split: {len(train_files)} training files, {len(val_files)} validation files\")\n",
    "            else:\n",
    "                print(f\"Warning: Found only {len(mat_files)} files. Need at least 10 for ICVL setup.\")\n",
    "                print(\"Using what's available with proportional split...\")\n",
    "                n_val = max(1, len(mat_files) // 10)\n",
    "                train_files = mat_files[:-n_val]\n",
    "                val_files = mat_files[-n_val:]\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing data directory: {e}\")\n",
    "            train_files, val_files = ['synthetic'], ['synthetic']\n",
    "    else:\n",
    "        print(f\"Data directory not found: {data_dir}\")\n",
    "        train_files, val_files = ['synthetic'], ['synthetic']\n",
    "\n",
    "    # Create datasets with enhanced logging\n",
    "    train_dataset = MemoryEfficientHSIDataset(\n",
    "        train_files,\n",
    "        patch_size=config['patch_size'],\n",
    "        noise_level=config['noise_level'],\n",
    "        patches_per_file=15,\n",
    "        target_bands=config['target_bands'],\n",
    "        augment=True,\n",
    "        dataset_type=\"training\",\n",
    "        train_crop_size=config['train_crop_size'],\n",
    "        scales=config['scales']\n",
    "    )\n",
    "\n",
    "    val_dataset_synthetic = MemoryEfficientHSIDataset(\n",
    "        val_files,\n",
    "        patch_size=config['patch_size'],\n",
    "        noise_level=config['noise_level'],\n",
    "        patches_per_file=10,\n",
    "        target_bands=config['target_bands'],\n",
    "        augment=False,\n",
    "        dataset_type=\"validation_synthetic\",\n",
    "        train_crop_size=config['train_crop_size'],\n",
    "        scales=[config['patch_size']]  # No multi-scale for validation\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'],\n",
    "                             shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader_synthetic = DataLoader(val_dataset_synthetic, batch_size=1,\n",
    "                           shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Pre-training data verification\n",
    "    print(\"\\n=== PRE-TRAINING DATA VERIFICATION ===\")\n",
    "    print(\"Testing first few training samples...\")\n",
    "\n",
    "    test_count = 3\n",
    "    samples_verified = 0\n",
    "    \n",
    "    for i, (noisy, clean) in enumerate(train_loader):\n",
    "        if i >= test_count:\n",
    "            break\n",
    "        print(f\"Sample {i+1}: Noisy shape: {noisy.shape}, Clean shape: {clean.shape}\")\n",
    "        samples_verified += noisy.shape[0]\n",
    "    \n",
    "    if train_dataset.file_info:\n",
    "        print(f\"✓ Successfully verified {samples_verified} samples from {len(train_dataset.file_info)} real data files\")\n",
    "    else:\n",
    "        print(f\"⚠ Using synthetic data (no real files loaded)\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Model setup - NEW: SOTA 4-stage Swin Transformer\n",
    "    model = MemoryOptimizedUNet(\n",
    "        in_channels=1,\n",
    "        base_dim=config['base_dim'],\n",
    "        window_sizes=[4,8,16],\n",
    "        num_bands=config['target_bands']\n",
    "    ).to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model parameters: {total_params / 1e6:.2f}M\")\n",
    "    #print(f\"Architecture: 4-stage SST Transformer + Hybrid Attention + Deep Supervision\")\n",
    "    print(f\"Architecture: 4-stage SST Transformer + Hybrid Attention\")\n",
    "\n",
    "    # Initialize EMA\n",
    "    ema = ExponentialMovingAverage(model.parameters(), decay=0.999)\n",
    "    print(\"EMA initialized with decay=0.999\")\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = MemoryEfficientLoss(device=device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['lr_max'],\n",
    "                            betas=(0.9, 0.999), eps=1e-8,\n",
    "                           weight_decay=config['weight_decay'])\n",
    "\n",
    "    # Scheduler\n",
    "    #from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "    #scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=config['lr_min'])\n",
    "    #scheduler = CosineAnnealingLR(optimizer, T_max=config['total_epochs'], eta_min=config['lr_min'])\n",
    "    scheduler = WarmupCosineScheduler(\n",
    "        optimizer=optimizer,\n",
    "        warmup_epochs=config['warmup_epochs'],\n",
    "        peak_epochs=50,\n",
    "        total_epochs=config['total_epochs'],\n",
    "        lr_max=config['lr_max'],\n",
    "        lr_min=config['lr_min']\n",
    "    )\n",
    "    print(f\"Scheduler: Warmup ({config['warmup_epochs']} epochs) + Cosine Annealing\")\n",
    "    # Mixed precision\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    # Training tracking\n",
    "    best_psnr = 0\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses, val_psnrs, val_ssims, val_sams = [], [], [], [], []\n",
    "    learning_rates = []\n",
    "\n",
    "    # NEW: Gradient accumulation setup\n",
    "    accumulation_steps = config.get('gradient_accumulation', 1)\n",
    "    print(f\"Using gradient accumulation: {accumulation_steps} steps (effective batch size: {config['batch_size'] * accumulation_steps})\")\n",
    "\n",
    "    print(f\"Starting training for {config['total_epochs']} epochs...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for epoch in range(1, config['total_epochs'] + 1):\n",
    "        # ============================================================\n",
    "        # TRAINING PHASE - WITH DEEP SUPERVISION & GRADIENT ACCUMULATION\n",
    "        # ============================================================\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_main_loss = 0\n",
    "        epoch_deep_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        optimizer.zero_grad()  # Initialize outside loop\n",
    "\n",
    "        for batch_idx, (noisy, clean) in enumerate(train_loader):\n",
    "            noisy = noisy.float().to(device, non_blocking=True)\n",
    "            clean = clean.float().to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                # NEW: Get main output and deep supervision outputs\n",
    "                output, deep_outputs = model(noisy, return_deep_sup=True)\n",
    "                \n",
    "                # Main loss\n",
    "                main_loss = criterion(output, clean)\n",
    "                \n",
    "                # NEW: Deep supervision losses (weighted progressively)\n",
    "                deep_loss = 0\n",
    "                if len(deep_outputs) > 0:\n",
    "                    weights = [0.4, 0.3, 0.3]  # Weights for 3 auxiliary outputs\n",
    "                    for i, deep_out in enumerate(deep_outputs):\n",
    "                        deep_loss += weights[i] * criterion(deep_out, clean)\n",
    "                \n",
    "                # Combined loss with gradient accumulation scaling\n",
    "                #deep supervision loss disabled\n",
    "                #loss = (main_loss + 0.3 * deep_loss) / accumulation_steps\n",
    "                loss = main_loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # NEW: Step optimizer only every accumulation_steps batches\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                # Gradient clipping for stability\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                #EMA\n",
    "                ema.update()\n",
    "\n",
    "            # Logging (unscale loss for accurate reporting)\n",
    "            epoch_loss += loss.item() * accumulation_steps\n",
    "            epoch_main_loss += main_loss.item()\n",
    "            epoch_deep_loss += deep_loss.item() if isinstance(deep_loss, torch.Tensor) else deep_loss\n",
    "            num_batches += 1\n",
    "\n",
    "            # Memory management\n",
    "            if num_batches % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "        current_lr = scheduler.step()\n",
    "        learning_rates.append(current_lr)\n",
    "        train_loss = epoch_loss / num_batches\n",
    "        train_main_loss = epoch_main_loss / num_batches\n",
    "        train_deep_loss = epoch_deep_loss / num_batches\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Data usage statistics every 50 epochs\n",
    "        #if epoch % 50 == 0:\n",
    "            #print(f\"  Data usage after {epoch} epochs: {train_dataset.get_usage_stats()}\")\n",
    "            #print(f\"  Loss breakdown - Main: {train_main_loss:.6f}, Deep: {train_deep_loss:.6f}\")\n",
    "\n",
    "        # ============================================================\n",
    "        # VALIDATION PHASE - WITHOUT DEEP SUPERVISION\n",
    "        # ============================================================\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "\n",
    "            #EMA\n",
    "            with ema.average_parameters():\n",
    "                val_loss = 0\n",
    "                total_psnr = 0\n",
    "                total_ssim = 0\n",
    "                total_sam = 0\n",
    "                num_val_batches = 0\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    for noisy, clean in val_loader_synthetic:\n",
    "                        noisy, clean = noisy.to(device, non_blocking=True), clean.to(device, non_blocking=True)\n",
    "            \n",
    "                        with torch.amp.autocast('cuda'):\n",
    "                            output = model(noisy, return_deep_sup=False)\n",
    "                            loss = criterion(output, clean)\n",
    "            \n",
    "                        val_loss += loss.item()\n",
    "                        psnr = calculate_psnr(output, clean)\n",
    "                        total_psnr += psnr.item()\n",
    "            \n",
    "                        output_np = output.squeeze(0).cpu().numpy() if output.dim() == 4 else output.squeeze(0).squeeze(0).cpu().numpy()\n",
    "                        clean_np = clean.squeeze(0).cpu().numpy() if clean.dim() == 4 else clean.squeeze(0).squeeze(0).cpu().numpy()\n",
    "            \n",
    "                        ssim_val = calculate_ssim(output_np, clean_np)\n",
    "                        sam_val = calculate_sam(output_np, clean_np)\n",
    "            \n",
    "                        total_ssim += ssim_val\n",
    "                        total_sam += sam_val\n",
    "                        num_val_batches += 1\n",
    "            \n",
    "                val_loss /= num_val_batches\n",
    "                val_psnr = total_psnr / num_val_batches\n",
    "                val_ssim = total_ssim / num_val_batches\n",
    "                val_sam = total_sam / num_val_batches\n",
    "            \n",
    "                # **FIX: Append metrics to tracking lists**\n",
    "                val_losses.append(val_loss)\n",
    "                val_psnrs.append(val_psnr)\n",
    "                val_ssims.append(val_ssim)\n",
    "                val_sams.append(val_sam)\n",
    "            \n",
    "                # Check improvement\n",
    "                if val_psnr > best_psnr:\n",
    "                    best_psnr = val_psnr\n",
    "                    patience_counter = 0\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        #EMA\n",
    "                        'ema_state_dict': ema.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'best_psnr': best_psnr,\n",
    "                        'config': config\n",
    "                    }, os.path.join(save_dir, 'best_model.pth'))\n",
    "                    print(f\"  *** New best model saved! PSNR: {best_psnr:.4f} dB ***\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "            \n",
    "                print(f\"Epoch {epoch:4d} | LR: {current_lr:.2e} | Train Loss: {train_loss:.6f} | \"\n",
    "                      f\"Val Loss: {val_loss:.6f} | Val PSNR: {val_psnr:.4f} | \"\n",
    "                      f\"Val SSIM: {val_ssim:.4f} | Val SAM: {val_sam:.4f} | Best: {best_psnr:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= config['patience']:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best PSNR: {best_psnr:.4f} dB\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Epoch {epoch:4d} | LR: {current_lr:.2e} | Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "        # Clear memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # Save complete model with all metadata\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'best_psnr': best_psnr,\n",
    "        'config': config,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_psnrs': val_psnrs,\n",
    "        'val_ssims': val_ssims,\n",
    "        'val_sams': val_sams,\n",
    "        'learning_rates': learning_rates,\n",
    "        'architecture': '4-stage SST Transformer + Hybrid Attention + Deep Supervision',\n",
    "        'total_params': total_params\n",
    "    }, os.path.join(save_dir, 'enhanced_denoising_pipeline_full.pth'))\n",
    "\n",
    "    # Create comprehensive visualizations\n",
    "    print(\"\\nCreating training visualizations...\")\n",
    "    create_training_visualizations(\n",
    "        train_losses=train_losses,\n",
    "        val_losses=val_losses,\n",
    "        val_psnrs=val_psnrs,\n",
    "        val_ssims=val_ssims,\n",
    "        val_sams=val_sams,\n",
    "        learning_rates=learning_rates,\n",
    "        save_dir=save_dir,\n",
    "        best_psnr=best_psnr\n",
    "    )\n",
    "\n",
    "    # Print final comprehensive scores\n",
    "    print_final_scores(\n",
    "        val_psnrs=val_psnrs,\n",
    "        val_ssims=val_ssims,\n",
    "        val_sams=val_sams,\n",
    "        best_psnr=best_psnr,\n",
    "        train_files=train_files,\n",
    "        val_files=val_files,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL ARCHITECTURE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model: 4-stage SST Transformer U-Net\")\n",
    "    print(f\"Hybrid attention: SST + Spectral + FusedBottleneck\")\n",
    "    print(f\"Deep supervision: 3 auxiliary losses\")\n",
    "    print(f\"Total parameters: {total_params / 1e6:.2f}M\")\n",
    "    print(f\"Best validation PSNR: {best_psnr:.4f} dB\")\n",
    "    print(f\"Training epochs: {epoch}\")\n",
    "    print(f\"Results directory: {save_dir}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return model, best_psnr\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ad265-f748-4157-ace6-b4add3ea28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3066e27-5245-4e8f-be76-ba2a6a954f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HSI DENOISING MODEL TESTING - ICVL Protocol\n",
      "Test crops: 512×512×31 with training-matched normalization\n",
      "================================================================================\n",
      "Using device: cuda\n",
      "GPU Memory: 12.5 GB\n",
      "\n",
      "Loading trained model...\n",
      "Loading full model with metadata...\n",
      "\n",
      "✓ Adjusted 18 spectral_pos_embed parameters:\n",
      "    enc_stage2.blocks.0.spectral_attn.spectral_pos_embed: torch.Size([1, 16, 128]) -> torch.Size([1, 15, 128])\n",
      "    enc_stage2.blocks.1.spectral_attn.spectral_pos_embed: torch.Size([1, 16, 128]) -> torch.Size([1, 15, 128])\n",
      "    enc_stage3.blocks.0.spectral_attn.spectral_pos_embed: torch.Size([1, 8, 256]) -> torch.Size([1, 7, 256])\n",
      "    enc_stage3.blocks.1.spectral_attn.spectral_pos_embed: torch.Size([1, 8, 256]) -> torch.Size([1, 7, 256])\n",
      "    enc_stage3.blocks.2.spectral_attn.spectral_pos_embed: torch.Size([1, 8, 256]) -> torch.Size([1, 7, 256])\n",
      "    ... and 13 more\n",
      "\n",
      "✓ Model weights loaded successfully!\n",
      "Model loaded successfully!\n",
      "Parameters: 53.67M\n",
      "Base dimension: 64\n",
      "Target bands: 31\n",
      "Training noise level: σ=50\n",
      "Training best PSNR: 37.28 dB\n",
      "Loading test data from: /home/habib/Documents/workspace/icvl_part/test_gauss\n",
      "Found 50 test files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test files: 100%|███████████████████████| 50/50 [00:45<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA LOADING SUMMARY (ICVL Protocol: 512×512×31)\n",
      "============================================================\n",
      "✓ Successfully loaded: 50/50 files\n",
      "============================================================\n",
      "\n",
      "Loaded 50 test images (512×512×31)\n",
      "Adding Gaussian noise (σ=50) to 50 samples...\n",
      "Noise protocol: MATCH TRAINING (scale [0,1] -> [0,255], add noise, clip, rescale)\n",
      "✓ Successfully processed: 50/50 samples\n",
      "\n",
      "\n",
      "Noise Analysis:\n",
      "Noise types distribution: {'unknown': 50}\n",
      "Actual noise levels - Mean: 0.0000, Std: 0.0000, Range: [0.0000, 0.0000]\n",
      "Sample noise verification - Mean: 0.0278, Std: 0.1653, Max: 0.9747\n",
      "Created 50 test samples with static noise σ=50\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL ON ALL SAMPLES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing samples: 100%|██████████████████████████| 50/50 [05:03<00:00,  6.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING COMPLETED - AVERAGE SCORES\n",
      "============================================================\n",
      "Total samples tested: 50/50\n",
      "\n",
      "Average Metrics:\n",
      "  • Loss:            0.000108 ± 0.000038\n",
      "  • Input PSNR:      15.75 ± 0.76 dB\n",
      "  • Output PSNR:     39.93 ± 1.56 dB\n",
      "  • PSNR Improvement: 24.17 ± 1.66 dB\n",
      "  • SSIM:            0.9517 ± 0.0107\n",
      "  • SAM:             0.0571 ± 0.0613\n",
      "\n",
      "Performance Range:\n",
      "  • Best PSNR:  44.55 dB (Labtest_0910-1504.mat)\n",
      "  • Worst PSNR: 36.53 dB (nachal_0823-1149.mat)\n",
      "============================================================\n",
      "\n",
      "\n",
      "Successfully tested 50 samples\n",
      "\n",
      "============================================================\n",
      "SUMMARY STATISTICS:\n",
      "============================================================\n",
      "Number of test samples: 50\n",
      "Average Test Loss: 0.000108 ± 0.000038\n",
      "Average PSNR:      39.928 ± 1.561 dB\n",
      "Average SSIM:      0.9517 ± 0.0107\n",
      "Average SAM:       0.0571 ± 0.0613\n",
      "Best PSNR:  Labtest_0910-1504.mat (44.554 dB)\n",
      "Worst PSNR: nachal_0823-1149.mat (36.532 dB)\n",
      "============================================================\n",
      "Creating comprehensive visualizations...\n",
      "Creating visualization for Labtest_0910-1504.mat\n",
      "  Using generic RGB bands: [22, 13, 5]\n",
      "  Saved visualization: sample_best_performance_Labtest_0910-1504.png\n",
      "Creating visualization for objects_0924-1557.mat\n",
      "  Using generic RGB bands: [22, 13, 5]\n",
      "  Saved visualization: sample_median_performance_objects_0924-1557.png\n",
      "Creating visualization for nachal_0823-1149.mat\n",
      "  Using generic RGB bands: [22, 13, 5]\n",
      "  Saved visualization: sample_challenging_case_nachal_0823-1149.png\n",
      "Creating spectral analysis...\n",
      "Saving visualization results...\n",
      "Results saved to ./HSI_denoising_ICVL_resultsV15/test_results\n",
      "• Comprehensive plots: comprehensive_test_results.png\n",
      "• Detailed analysis: detailed_analysis_plots.png\n",
      "• Spectral analysis: spectral_analysis.png\n",
      "• Sample visualizations: sample_*.png\n",
      "• Testing Mode: Static noise (σ=0.19607843137254902)\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE TEST RESULTS SUMMARY\n",
      "================================================================================\n",
      "Test Configuration:\n",
      "  • Test Directory: /home/habib/Documents/workspace/icvl_part/test_gauss\n",
      "  • Model Path: ./HSI_denoising_ICVL_resultsV15\n",
      "  • Total Samples: 50\n",
      "  • Unique Images: 50\n",
      "  • Noise Level: σ=0.19607843137254902\n",
      "  • Device: cuda\n",
      "\n",
      "Performance Metrics:\n",
      "  • Mean PSNR: 39.93 ± 1.56 dB\n",
      "  • Mean Improvement: 24.17 dB\n",
      "  • Max PSNR: 44.55 dB\n",
      "  • Min PSNR: 36.53 dB\n",
      "  • Mean SSIM: 0.9517 ± 0.0107\n",
      "  • Mean SAM: 0.0571 ± 0.0613\n",
      "\n",
      "Target Achievement (PSNR > 40 dB):\n",
      "  • Success Rate: 54.0% (27 / 50 samples)\n",
      "  • Status: GOOD\n",
      "\n",
      "Model Architecture:\n",
      "  • Parameters: 53.67M\n",
      "  • Base Dimension: 64\n",
      "  • Input Bands: 31\n",
      "  • Patch Size: 64\n",
      "\n",
      "Generalization Analysis:\n",
      "  • Training PSNR: 37.28 dB\n",
      "  • Test PSNR: 39.93 dB\n",
      "  • Difference: 2.65 dB\n",
      "  • Generalization: Good\n",
      "\n",
      "================================================================================\n",
      "Final Average PSNR: 39.928 ± 1.561 dB\n",
      "Final Average SSIM: 0.9517 ± 0.0107\n",
      "Final Average SAM:  0.0571 ± 0.0613\n",
      "\n",
      "Results saved to: ./HSI_denoising_ICVL_resultsV15/test_results\n",
      "================================================================================\n",
      "TESTING COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from einops import rearrange\n",
    "warnings.filterwarnings('ignore')\n",
    "import h5py\n",
    "try:\n",
    "    import scipy.io as sio\n",
    "except ImportError:\n",
    "    sio = None\n",
    "\n",
    "# ----------------------------\n",
    "# Memory-Efficient Utilities\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# Memory-Efficient Utilities\n",
    "# ----------------------------\n",
    "class LayerNormChannel3d(nn.Module):\n",
    "    \"\"\"Lightweight channel normalization\"\"\"\n",
    "    def __init__(self, num_channels: int = None, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.num_channels = num_channels\n",
    "        self.gn = None\n",
    "        if num_channels is not None:\n",
    "            self.gn = nn.GroupNorm(1, num_channels, eps=eps, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        C = x.shape[1]\n",
    "        if self.gn is None or C != self.num_channels:\n",
    "            self.num_channels = C\n",
    "            self.gn = nn.GroupNorm(1, C, eps=self.eps, affine=True).to(x.device)\n",
    "        return self.gn(x)\n",
    "\n",
    "def depthwise_conv3d(channels: int, kernel_size: int = 3, stride: int = 1, padding: int = 1, dilation: int = 1):\n",
    "    return nn.Conv3d(channels, channels, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, groups=channels, bias=True, dilation=dilation)\n",
    "\n",
    "class EfficientChannelAttention(nn.Module):\n",
    "    \"\"\"Memory-efficient channel attention\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        hidden = max(4, channels // reduction)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, hidden, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        y = self.avg_pool(x).view(B, C)\n",
    "        y = self.fc(y).view(B, C, 1, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class AdaptiveDropout3d(nn.Module):\n",
    "    \"\"\"Adaptive dropout that adjusts rate based on a factor (e.g., layer depth)\"\"\"\n",
    "    def __init__(self, base_drop=0.25, factor=1.0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout3d(base_drop * factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x)\n",
    "\n",
    "# ----------------------------\n",
    "# Memory-Optimized Blocks\n",
    "# ----------------------------\n",
    "class PatchMerging3D(nn.Module):\n",
    "    \"\"\"\n",
    "    3D Patch Merging Layer (downsampling)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(8 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, D, H, W)\n",
    "        \"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "\n",
    "        # Pad if needed\n",
    "        pad_d = (2 - D % 2) % 2\n",
    "        pad_h = (2 - H % 2) % 2\n",
    "        pad_w = (2 - W % 2) % 2\n",
    "\n",
    "        if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h, 0, pad_d))\n",
    "            D, H, W = x.shape[2:]\n",
    "\n",
    "        # Convert to (B, D, H, W, C)\n",
    "        x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
    "\n",
    "        # Downsample by merging 2x2x2 patches\n",
    "        x0 = x[:, 0::2, 0::2, 0::2, :]  # (B, D/2, H/2, W/2, C)\n",
    "        x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, 0::2, :]\n",
    "        x4 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x5 = x[:, 1::2, 0::2, 1::2, :]\n",
    "        x6 = x[:, 0::2, 1::2, 1::2, :]\n",
    "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "\n",
    "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)  # (B, D/2, H/2, W/2, 8*C)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)  # (B, D/2, H/2, W/2, 2*C)\n",
    "\n",
    "        # Convert back to (B, C, D, H, W)\n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "class PatchMerging3D(nn.Module):\n",
    "    \"\"\"3D Patch Merging with better handling\"\"\"\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(8 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Pad to multiples of 2\n",
    "        pad_d = (2 - D % 2) % 2\n",
    "        pad_h = (2 - H % 2) % 2\n",
    "        pad_w = (2 - W % 2) % 2\n",
    "        \n",
    "        if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h, 0, pad_d))\n",
    "            D, H, W = x.shape[2:]\n",
    "\n",
    "        x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        \n",
    "        x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, 0::2, :]\n",
    "        x4 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x5 = x[:, 1::2, 0::2, 1::2, :]\n",
    "        x6 = x[:, 0::2, 1::2, 1::2, :]\n",
    "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "        \n",
    "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        \n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "class PatchExpanding3D(nn.Module):\n",
    "    \"\"\"3D Patch Expanding for decoder\"\"\"\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.expand = nn.Linear(dim, 4 * dim, bias=False)\n",
    "        self.norm = norm_layer(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        x = self.norm(x)\n",
    "        x = self.expand(x)\n",
    "        \n",
    "        x = rearrange(x, 'b d h w (p1 p2 p3 c) -> b (d p1) (h p2) (w p3) c', \n",
    "                     p1=2, p2=2, p3=2, c=C//2)\n",
    "        \n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "class SpectralAttentionModule(nn.Module):\n",
    "    \"\"\"Dedicated spectral attention for bottleneck - HYBRID ATTENTION\"\"\"\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Conv3d(dim, dim * 3, 1)\n",
    "        self.proj = nn.Conv3d(dim, dim, 1)\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        qkv = self.qkv(x_norm)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=1)\n",
    "        \n",
    "        # Reshape for spectral attention: (B*H*W, num_heads, D, head_dim)\n",
    "        q = q.permute(0, 3, 4, 2, 1).reshape(B * H * W, D, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.permute(0, 3, 4, 2, 1).reshape(B * H * W, D, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 3, 4, 2, 1).reshape(B * H * W, D, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Spectral attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = (attn @ v).permute(0, 2, 1, 3).reshape(B, H, W, D, C)\n",
    "        out = out.permute(0, 4, 3, 1, 2).contiguous()\n",
    "        \n",
    "        out = self.proj(out)\n",
    "        return x + out\n",
    "\n",
    "# ----------------------------\n",
    "# SST Blocks\n",
    "# ----------------------------\n",
    "\n",
    "class SpectralSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Spectral Self-Attention: Attends along the spectral dimension (bands)\n",
    "    Treats each spatial position independently and attends across all bands\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_bands, num_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.num_bands = num_bands  # Expected bands from config\n",
    "            self.num_heads = num_heads\n",
    "            self.head_dim = dim // num_heads\n",
    "            self.scale = self.head_dim ** -0.5\n",
    "            \n",
    "            assert dim % num_heads == 0, f\"dim {dim} must be divisible by num_heads {num_heads}\"\n",
    "            \n",
    "            # Linear projections for Q, K, V\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "            self.attn_drop = nn.Dropout(attn_drop)\n",
    "            self.proj = nn.Linear(dim, dim)\n",
    "            self.proj_drop = nn.Dropout(proj_drop)\n",
    "            \n",
    "            # FIXED: Initialize buffer with expected bands, will expand automatically if needed\n",
    "            spectral_pos_embed = torch.zeros(1, num_bands, dim)\n",
    "            nn.init.trunc_normal_(spectral_pos_embed, std=0.02)\n",
    "            self.register_buffer('spectral_pos_embed', spectral_pos_embed)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, D, H, W) - Input feature map\n",
    "        Returns:\n",
    "            x: (B, C, D, H, W) - Output feature map\n",
    "        \"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Reshape to (B, H, W, D, C) for spectral attention\n",
    "        x = x.permute(0, 3, 4, 2, 1).contiguous()  # (B, H, W, D, C)\n",
    "        \n",
    "        # Flatten spatial dimensions: (B*H*W, D, C)\n",
    "        x_flat = x.reshape(B * H * W, D, C)\n",
    "        \n",
    "        # FIXED: Dynamically expand buffer if we encounter more bands than initialized\n",
    "        if D > self.spectral_pos_embed.shape[1]:\n",
    "            # This happens rarely (only when encountering new max bands)\n",
    "            old_size = self.spectral_pos_embed.shape[1]\n",
    "            new_size = D\n",
    "            \n",
    "            # Create expanded buffer on same device\n",
    "            expanded = torch.zeros(1, new_size, self.dim, \n",
    "                                  device=self.spectral_pos_embed.device,\n",
    "                                  dtype=self.spectral_pos_embed.dtype)\n",
    "            \n",
    "            # Copy existing learned embeddings\n",
    "            expanded[:, :old_size, :] = self.spectral_pos_embed\n",
    "            \n",
    "            # Initialize new bands with small random values\n",
    "            nn.init.trunc_normal_(expanded[:, old_size:, :], std=0.02)\n",
    "            \n",
    "            # Update the buffer in-place\n",
    "            self.spectral_pos_embed.resize_(expanded.shape)\n",
    "            self.spectral_pos_embed.copy_(expanded)\n",
    "            \n",
    "            #print(f\"[SpectralPosEmbed] Auto-expanded from {old_size} to {new_size} bands\")\n",
    "        \n",
    "        # Use only the bands we need (safe slicing - buffer is always >= D now)\n",
    "        pos_embed = self.spectral_pos_embed[:, :D, :]  # (1, D, dim)\n",
    "        \n",
    "        # Add spectral positional encoding\n",
    "        x_flat = x_flat + pos_embed\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x_flat).reshape(B * H * W, D, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each: (B*H*W, num_heads, D, head_dim)\n",
    "        \n",
    "        # Scaled dot-product attention across spectral dimension\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))  # (B*H*W, num_heads, D, D)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x_attn = (attn @ v).transpose(1, 2).reshape(B * H * W, D, C)  # (B*H*W, D, C)\n",
    "        \n",
    "        # Project and reshape back\n",
    "        x_attn = self.proj(x_attn)\n",
    "        x_attn = self.proj_drop(x_attn)\n",
    "        \n",
    "        # Reshape back to (B, H, W, D, C)\n",
    "        x_attn = x_attn.reshape(B, H, W, D, C)\n",
    "        \n",
    "        # Reshape to original format (B, C, D, H, W)\n",
    "        x_attn = x_attn.permute(0, 4, 3, 1, 2).contiguous()\n",
    "        \n",
    "        return x_attn\n",
    "\n",
    "class SpatialSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    FIXED Spatial Self-Attention - SAME NAME, IMPROVED IMPLEMENTATION\n",
    "    Drop-in replacement - no API changes, just better internals\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, window_size=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        assert dim % num_heads == 0, f\"dim {dim} must be divisible by num_heads {num_heads}\"\n",
    "        \n",
    "        # FIXED: Full 3D convolution instead of depthwise\n",
    "        # OLD: self.dwconv = depthwise_conv3d(dim, kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
    "        # NEW: Full conv for cross-spectral information flow\n",
    "        self.dwconv = nn.Conv3d(dim, dim, kernel_size=(1, 3, 3), padding=(0, 1, 1), bias=True)\n",
    "        \n",
    "        # Keep same API as before\n",
    "        self.qkv = nn.Conv3d(dim, dim * 3, kernel_size=1, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Conv3d(dim, dim, kernel_size=1)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "        # NEW: Relative position bias (optional, improves performance)\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads)\n",
    "        )\n",
    "        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        \n",
    "        coords_h = torch.arange(window_size)\n",
    "        coords_w = torch.arange(window_size)\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += window_size - 1\n",
    "        relative_coords[:, :, 1] += window_size - 1\n",
    "        relative_coords[:, :, 0] *= 2 * window_size - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        SAME API: (B, C, D, H, W) -> (B, C, D, H, W)\n",
    "        Just improved internals\n",
    "        \"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Apply full 3D conv (cross-spectral enabled)\n",
    "        x_local = self.dwconv(x)\n",
    "        \n",
    "        qkv = self.qkv(x_local)\n",
    "        \n",
    "        if H * W > self.window_size ** 2:\n",
    "            # Efficient attention for large spatial dims\n",
    "            qkv = rearrange(qkv, 'b (three head c) d h w -> three b d head c (h w)', \n",
    "                           three=3, head=self.num_heads)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "            \n",
    "            q_flat = q.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            k_flat = k.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            v_flat = v.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            \n",
    "            k_global = k_flat.mean(dim=-1, keepdim=True)\n",
    "            v_global = v_flat.mean(dim=-1, keepdim=True)\n",
    "            \n",
    "            q_flat = q_flat * self.scale\n",
    "            attn = torch.bmm(q_flat.transpose(1, 2), k_global)\n",
    "            attn = F.softmax(attn, dim=1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            \n",
    "            x_attn = v_global * attn.transpose(1, 2)\n",
    "            x_attn = x_attn.reshape(B, D, self.num_heads, self.head_dim, H * W)\n",
    "            \n",
    "        else:\n",
    "            # Full attention for small spatial dims\n",
    "            qkv = rearrange(qkv, 'b (three head c) d h w -> three b d head c (h w)', \n",
    "                           three=3, head=self.num_heads)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "            \n",
    "            q_flat = q.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            k_flat = k.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            v_flat = v.reshape(B * D * self.num_heads, self.head_dim, H * W)\n",
    "            \n",
    "            q_flat = q_flat * self.scale\n",
    "            attn = torch.bmm(q_flat.transpose(1, 2), k_flat)\n",
    "            \n",
    "            if H == self.window_size and W == self.window_size:\n",
    "                relative_position_bias = self.relative_position_bias_table[\n",
    "                    self.relative_position_index.view(-1)\n",
    "                ].view(self.window_size * self.window_size, self.window_size * self.window_size, -1)\n",
    "                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "                attn = attn.view(B * D, self.num_heads, H * W, H * W) + relative_position_bias.unsqueeze(0)\n",
    "                attn = attn.view(B * D * self.num_heads, H * W, H * W)\n",
    "            \n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            \n",
    "            x_attn = torch.bmm(attn, v_flat.transpose(1, 2))\n",
    "            x_attn = x_attn.transpose(1, 2)\n",
    "            x_attn = x_attn.reshape(B, D, self.num_heads, self.head_dim, H * W)\n",
    "        \n",
    "        x_attn = rearrange(x_attn, 'b d head c (h w) -> b (head c) d h w', \n",
    "                          head=self.num_heads, h=H, w=W)\n",
    "        \n",
    "        x_attn = self.proj(x_attn)\n",
    "        x_attn = self.proj_drop(x_attn)\n",
    "        \n",
    "        return x_attn\n",
    "\n",
    "\n",
    "class SSTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Spectral-Spatial Transformer Block\n",
    "    Combines spectral and spatial self-attention with feed-forward network\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_bands, num_heads=8, window_size=8, \n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., \n",
    "                 drop_path=0., norm_layer=None):\n",
    "        super().__init__()\n",
    "        norm_layer = norm_layer or LayerNormChannel3d\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_bands = num_bands\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        \n",
    "        # Normalization layers\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.norm3 = norm_layer(dim)\n",
    "        \n",
    "        # Spectral attention\n",
    "        self.spectral_attn = SpectralSelfAttention(\n",
    "            dim=dim,\n",
    "            num_bands=num_bands,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop\n",
    "        )\n",
    "        \n",
    "        # Spatial attention\n",
    "        self.spatial_attn = SpatialSelfAttention(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            window_size=window_size,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop\n",
    "        )\n",
    "        \n",
    "        # Drop path for stochastic depth\n",
    "        self.drop_path = nn.Dropout(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        \n",
    "        # Feed-forward network (reuse existing GDFN)\n",
    "        self.ffn = GDFN(dim, ffn_expansion_factor=mlp_ratio, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, D, H, W)\n",
    "        Returns:\n",
    "            x: (B, C, D, H, W)\n",
    "        \"\"\"\n",
    "        # Spectral attention with residual\n",
    "        x = x + self.drop_path(self.spectral_attn(self.norm1(x)))\n",
    "        \n",
    "        # Spatial attention with residual\n",
    "        x = x + self.drop_path(self.spatial_attn(self.norm2(x)))\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        x = x + self.drop_path(self.ffn(self.norm3(x)))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SSTStage(nn.Module):\n",
    "    \"\"\"\n",
    "    SST Stage: Multiple SST blocks with optional downsampling\n",
    "    Replaces SwinTransformerStage3D\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_bands, depth, num_heads=8, window_size=8,\n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n",
    "                 drop_path_rate=0., norm_layer=None, downsample=None):\n",
    "        super().__init__()\n",
    "        norm_layer = norm_layer or LayerNormChannel3d\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        \n",
    "        # Stochastic depth decay rule\n",
    "        dpr = [drop_path_rate * (i / (depth - 1)) if depth > 1 else drop_path_rate \n",
    "               for i in range(depth)]\n",
    "        \n",
    "        # Build SST blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SSTBlock(\n",
    "                dim=dim,\n",
    "                num_bands=num_bands,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Downsampling layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=nn.LayerNorm)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, D, H, W)\n",
    "        Returns:\n",
    "            x: (B, C, D, H, W) - features before downsampling\n",
    "            x_down: (B, 2*C, D/2, H/2, W/2) - downsampled features (if downsample exists)\n",
    "        \"\"\"\n",
    "        # Pass through SST blocks\n",
    "        for blk in self.blocks:\n",
    "            if self.training:\n",
    "                x = checkpoint(blk, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        \n",
    "        # Downsample if needed\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x)\n",
    "            return x, x_down\n",
    "        else:\n",
    "            return x, x\n",
    "\n",
    "# ----------------------------\n",
    "# SST Blocks\n",
    "# ----------------------------\n",
    "\n",
    "class SpectralSelfModulatingResidualBlock(nn.Module):\n",
    "    \"\"\"Spectral Self-Modulating Residual Block (SSMRB) for adaptive feature transformation.\"\"\"\n",
    "    def __init__(self, dim, ffn_expand=2, drop=0.25, drop_factor=1.0):\n",
    "        super().__init__()\n",
    "        hidden = dim * ffn_expand\n",
    "\n",
    "        # Main FFN path (vanilla-like)\n",
    "        self.ffn_pw1 = nn.Conv3d(dim, hidden * 2, kernel_size=1, bias=True)\n",
    "        self.ffn_dw = depthwise_conv3d(hidden * 2, kernel_size=3, padding=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.ffn_pw2 = nn.Conv3d(hidden, dim, kernel_size=1, bias=True)\n",
    "\n",
    "        # Self-modulation branch: Generate gamma (scale) and beta (shift) using spectral-adjacent conv\n",
    "        # Kernel (3,1,1) captures adjacent spectral bands; depthwise for efficiency\n",
    "        self.mod_gamma = nn.Sequential(\n",
    "            depthwise_conv3d(dim, kernel_size=(3,1,1), padding=(1,0,0)),\n",
    "            nn.Sigmoid()  # For scaling (0 to 1)\n",
    "        )\n",
    "        self.mod_beta = nn.Sequential(\n",
    "            depthwise_conv3d(dim, kernel_size=(3,1,1), padding=(1,0,0)),\n",
    "            nn.Tanh()  # For shifting (-1 to 1)\n",
    "        )\n",
    "\n",
    "        # Normalization and dropout\n",
    "        self.norm = LayerNormChannel3d(dim)\n",
    "        self.dropout = AdaptiveDropout3d(drop, factor=drop_factor)  # Adaptive dropout\n",
    "\n",
    "        # Layer scale for residual contribution\n",
    "        self.gamma_res = nn.Parameter(torch.ones(1, dim, 1, 1, 1) * 1e-4)\n",
    "\n",
    "    def _ffn_path(self, x):\n",
    "        \"\"\"Vanilla FFN computation.\"\"\"\n",
    "        x2 = self.ffn_pw1(x)\n",
    "        x2 = self.ffn_dw(x2)\n",
    "        a, b = torch.chunk(x2, 2, dim=1)\n",
    "        x2 = self.act(a) * b\n",
    "        x2 = self.ffn_pw2(x2)\n",
    "        return x2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize input\n",
    "        x_norm = self.norm(x)\n",
    "\n",
    "        # Compute main FFN path\n",
    "        ffn_out = self._ffn_path(x_norm)\n",
    "\n",
    "        # Compute self-modulation parameters from input (using adjacent spectral info)\n",
    "        gamma = self.mod_gamma(x_norm)  # Scale\n",
    "        beta = self.mod_beta(x_norm)    # Shift\n",
    "\n",
    "        # Apply modulation: gamma * ffn_out + beta\n",
    "        modulated = gamma * ffn_out + beta\n",
    "\n",
    "        # Apply dropout and scale\n",
    "        modulated = self.dropout(modulated) * self.gamma_res\n",
    "\n",
    "        # Residual connection: x + modulated\n",
    "        return x + modulated\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# New Classes Added (for Restormer Integration in Architecture)\n",
    "# ----------------------------\n",
    "class LayerNorm3d(nn.Module):\n",
    "    \"\"\"3D LayerNorm for channel normalization\"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(dim=1, keepdim=True)\n",
    "        var = x.var(dim=1, keepdim=True, unbiased=False)\n",
    "        inv_std = torch.rsqrt(var + self.eps)\n",
    "        out = (x - mu) * inv_std * self.weight.view(1, -1, 1, 1, 1) + self.bias.view(1, -1, 1, 1, 1)\n",
    "        return out\n",
    "\n",
    "class GDFN(nn.Module):\n",
    "    \"\"\"Gated-Dconv Feed-Forward Network adapted to 3D, focusing on spatial\"\"\"\n",
    "    def __init__(self, dim, ffn_expansion_factor=2.66, bias=False):\n",
    "        super().__init__()\n",
    "        hidden = int(dim * ffn_expansion_factor)\n",
    "        self.project_in = nn.Conv3d(dim, hidden * 2, kernel_size=1, bias=bias)\n",
    "        self.dwconv = nn.Conv3d(hidden * 2, hidden * 2, kernel_size=(1, 3, 3), stride=1, padding=(0, 1, 1), groups=hidden * 2, bias=bias)  # Spatial focus\n",
    "        self.project_out = nn.Conv3d(hidden, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x = F.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "class MDTA(nn.Module):\n",
    "    \"\"\"Multi-Dconv Head Transposed Attention adapted to 3D, focusing on spatial\"\"\"\n",
    "    def __init__(self, dim, num_heads, bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        self.qkv = nn.Conv3d(dim, dim * 3, kernel_size=1, bias=bias)\n",
    "        self.qkv_dwconv = nn.Conv3d(dim * 3, dim * 3, kernel_size=(1, 3, 3), stride=1, padding=(0, 1, 1), groups=dim * 3, bias=bias)  # Spatial focus\n",
    "        self.project_out = nn.Conv3d(dim, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.qkv_dwconv(self.qkv(x))\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        b, c, d, h, w = q.shape\n",
    "        q = rearrange(q, 'b (head cc) d h w -> b head cc (d h w)', head=self.num_heads, cc=c // self.num_heads)\n",
    "        k = rearrange(k, 'b (head cc) d h w -> b head cc (d h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head cc) d h w -> b head cc (d h w)', head=self.num_heads)\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v)\n",
    "        out = rearrange(out, 'b head cc (d h w) -> b (head cc) d h w', head=self.num_heads, d=d, h=h, w=w)\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "\n",
    "class RestormerBlock(nn.Module):\n",
    "    \"\"\"Restormer Transformer Block adapted to 3D\"\"\"\n",
    "    def __init__(self, dim, num_heads=4, ffn_expansion_factor=2.66, bias=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = LayerNorm3d(dim)\n",
    "        self.attn = MDTA(dim, num_heads, bias)\n",
    "        self.norm2 = LayerNorm3d(dim)\n",
    "        self.ffn = GDFN(dim, ffn_expansion_factor, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "# ----------------------------\n",
    "# New Classes Added (for Restormer Integration in Architecture)\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Modified FusedBottleneck (for Restormer Integration in Architecture)\n",
    "# ----------------------------\n",
    "# ============================================================\n",
    "# INSERT THESE NEW CLASSES BEFORE FusedBottleneck (around line ~680)\n",
    "# These are ADDITIONS, not replacements\n",
    "# ============================================================\n",
    "\n",
    "class PositionalEncoding3D(nn.Module):\n",
    "    \"\"\"\n",
    "    FIXED: Lightweight 3D Positional Encoding\n",
    "    \n",
    "    Instead of storing full (C, D, H, W) tensor, we use:\n",
    "    1. Separate 1D embeddings for each dimension (much smaller)\n",
    "    2. Broadcast and combine at runtime\n",
    "    \n",
    "    Memory: O(C*D + C*H + C*W) instead of O(C*D*H*W)\n",
    "    Example: 64*128 + 64*256 + 64*256 = 41K params vs 537M params!\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, max_d=128, max_h=256, max_w=256):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.max_d = max_d\n",
    "        self.max_h = max_h\n",
    "        self.max_w = max_w\n",
    "        \n",
    "        # Separate 1D positional embeddings for each dimension\n",
    "        # These will be broadcast and combined\n",
    "        self.pos_embed_d = nn.Parameter(torch.zeros(1, channels, max_d, 1, 1))\n",
    "        self.pos_embed_h = nn.Parameter(torch.zeros(1, channels, 1, max_h, 1))\n",
    "        self.pos_embed_w = nn.Parameter(torch.zeros(1, channels, 1, 1, max_w))\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        nn.init.trunc_normal_(self.pos_embed_d, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed_h, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed_w, std=0.02)\n",
    "        \n",
    "        # Learnable scaling factors for each dimension\n",
    "        self.scale_d = nn.Parameter(torch.ones(1))\n",
    "        self.scale_h = nn.Parameter(torch.ones(1))\n",
    "        self.scale_w = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, D, H, W)\n",
    "        Returns:\n",
    "            x + positional encoding: (B, C, D, H, W)\n",
    "        \"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Slice and broadcast each dimension\n",
    "        pe_d = self.pos_embed_d[:, :, :D, :, :] * self.scale_d\n",
    "        pe_h = self.pos_embed_h[:, :, :, :H, :] * self.scale_h\n",
    "        pe_w = self.pos_embed_w[:, :, :, :, :W] * self.scale_w\n",
    "        \n",
    "        # Combine positional encodings (broadcasting happens automatically)\n",
    "        pe = pe_d + pe_h + pe_w  # Shape: (1, C, D, H, W)\n",
    "        \n",
    "        return x + pe\n",
    "\n",
    "\n",
    "class CrossSpectralSpatialAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    NEW CLASS: Cross-attention between spectral and spatial features\n",
    "    Enables joint spectral-spatial modeling instead of separate processing\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Separate projections for cross-attention paths\n",
    "        self.q_spectral = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv_spatial = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.q_spatial = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv_spectral = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_spectral = nn.Linear(dim, dim)\n",
    "        self.proj_spatial = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "        # Gated fusion\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"(B, C, D, H, W) -> (B, C, D, H, W)\"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Path 1: Spectral features with spatial context\n",
    "        x_spectral = x.permute(0, 3, 4, 2, 1).reshape(B * H * W, D, C)\n",
    "        q_spec = self.q_spectral(x_spectral)\n",
    "        q_spec = q_spec.reshape(B * H * W, D, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        q_spec_scaled = q_spec * self.scale\n",
    "        attn_spec = torch.matmul(q_spec_scaled, q_spec_scaled.transpose(-2, -1))\n",
    "        attn_spec = F.softmax(attn_spec, dim=-1)\n",
    "        attn_spec = self.attn_drop(attn_spec)\n",
    "        \n",
    "        out_spec = torch.matmul(attn_spec, q_spec)\n",
    "        out_spec = out_spec.transpose(1, 2).reshape(B * H * W, D, C)\n",
    "        out_spec = self.proj_spectral(out_spec)\n",
    "        out_spec = self.proj_drop(out_spec)\n",
    "        out_spectral = out_spec.reshape(B, H, W, D, C).permute(0, 4, 3, 1, 2)\n",
    "        \n",
    "        # Path 2: Spatial features with spectral context\n",
    "        x_spatial_q = x.permute(0, 2, 3, 4, 1).reshape(B * D, H * W, C)\n",
    "        q_spat = self.q_spatial(x_spatial_q)\n",
    "        q_spat = q_spat.reshape(B * D, H * W, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        q_spat_scaled = q_spat * self.scale\n",
    "        attn_spat = torch.matmul(q_spat_scaled, q_spat_scaled.transpose(-2, -1))\n",
    "        attn_spat = F.softmax(attn_spat, dim=-1)\n",
    "        attn_spat = self.attn_drop(attn_spat)\n",
    "        \n",
    "        out_spat = torch.matmul(attn_spat, q_spat)\n",
    "        out_spat = out_spat.transpose(1, 2).reshape(B * D, H * W, C)\n",
    "        out_spat = self.proj_spatial(out_spat)\n",
    "        out_spat = self.proj_drop(out_spat)\n",
    "        out_spatial = out_spat.reshape(B, D, H, W, C).permute(0, 4, 1, 2, 3)\n",
    "        \n",
    "        # Gated fusion\n",
    "        concat_features = torch.cat([out_spectral, out_spatial], dim=1)\n",
    "        gate_input = F.adaptive_avg_pool3d(concat_features, 1).squeeze(-1).squeeze(-1).squeeze(-1)\n",
    "        gate = self.gate(gate_input).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        fused = gate * out_spectral + (1 - gate) * out_spatial\n",
    "        return fused\n",
    "\n",
    "\n",
    "class EnhancedBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    NEW CLASS: SOTA-level bottleneck with cross-attention and joint modeling\n",
    "    Will be used in the main architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.norm1 = LayerNormChannel3d(dim)\n",
    "        self.norm2 = LayerNormChannel3d(dim)\n",
    "        self.norm3 = LayerNormChannel3d(dim)\n",
    "        \n",
    "        # Cross-attention for spectral-spatial joint modeling\n",
    "        self.cross_attn = CrossSpectralSpatialAttention(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=True,\n",
    "            attn_drop=0.1,\n",
    "            proj_drop=0.1\n",
    "        )\n",
    "        \n",
    "        # Non-local attention for global context\n",
    "        self.non_local = nn.Sequential(\n",
    "            nn.Conv3d(dim, dim // 2, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv3d(dim // 2, dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Keep using existing GDFN and SSMRB\n",
    "        self.ffn = GDFN(dim, ffn_expansion_factor=mlp_ratio, bias=False)\n",
    "        self.spectral_refine = SpectralSelfModulatingResidualBlock(\n",
    "            dim, ffn_expand=2, drop=0.1, drop_factor=1.0\n",
    "        )\n",
    "        \n",
    "        # Gated fusion\n",
    "        self.fusion_gate = nn.Sequential(\n",
    "            nn.Conv3d(dim * 3, dim, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv3d(dim, dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"(B, C, D, H, W) -> (B, C, D, H, W)\"\"\"\n",
    "        identity = x\n",
    "        \n",
    "        # Path 1: Cross-attention\n",
    "        cross_out = self.cross_attn(self.norm1(x))\n",
    "        x = x + cross_out\n",
    "        \n",
    "        # Path 2: Non-local\n",
    "        non_local_weight = self.non_local(self.norm2(x))\n",
    "        non_local_out = x * non_local_weight\n",
    "        x = x + non_local_out\n",
    "        \n",
    "        # Path 3: FFN + Spectral refinement\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        spectral_out = self.spectral_refine(x + ffn_out)\n",
    "        \n",
    "        # Gated fusion\n",
    "        fusion_input = torch.cat([cross_out, non_local_out, spectral_out], dim=1)\n",
    "        fusion_weight = self.fusion_gate(fusion_input)\n",
    "        \n",
    "        out = identity + fusion_weight * (cross_out + non_local_out + spectral_out) / 3.0\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# Modified FusedBottleneck (for Restormer Integration in Architecture)\n",
    "# ----------------------------\n",
    "        \n",
    "class FusedBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    IMPROVED FusedBottleneck - SAME NAME, better implementation\n",
    "    Now uses stacked EnhancedBottleneck blocks for SOTA performance\n",
    "    Drop-in replacement - same API, just calls different internals\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dim, window_sizes=[2, 4]):\n",
    "        super().__init__()\n",
    "        # Calculate actual dim from base_dim (maintains compatibility)\n",
    "        # Your original: dim = base_dim * 4\n",
    "        # But you call it with base_dim * 2, so actual dim is base_dim * 8\n",
    "        dim = base_dim * 4  # This gives base_dim * 8 when called with base_dim * 2\n",
    "        \n",
    "        # Use stacked enhanced bottleneck blocks instead of old approach\n",
    "        self.blocks = nn.ModuleList([\n",
    "            EnhancedBottleneck(dim, num_heads=8, mlp_ratio=4.),\n",
    "            #EnhancedBottleneck(dim, num_heads=8, mlp_ratio=4.)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        SAME API: (B, C, D, H, W) -> (B, C, D, H, W)\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Modified FusedBottleneck (for Restormer Integration in Architecture)\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Efficient Loss Function\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Modified MemoryEfficientLoss (for Spatial-Focused Loss Improvements)\n",
    "# ----------------------------\n",
    "class MemoryEfficientLoss(nn.Module):\n",
    "    \"\"\"Lightweight but effective loss function with FIXED weights and tensor handling\"\"\"\n",
    "    def __init__(self, device='cuda', mse_weight=1.0, l1_weight=1.0, sam_weight=0.5, edge_weight=0.2):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.device = device\n",
    "        self.mse_weight = mse_weight\n",
    "        self.l1_weight = l1_weight\n",
    "        self.sam_weight = sam_weight\n",
    "        self.edge_weight = edge_weight\n",
    "\n",
    "    def forward(self, pred, target, epoch=None):\n",
    "        # FIXED: Ensure both tensors have same shape\n",
    "        if pred.shape != target.shape:\n",
    "            # If shapes don't match, interpolate pred to match target\n",
    "            if pred.dim() == 5 and target.dim() == 5:\n",
    "                pred = F.interpolate(pred, size=target.shape[2:], mode='trilinear', align_corners=False)\n",
    "            elif pred.dim() == 4 and target.dim() == 4:\n",
    "                pred = F.interpolate(pred, size=target.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Main losses\n",
    "        mse_loss = self.mse(pred, target)\n",
    "        l1_loss = self.l1(pred, target)\n",
    "\n",
    "        # FIXED: SAM calculation with proper tensor handling\n",
    "        eps = 1e-8\n",
    "\n",
    "        # Handle both 4D and 5D tensors\n",
    "        if pred.dim() == 5:  # (B, C, D, H, W)\n",
    "            B, C, D, H, W = pred.shape\n",
    "            pred_flat = pred.reshape(B, C, D * H * W)  # (B, C, D*H*W)\n",
    "            target_flat = target.reshape(B, C, D * H * W)  # (B, C, D*H*W)\n",
    "\n",
    "            # Normalize along channel dimension (spectral bands)\n",
    "            pred_norm = F.normalize(pred_flat, dim=1, eps=eps)\n",
    "            target_norm = F.normalize(target_flat, dim=1, eps=eps)\n",
    "\n",
    "            # Compute cosine similarity along spectral dimension\n",
    "            cos_sim = torch.sum(pred_norm * target_norm, dim=1)  # (B, D*H*W)\n",
    "\n",
    "        elif pred.dim() == 4:  # (B, D, H, W) - spectral first\n",
    "            B, D, H, W = pred.shape\n",
    "            pred_flat = pred.reshape(B, D, H * W)  # (B, D, H*W)\n",
    "            target_flat = target.reshape(B, D, H * W)  # (B, D, H*W)\n",
    "\n",
    "            # Normalize along spectral dimension\n",
    "            pred_norm = F.normalize(pred_flat, dim=1, eps=eps)\n",
    "            target_norm = F.normalize(target_flat, dim=1, eps=eps)\n",
    "\n",
    "            # Compute cosine similarity along spectral dimension\n",
    "            cos_sim = torch.sum(pred_norm * target_norm, dim=1)  # (B, H*W)\n",
    "\n",
    "        else:\n",
    "            # Fallback for other dimensions\n",
    "            pred_flat = pred.flatten(start_dim=1)\n",
    "            target_flat = target.flatten(start_dim=1)\n",
    "            pred_norm = F.normalize(pred_flat, dim=1, eps=eps)\n",
    "            target_norm = F.normalize(target_flat, dim=1, eps=eps)\n",
    "            cos_sim = torch.sum(pred_norm * target_norm, dim=1)\n",
    "\n",
    "        cos_sim = torch.clamp(cos_sim, -1 + eps, 1 - eps)\n",
    "        sam_loss = torch.mean(1 - cos_sim)\n",
    "\n",
    "        # FIXED: Edge loss with proper spatial dimension handling\n",
    "        def spatial_gradient(x):\n",
    "            if x.dim() == 5:  # (B, C, D, H, W)\n",
    "                grad_h = torch.abs(x[:, :, :, 1:, :] - x[:, :, :, :-1, :])\n",
    "                grad_w = torch.abs(x[:, :, :, :, 1:] - x[:, :, :, :, :-1])\n",
    "            elif x.dim() == 4:  # (B, D, H, W)\n",
    "                grad_h = torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :])\n",
    "                grad_w = torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1])\n",
    "            else:\n",
    "                return 0, 0\n",
    "            return grad_h.mean(), grad_w.mean()\n",
    "\n",
    "        pred_grad_h, pred_grad_w = spatial_gradient(pred)\n",
    "        target_grad_h, target_grad_w = spatial_gradient(target)\n",
    "        edge_loss = abs(pred_grad_h - target_grad_h) + abs(pred_grad_w - target_grad_w)\n",
    "\n",
    "        # Static combination\n",
    "        total_loss = (\n",
    "            self.mse_weight * mse_loss +\n",
    "            self.l1_weight * l1_loss +\n",
    "            self.sam_weight * sam_loss +\n",
    "            self.edge_weight * edge_loss\n",
    "        )\n",
    "\n",
    "        return total_loss\n",
    "# ----------------------------\n",
    "# Modified MemoryEfficientLoss (for Spatial-Focused Loss Improvements)\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Memory-Efficient U-Net\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Memory-Efficient U-Net\n",
    "# ----------------------------\n",
    "class MemoryOptimizedUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    SST-based U-Net for HSI Denoising (MODIFIED)\n",
    "    - 4 hierarchical stages with SST blocks instead of Swin\n",
    "    - Spectral-aware attention at all levels\n",
    "    - Deep supervision at each decoder stage\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, base_dim=48, window_sizes=[4, 8, 16], num_bands=64):\n",
    "        super().__init__()\n",
    "        self.base_dim = base_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.num_bands = num_bands\n",
    "        \n",
    "        # Initial projection\n",
    "        self.patch_embed = nn.Conv3d(in_channels, base_dim, kernel_size=3, padding=1)\n",
    "        self.pos_embed_init = PositionalEncoding3D(base_dim, 128, 256, 256)\n",
    "        \n",
    "        # ENCODER: 4 SST stages with [2, 2, 6, 2] depth\n",
    "        # Stage 1: base_dim, shallow features\n",
    "        self.enc_stage1 = SSTStage(\n",
    "            dim=base_dim,\n",
    "            num_bands=num_bands,\n",
    "            depth=2,\n",
    "            num_heads=8,\n",
    "            window_size=8,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.05,\n",
    "            downsample=PatchMerging3D\n",
    "        )\n",
    "        \n",
    "        # Stage 2: base_dim*2, intermediate features\n",
    "        self.enc_stage2 = SSTStage(\n",
    "            dim=base_dim * 2,\n",
    "            num_bands=num_bands // 2,  # Bands halved after merging\n",
    "            depth=2,\n",
    "            num_heads=8,\n",
    "            window_size=8,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.1,\n",
    "            downsample=PatchMerging3D\n",
    "        )\n",
    "        \n",
    "        # Stage 3: base_dim*4, deep features (6 blocks)\n",
    "        self.enc_stage3 = SSTStage(\n",
    "            dim=base_dim * 4,\n",
    "            num_bands=num_bands // 4,\n",
    "            depth=6,\n",
    "            num_heads=16,\n",
    "            window_size=4,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.15,\n",
    "            downsample=PatchMerging3D\n",
    "        )\n",
    "        \n",
    "        # Stage 4 (deepest): base_dim*8, bottleneck\n",
    "        self.enc_stage4 = SSTStage(\n",
    "            dim=base_dim * 8,\n",
    "            num_bands=num_bands // 8,\n",
    "            depth=2,\n",
    "            num_heads=16,\n",
    "            window_size=2,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.2,\n",
    "            downsample=None\n",
    "        )\n",
    "\n",
    "        self.pe_enc1 = PositionalEncoding3D(base_dim, 128, 256, 256)\n",
    "        self.pe_enc2 = PositionalEncoding3D(base_dim * 2, 64, 128, 128)\n",
    "        self.pe_enc3 = PositionalEncoding3D(base_dim * 4, 32, 64, 64)\n",
    "        self.pe_enc4 = PositionalEncoding3D(base_dim * 8, 16, 32, 32)\n",
    "        \n",
    "        # BOTTLENECK: Keep your custom FusedBottleneck (works well with SST)\n",
    "        self.spectral_attention = SpectralAttentionModule(base_dim * 8, num_heads=8)\n",
    "        self.bottleneck_fusion = FusedBottleneck(base_dim * 2, window_sizes=window_sizes)\n",
    "        \n",
    "        # DECODER: 4 SST stages matching encoder\n",
    "        # Stage 3 decoder\n",
    "        self.dec_stage3_up = PatchExpanding3D(dim=base_dim * 8)\n",
    "        self.dec_stage3 = SSTStage(\n",
    "            dim=base_dim * 4,\n",
    "            num_bands=num_bands // 4,\n",
    "            depth=6,\n",
    "            num_heads=16,\n",
    "            window_size=4,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.1,\n",
    "            downsample=None\n",
    "        )\n",
    "        \n",
    "        # Stage 2 decoder\n",
    "        self.dec_stage2_up = PatchExpanding3D(dim=base_dim * 4)\n",
    "        self.dec_stage2 = SSTStage(\n",
    "            dim=base_dim * 2,\n",
    "            num_bands=num_bands // 2,\n",
    "            depth=2,\n",
    "            num_heads=8,\n",
    "            window_size=8,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.08,\n",
    "            downsample=None\n",
    "        )\n",
    "        \n",
    "        # Stage 1 decoder\n",
    "        self.dec_stage1_up = PatchExpanding3D(dim=base_dim * 2)\n",
    "        self.dec_stage1 = SSTStage(\n",
    "            dim=base_dim,\n",
    "            num_bands=num_bands,\n",
    "            depth=2,\n",
    "            num_heads=8,\n",
    "            window_size=8,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path_rate=0.02,\n",
    "            downsample=None\n",
    "        )\n",
    "        \n",
    "        # DEEP SUPERVISION: Auxiliary outputs at each decoder stage\n",
    "        self.deep_sup3 = nn.Conv3d(base_dim * 4, in_channels, 1)\n",
    "        self.deep_sup2 = nn.Conv3d(base_dim * 2, in_channels, 1)\n",
    "        self.deep_sup1 = nn.Conv3d(base_dim, in_channels, 1)\n",
    "        \n",
    "        # Final reconstruction\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv3d(base_dim, base_dim // 2, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv3d(base_dim // 2, in_channels, 1),\n",
    "        )\n",
    "        \n",
    "        # Global residual\n",
    "        self.global_residual = nn.Conv3d(in_channels, in_channels, 1)\n",
    "        \n",
    "        # Deep supervision flag\n",
    "        self.use_deep_supervision = True\n",
    "\n",
    "    def _align_tensors(self, x, target_size):\n",
    "        if x.shape[2:] != target_size:\n",
    "            x = F.interpolate(x, size=target_size, mode='trilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, return_deep_sup=False):\n",
    "        # Handle input shape\n",
    "        original_was_4d = False\n",
    "        if x.dim() == 4:\n",
    "            original_was_4d = True\n",
    "            x = x.unsqueeze(1)\n",
    "        elif x.dim() == 5 and x.shape[1] != 1:\n",
    "            if x.shape[2] == 1:\n",
    "                x = x.transpose(1, 2)\n",
    "        \n",
    "        original_size = x.shape[2:]\n",
    "        input_residual = self.global_residual(x)\n",
    "        \n",
    "        # Initial embedding\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.pos_embed_init(x)\n",
    "        \n",
    "        # ENCODER (4 SST stages)\n",
    "        e1, e1_down = self.enc_stage1(self.pe_enc1(x))       # Skip 1\n",
    "        e2, e2_down = self.enc_stage2(self.pe_enc2(e1_down))  # Skip 2\n",
    "        e3, e3_down = self.enc_stage3(self.pe_enc3(e2_down))  # Skip 3\n",
    "        e4, _ = self.enc_stage4(self.pe_enc4(e3_down))        # Deepest features\n",
    "        \n",
    "        # BOTTLENECK: Hybrid attention\n",
    "        b = self.spectral_attention(e4)  # Add spectral attention\n",
    "        b = self.bottleneck_fusion(b)   # Your custom fusion\n",
    "        \n",
    "        # DECODER with deep supervision\n",
    "        deep_outputs = []\n",
    "        \n",
    "        # Decoder stage 3\n",
    "        d3 = self.dec_stage3_up(b)\n",
    "        d3 = self._align_tensors(d3, e3.shape[2:])\n",
    "        d3 = d3 + e3  # Skip connection\n",
    "        d3, _ = self.dec_stage3(d3)\n",
    "        if self.training and self.use_deep_supervision:\n",
    "            sup3 = self.deep_sup3(d3)\n",
    "            sup3 = self._align_tensors(sup3, original_size)\n",
    "            deep_outputs.append(sup3)\n",
    "        \n",
    "        # Decoder stage 2\n",
    "        d2 = self.dec_stage2_up(d3)\n",
    "        d2 = self._align_tensors(d2, e2.shape[2:])\n",
    "        d2 = d2 + e2\n",
    "        d2, _ = self.dec_stage2(d2)\n",
    "        if self.training and self.use_deep_supervision:\n",
    "            sup2 = self.deep_sup2(d2)\n",
    "            sup2 = self._align_tensors(sup2, original_size)\n",
    "            deep_outputs.append(sup2)\n",
    "        \n",
    "        # Decoder stage 1\n",
    "        d1 = self.dec_stage1_up(d2)\n",
    "        d1 = self._align_tensors(d1, e1.shape[2:])\n",
    "        d1 = d1 + e1\n",
    "        d1, _ = self.dec_stage1(d1)\n",
    "        if self.training and self.use_deep_supervision:\n",
    "            sup1 = self.deep_sup1(d1)\n",
    "            sup1 = self._align_tensors(sup1, original_size)\n",
    "            deep_outputs.append(sup1)\n",
    "        \n",
    "        # Final reconstruction\n",
    "        out = self.final_conv(d1)\n",
    "        out = self._align_tensors(out, original_size)\n",
    "        input_residual = self._align_tensors(input_residual, original_size)\n",
    "        out = out + input_residual\n",
    "        \n",
    "        # Return format handling\n",
    "        if original_was_4d and out.shape[1] == 1:\n",
    "            out = out.squeeze(1)\n",
    "            if self.training and self.use_deep_supervision:\n",
    "                deep_outputs = [o.squeeze(1) for o in deep_outputs]\n",
    "        \n",
    "        if return_deep_sup and self.training:\n",
    "            return out, deep_outputs\n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# Memory-Efficient U-Net\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Metric Calculation Functions\n",
    "# ----------------------------\n",
    "def calculate_psnr(pred, target):\n",
    "    \"\"\"Calculate Peak Signal-to-Noise Ratio\"\"\"\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    return 10 * torch.log10(1.0 / (mse + 1e-8)).item()\n",
    "\n",
    "def calculate_ssim(pred_np, target_np):\n",
    "    \"\"\"Calculate SSIM across all spectral bands\"\"\"\n",
    "    if pred_np.ndim == 3:  # (D, H, W)\n",
    "        D, H, W = pred_np.shape\n",
    "        ssim_vals = []\n",
    "        for d in range(D):\n",
    "            try:\n",
    "                ssim_val = compare_ssim(pred_np[d], target_np[d], data_range=1.0)\n",
    "                ssim_vals.append(ssim_val)\n",
    "            except Exception:\n",
    "                ssim_vals.append(0.5)  # Fallback\n",
    "        return np.mean(ssim_vals)\n",
    "    return compare_ssim(pred_np, target_np, data_range=1.0)\n",
    "\n",
    "def calculate_sam(pred_np, target_np):\n",
    "    \"\"\"Calculate Spectral Angle Mapper\"\"\"\n",
    "    eps = 1e-8\n",
    "    if pred_np.ndim == 3:  # (D, H, W)\n",
    "        pred_flat = pred_np.reshape(pred_np.shape[0], -1)\n",
    "        target_flat = target_np.reshape(target_np.shape[0], -1)\n",
    "        dot = np.sum(pred_flat * target_flat, axis=0)\n",
    "        norm_pred = np.linalg.norm(pred_flat, axis=0) + eps\n",
    "        norm_target = np.linalg.norm(target_flat, axis=0) + eps\n",
    "        cos_angle = np.clip(dot / (norm_pred * norm_target), -1, 1)\n",
    "        angles = np.arccos(cos_angle)\n",
    "        return np.mean(angles)\n",
    "    dot = np.sum(pred_np * target_np)\n",
    "    norm_pred = np.linalg.norm(pred_np) + eps\n",
    "    norm_target = np.linalg.norm(target_np) + eps\n",
    "    cos_angle = np.clip(dot / (norm_pred * norm_target), -1, 1)\n",
    "    return np.arccos(cos_angle)\n",
    "\n",
    "# ----------------------------\n",
    "# Test Data Loading\n",
    "# ----------------------------\n",
    "def diagnose_dataset_structure(file_path):\n",
    "    \"\"\"Diagnose the structure of a .mat file to understand data organization\"\"\"\n",
    "    if not sio:\n",
    "        print(\"scipy not available\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        mat = sio.loadmat(file_path)\n",
    "        print(f\"\\n=== Diagnosing: {os.path.basename(file_path)} ===\")\n",
    "        \n",
    "        # Print all keys\n",
    "        keys = [k for k in mat.keys() if not k.startswith('__')]\n",
    "        print(f\"Available keys: {keys}\")\n",
    "        \n",
    "        for key in keys:\n",
    "            data = mat[key]\n",
    "            print(f\"Key '{key}': shape={data.shape}, dtype={data.dtype}\")\n",
    "            print(f\"  Value range: [{data.min():.4f}, {data.max():.4f}]\")\n",
    "            print(f\"  Mean: {data.mean():.4f}, Std: {data.std():.4f}\")\n",
    "            \n",
    "            # Check if it looks like spectral data\n",
    "            if len(data.shape) == 3:\n",
    "                print(f\"  3D data - possible interpretations:\")\n",
    "                print(f\"    As (H, W, Bands): {data.shape}\")\n",
    "                print(f\"    As (Bands, H, W): {data.shape}\")\n",
    "                \n",
    "                # Check which dimension might be spectral\n",
    "                dim_variances = [data.var(axis=i).mean() for i in range(3)]\n",
    "                spectral_dim = np.argmax(dim_variances)\n",
    "                print(f\"  Dimension variances: {dim_variances}\")\n",
    "                print(f\"  Likely spectral dimension: {spectral_dim} (highest variance)\")\n",
    "                \n",
    "        return mat, keys\n",
    "    except Exception as e:\n",
    "        print(f\"Error diagnosing {file_path}: {e}\")\n",
    "        return None, []\n",
    "        \n",
    "def load_test_data(test_dir, target_bands=31, max_files=None, test_crop_size=512):\n",
    "    \"\"\"Load test data with ICVL-specific preprocessing: 512×512×31 crops\"\"\"\n",
    "    print(f\"Loading test data from: {test_dir}\")\n",
    "\n",
    "    if not os.path.exists(test_dir):\n",
    "        print(f\"Test directory not found: {test_dir}\")\n",
    "        return []\n",
    "\n",
    "    mat_files = glob.glob(os.path.join(test_dir, '*.mat'))\n",
    "    if max_files:\n",
    "        mat_files = mat_files[:max_files]\n",
    "\n",
    "    print(f\"Found {len(mat_files)} test files\\n\")\n",
    "\n",
    "    test_data = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in tqdm(mat_files, desc=\"Loading test files\", ncols=80):\n",
    "        try:\n",
    "            # Try loading with h5py first\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                if 'rad' in f:\n",
    "                    data_key = 'rad'\n",
    "                    cube_raw = np.array(f[data_key]).astype(np.float32)\n",
    "                else:\n",
    "                    keys = [k for k in f.keys() if not k.startswith('#')]\n",
    "                    if keys:\n",
    "                        data_key = keys[0]\n",
    "                        cube_raw = np.array(f[data_key]).astype(np.float32)\n",
    "                    else:\n",
    "                        failed_files.append((os.path.basename(file_path), \"No valid keys found\"))\n",
    "                        continue\n",
    "                        \n",
    "        except Exception as h5_error:\n",
    "            if sio:\n",
    "                try:\n",
    "                    mat = sio.loadmat(file_path)\n",
    "                    keys = [k for k in mat.keys() if not k.startswith('__')]\n",
    "                    data_key = keys[0] if keys else None\n",
    "                    if data_key:\n",
    "                        cube_raw = mat[data_key].astype(np.float32)\n",
    "                    else:\n",
    "                        failed_files.append((os.path.basename(file_path), \"No valid keys in scipy load\"))\n",
    "                        continue\n",
    "                except Exception as scipy_error:\n",
    "                    failed_files.append((os.path.basename(file_path), f\"Both loaders failed\"))\n",
    "                    continue\n",
    "            else:\n",
    "                failed_files.append((os.path.basename(file_path), \"h5py failed, scipy unavailable\"))\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            cube_original = cube_raw.copy()\n",
    "\n",
    "            # ICVL format handling: (D, H, W) -> transpose to (H, W, D)\n",
    "            if cube_raw.ndim == 3:\n",
    "                if cube_raw.shape[0] < min(cube_raw.shape[1:]):  # (D, H, W)\n",
    "                    cube_raw = cube_raw.transpose(1, 2, 0)  # -> (H, W, D)\n",
    "                    cube_original = cube_original.transpose(1, 2, 0)\n",
    "            elif cube_raw.ndim == 2:\n",
    "                cube_raw = cube_raw[np.newaxis, np.newaxis, ...]\n",
    "                cube_original = cube_original[np.newaxis, np.newaxis, ...]\n",
    "\n",
    "            if cube_raw.ndim != 3:\n",
    "                failed_files.append((os.path.basename(file_path), f\"Invalid dimensions: {cube_raw.ndim}\"))\n",
    "                continue\n",
    "\n",
    "            H, W, D = cube_raw.shape\n",
    "\n",
    "            # CRITICAL: Center crop to 512×512 for testing (ICVL protocol)\n",
    "            if H > test_crop_size or W > test_crop_size:\n",
    "                start_h = (H - test_crop_size) // 2\n",
    "                start_w = (W - test_crop_size) // 2\n",
    "                cube_raw = cube_raw[start_h:start_h+test_crop_size,\n",
    "                                   start_w:start_w+test_crop_size, :]\n",
    "                cube_original = cube_original[start_h:start_h+test_crop_size,\n",
    "                                             start_w:start_w+test_crop_size, :]\n",
    "                H, W = test_crop_size, test_crop_size\n",
    "                #print(f\"  Cropped {os.path.basename(file_path)} to {test_crop_size}×{test_crop_size}\")\n",
    "            elif H < test_crop_size or W < test_crop_size:\n",
    "                # Pad if smaller than 512\n",
    "                pad_h = max(0, test_crop_size - H)\n",
    "                pad_w = max(0, test_crop_size - W)\n",
    "                cube_raw = np.pad(cube_raw, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "                cube_original = np.pad(cube_original, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "                H, W = test_crop_size, test_crop_size\n",
    "                print(f\"  Padded {os.path.basename(file_path)} to {test_crop_size}×{test_crop_size}\")\n",
    "\n",
    "            # Select exactly 31 bands (ICVL standard)\n",
    "            if D >= target_bands:\n",
    "                cube_raw = cube_raw[:, :, :target_bands]\n",
    "                cube_original = cube_original[:, :, :target_bands]\n",
    "            else:\n",
    "                failed_files.append((os.path.basename(file_path), f\"Insufficient bands: {D} < {target_bands}\"))\n",
    "                continue\n",
    "\n",
    "            # Store original range\n",
    "            original_min = cube_original.min()\n",
    "            original_max = cube_original.max()\n",
    "\n",
    "            # MATCH TRAINING: Global min-max normalization to [0,1]\n",
    "            cube = (cube_raw - cube_raw.min()) / (cube_raw.max() - cube_raw.min() + 1e-8)\n",
    "\n",
    "            # Convert to (D, H, W) format for model\n",
    "            cube = cube.transpose(2, 0, 1)\n",
    "\n",
    "            test_data.append({\n",
    "                'clean': cube,\n",
    "                'original_range': (original_min, original_max),\n",
    "                'filename': os.path.basename(file_path),\n",
    "                'shape': cube.shape,\n",
    "                'dataset_type': 'icvl'\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_files.append((os.path.basename(file_path), f\"Processing error: {str(e)[:50]}\"))\n",
    "            continue\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DATA LOADING SUMMARY (ICVL Protocol: {test_crop_size}×{test_crop_size}×{target_bands})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Successfully loaded: {len(test_data)}/{len(mat_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"✗ Failed to load: {len(failed_files)} files\")\n",
    "        for fname, reason in failed_files[:5]:\n",
    "            print(f\"    - {fname}: {reason}\")\n",
    "        if len(failed_files) > 5:\n",
    "            print(f\"    ... and {len(failed_files) - 5} more\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return test_data\n",
    "    \n",
    "def add_static_noise_to_data(clean_data, noise_level=50, batch_size=5):\n",
    "    \"\"\"Add Gaussian noise matching TRAINING protocol exactly\"\"\"\n",
    "    import gc\n",
    "    \n",
    "    test_samples = []\n",
    "    total_samples = len(clean_data)\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size\n",
    "    failed_samples = []\n",
    "    \n",
    "    print(f\"Adding Gaussian noise (σ={noise_level}) to {total_samples} samples...\")\n",
    "    print(f\"Noise protocol: MATCH TRAINING (scale [0,1] -> [0,255], add noise, clip, rescale)\")\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = min(batch_start + batch_size, total_samples)\n",
    "        batch_data = clean_data[batch_start:batch_end]\n",
    "        \n",
    "        for local_idx, data_item in enumerate(batch_data):\n",
    "            global_idx = batch_start + local_idx\n",
    "            filename = data_item['filename']\n",
    "            \n",
    "            try:\n",
    "                clean_raw = data_item['clean']\n",
    "                \n",
    "                if np.isnan(clean_raw).any() or np.isinf(clean_raw).any():\n",
    "                    raise ValueError(f\"Invalid values in {filename}\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    clean_tensor = torch.from_numpy(clean_raw).float().unsqueeze(0)\n",
    "                    \n",
    "                    # EXACT MATCH WITH TRAINING: Paper-standard Gaussian noise\n",
    "                    clean_255 = clean_tensor * 255.0\n",
    "                    noise = torch.randn_like(clean_255) * noise_level\n",
    "                    noisy_255 = torch.clamp(clean_255 + noise, 0, 255)\n",
    "                    noisy_normalized = noisy_255 / 255.0\n",
    "\n",
    "                    clean_np = clean_tensor[0].cpu().numpy()\n",
    "                    noisy_np = noisy_normalized[0].cpu().numpy()\n",
    "                    \n",
    "                    del clean_tensor, clean_255, noise, noisy_255, noisy_normalized\n",
    "\n",
    "                test_samples.append({\n",
    "                    'clean': clean_np,\n",
    "                    'noisy': noisy_np,\n",
    "                    'noise_level': noise_level / 255.0,\n",
    "                    'original_range': data_item['original_range'],\n",
    "                    'filename': filename,\n",
    "                    'shape': clean_np.shape\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_samples.append((global_idx + 1, filename, str(e)))\n",
    "                continue\n",
    "        \n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"✓ Successfully processed: {len(test_samples)}/{total_samples} samples\\n\")\n",
    "    \n",
    "    return test_samples\n",
    "\n",
    "    \n",
    "# ----------------------------\n",
    "# Testing and Visualization Functions\n",
    "# ----------------------------\n",
    "def test_model_comprehensive(model, test_samples, device, patch_size=64):\n",
    "    \"\"\"Comprehensive model testing with proper patch handling - SILENT VERSION\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING MODEL ON ALL SAMPLES\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(tqdm(test_samples, desc=\"Testing samples\", ncols=80)):\n",
    "            try:\n",
    "                clean_np = sample['clean']\n",
    "                noisy_np = sample['noisy']\n",
    "\n",
    "                # Create 4D tensors (B, D, H, W)\n",
    "                clean_tensor = torch.from_numpy(clean_np).unsqueeze(0).float().to(device)\n",
    "                noisy_tensor = torch.from_numpy(noisy_np).unsqueeze(0).float().to(device)\n",
    "\n",
    "                B, D, H, W = clean_tensor.shape\n",
    "\n",
    "                # Process based on image size\n",
    "                if H <= patch_size and W <= patch_size:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        output_tensor = model(noisy_tensor)\n",
    "                else:\n",
    "                    # Patch-based processing for large images\n",
    "                    output_tensor = torch.zeros_like(clean_tensor)\n",
    "                    \n",
    "                    for h_start in range(0, H, patch_size):\n",
    "                        for w_start in range(0, W, patch_size):\n",
    "                            h_end = min(h_start + patch_size, H)\n",
    "                            w_end = min(w_start + patch_size, W)\n",
    "                            \n",
    "                            patch_noisy = noisy_tensor[:, :, h_start:h_end, w_start:w_end]\n",
    "                            \n",
    "                            # Pad to patch_size if needed\n",
    "                            pad_h = patch_size - (h_end - h_start)\n",
    "                            pad_w = patch_size - (w_end - w_start)\n",
    "                            if pad_h > 0 or pad_w > 0:\n",
    "                                patch_noisy = F.pad(patch_noisy, (0, pad_w, 0, pad_h))\n",
    "                            \n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                patch_output = model(patch_noisy)\n",
    "                            \n",
    "                            # Remove padding\n",
    "                            if pad_h > 0 or pad_w > 0:\n",
    "                                patch_output = patch_output[:, :, :h_end-h_start, :w_end-w_start]\n",
    "                            \n",
    "                            output_tensor[:, :, h_start:h_end, w_start:w_end] = patch_output\n",
    "\n",
    "                output_np = output_tensor[0].cpu().numpy()\n",
    "\n",
    "                # Calculate metrics\n",
    "                loss = torch.mean((output_tensor - clean_tensor) ** 2).item()\n",
    "                psnr = calculate_psnr(output_tensor, clean_tensor)\n",
    "                ssim = calculate_ssim(output_np, clean_np)\n",
    "                sam = calculate_sam(output_np, clean_np)\n",
    "                \n",
    "                input_psnr = calculate_psnr(noisy_tensor, clean_tensor)\n",
    "                input_ssim = calculate_ssim(noisy_np, clean_np)\n",
    "                input_sam = calculate_sam(noisy_np, clean_np)\n",
    "\n",
    "                results.append({\n",
    "                    'filename': sample['filename'],\n",
    "                    'noise_level': sample['noise_level'],\n",
    "                    'shape': sample['shape'],\n",
    "                    'input_psnr': input_psnr,\n",
    "                    'output_psnr': psnr,\n",
    "                    'psnr_improvement': psnr - input_psnr,\n",
    "                    'input_ssim': input_ssim,\n",
    "                    'ssim': ssim,\n",
    "                    'ssim_improvement': ssim - input_ssim,\n",
    "                    'input_sam': input_sam,\n",
    "                    'sam': sam,\n",
    "                    'sam_improvement': sam - input_sam     \n",
    "                    'loss': loss,\n",
    "                    'clean': clean_np,\n",
    "                    'noisy': noisy_np,\n",
    "                    'denoised': output_np,\n",
    "                    'original_range': sample['original_range']\n",
    "                })\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing sample {i+1} ({sample['filename']}): {e}\")\n",
    "                continue\n",
    "\n",
    "    # Print summary statistics\n",
    "    if results:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TESTING COMPLETED - AVERAGE SCORES\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total samples tested: {len(results)}/{len(test_samples)}\")\n",
    "        print(f\"\\nAverage Metrics:\")\n",
    "        print(f\"  • Loss:            {np.mean([r['loss'] for r in results]):.6f} ± {np.std([r['loss'] for r in results]):.6f}\")\n",
    "        print(f\"  • Input PSNR:      {np.mean([r['input_psnr'] for r in results]):.2f} ± {np.std([r['input_psnr'] for r in results]):.2f} dB\")\n",
    "        print(f\"  • Output PSNR:     {np.mean([r['output_psnr'] for r in results]):.2f} ± {np.std([r['output_psnr'] for r in results]):.2f} dB\")\n",
    "        print(f\"  • PSNR Improvement: {np.mean([r['psnr_improvement'] for r in results]):.2f} ± {np.std([r['psnr_improvement'] for r in results]):.2f} dB\")\n",
    "        print(f\"  • Input SSIM:            {np.mean([r['input_ssim'] for r in results]):.4f} ± {np.std([r['input_ssim'] for r in results]):.4f}\")\n",
    "        print(f\"  • Output SSIM:            {np.mean([r['ssim_improvement'] for r in results]):.4f} ± {np.std([r['ssim_improvement'] for r in results]):.4f}\")\n",
    "        print(f\"  • SSIM Improvement:            {np.mean([r['ssim'] for r in results]):.4f} ± {np.std([r['ssim'] for r in results]):.4f}\")\n",
    "        print(f\"  • Input SAM:             {np.mean([r['input_sam'] for r in results]):.4f} ± {np.std([r['input_sam'] for r in results]):.4f}\")\n",
    "        print(f\"  • Output SAM:             {np.mean([r['sam'] for r in results]):.4f} ± {np.std([r['sam'] for r in results]):.4f}\")\n",
    "        print(f\"  • SAM Improvement:             {np.mean([r['sam_improvement'] for r in results]):.4f} ± {np.std([r['sam_improvement'] for r in results]):.4f}\")\n",
    "        \n",
    "        # Best and worst cases\n",
    "        sorted_by_psnr = sorted(results, key=lambda x: x['output_psnr'])\n",
    "        print(f\"\\nPerformance Range:\")\n",
    "        print(f\"  • Best PSNR:  {sorted_by_psnr[-1]['output_psnr']:.2f} dB ({sorted_by_psnr[-1]['filename']})\")\n",
    "        print(f\"  • Worst PSNR: {sorted_by_psnr[0]['output_psnr']:.2f} dB ({sorted_by_psnr[0]['filename']})\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    else:\n",
    "        print(\"\\nNo results generated!\")\n",
    "\n",
    "    return results\n",
    "    \n",
    "def create_comprehensive_visualizations(results, save_dir):\n",
    "    \"\"\"Create comprehensive test result visualizations\"\"\"\n",
    "    print(\"Creating comprehensive visualizations...\")\n",
    "\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "    noise_level = results[0]['noise_level']\n",
    "    input_psnrs = [r['input_psnr'] for r in results]\n",
    "    output_psnrs = [r['output_psnr'] for r in results]\n",
    "    psnr_improvements = [r['psnr_improvement'] for r in results]\n",
    "    ssims = [r['ssim'] for r in results]\n",
    "    sams = [r['sam'] for r in results]\n",
    "\n",
    "    # PSNR comparison\n",
    "    axes[0, 0].scatter(input_psnrs, output_psnrs, alpha=0.7, s=60, color='blue')\n",
    "    axes[0, 0].plot([min(input_psnrs), max(output_psnrs)], [min(input_psnrs), max(output_psnrs)], 'r--', alpha=0.5)\n",
    "    axes[0, 0].axhline(y=40, color='red', linestyle='-', alpha=0.7, label='Target (40 dB)')\n",
    "    axes[0, 0].set_xlabel('Input PSNR (dB)')\n",
    "    axes[0, 0].set_ylabel('Output PSNR (dB)')\n",
    "    axes[0, 0].set_title(f'PSNR: Input vs Output (σ={noise_level})')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # PSNR improvement distribution\n",
    "    axes[0, 1].hist(psnr_improvements, bins=15, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[0, 1].axvline(x=np.mean(psnr_improvements), color='red', linestyle='--',\n",
    "                      label=f'Mean: {np.mean(psnr_improvements):.2f} dB')\n",
    "    axes[0, 1].set_xlabel('PSNR Improvement (dB)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('PSNR Improvement Distribution')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    # SSIM distribution\n",
    "    axes[0, 2].hist(ssims, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[0, 2].axvline(x=np.mean(ssims), color='red', linestyle='--',\n",
    "                      label=f'Mean: {np.mean(ssims):.3f}')\n",
    "    axes[0, 2].set_xlabel('SSIM')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].set_title('SSIM Distribution')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].legend()\n",
    "\n",
    "    # SAM distribution\n",
    "    axes[1, 0].hist(sams, bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1, 0].axvline(x=np.mean(sams), color='red', linestyle='--',\n",
    "                      label=f'Mean: {np.mean(sams):.3f}')\n",
    "    axes[1, 0].set_xlabel('SAM (radians)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('SAM Distribution')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    # Performance vs Image Size\n",
    "    image_sizes = [r['shape'][1] * r['shape'][2] for r in results]\n",
    "    axes[1, 1].scatter(image_sizes, output_psnrs, alpha=0.7, s=60, color='purple')\n",
    "    axes[1, 1].set_xlabel('Image Size (pixels)')\n",
    "    axes[1, 1].set_ylabel('Output PSNR (dB)')\n",
    "    axes[1, 1].set_title('Performance vs Image Size')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].axhline(y=40, color='red', linestyle='--', alpha=0.7, label='Target (40 dB)')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "    # Summary statistics\n",
    "    target_achieved = sum(1 for p in output_psnrs if p > 40)\n",
    "    success_rate = target_achieved / len(output_psnrs) * 100\n",
    "    axes[1, 2].axis('off')\n",
    "    stats_text = f\"\"\"\n",
    "TEST RESULTS SUMMARY\n",
    "{'='*25}\n",
    "Static Noise Testing (σ={noise_level})\n",
    "Total Samples: {len(results)}\n",
    "Unique Images: {len(set([r['filename'] for r in results]))}\n",
    "\n",
    "PSNR STATISTICS:\n",
    "- Mean Output PSNR: {np.mean(output_psnrs):.2f} ± {np.std(output_psnrs):.2f} dB\n",
    "- Mean Input PSNR: {np.mean(input_psnrs):.2f} ± {np.std(input_psnrs):.2f} dB\n",
    "- Mean Improvement: {np.mean(psnr_improvements):.2f} ± {np.std(psnr_improvements):.2f} dB\n",
    "- Target Achievement: {target_achieved} / {len(output_psnrs)} ({success_rate:.1f}%)\n",
    "- Max PSNR: {max(output_psnrs):.2f} dB\n",
    "- Min PSNR: {min(output_psnrs):.2f} dB\n",
    "\n",
    "OTHER METRICS:\n",
    "- Mean SSIM: {np.mean(ssims):.4f} ± {np.std(ssims):.4f}\n",
    "- Mean SAM: {np.mean(sams):.4f} ± {np.std(sams):.4f}\n",
    "\n",
    "CONSISTENCY:\n",
    "- PSNR Std Dev: {np.std(output_psnrs):.2f} dB\n",
    "- Performance: {'Excellent' if success_rate > 80 else 'Good' if success_rate > 50 else 'Needs Improvement'}\n",
    "    \"\"\"\n",
    "\n",
    "    axes[1, 2].text(0.05, 0.95, stats_text, transform=axes[1, 2].transAxes,\n",
    "                    fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "    plt.suptitle(f'Comprehensive HSI Denoising Test Results (Static noise σ={noise_level})',\n",
    "                 fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.94)\n",
    "    plt.savefig(os.path.join(save_dir, 'comprehensive_test_results.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        'mean_psnr': np.mean(output_psnrs),\n",
    "        'std_psnr': np.std(output_psnrs),\n",
    "        'mean_improvement': np.mean(psnr_improvements),\n",
    "        'std_improvement': np.std(psnr_improvements),\n",
    "        'mean_ssim': np.mean(ssims),\n",
    "        'std_ssim': np.std(ssims),\n",
    "        'mean_sam': np.mean(sams),\n",
    "        'std_sam': np.std(sams),\n",
    "        'mean_loss': np.mean([r['loss'] for r in results]),\n",
    "        'std_loss': np.std([r['loss'] for r in results]),\n",
    "        'target_achievement_rate': success_rate / 100,\n",
    "        'max_psnr': max(output_psnrs),\n",
    "        'min_psnr': min(output_psnrs),\n",
    "        'noise_level': noise_level\n",
    "    }\n",
    "\n",
    "def create_sample_visualization(result, title, save_dir):\n",
    "    \"\"\"Create detailed visualization with dataset-specific RGB band selection\"\"\"\n",
    "    clean = result['clean']\n",
    "    noisy = result['noisy'] \n",
    "    denoised = result['denoised']\n",
    "    D, H, W = clean.shape\n",
    "\n",
    "    # Dataset-specific RGB band selection\n",
    "    filename_lower = result['filename'].lower()\n",
    "    \n",
    "    print(f\"Creating visualization for {result['filename']}\")\n",
    "    \n",
    "    # FIXED: Correct RGB band selections for common datasets\n",
    "    if 'indian' in filename_lower:\n",
    "        # Indian Pines (220 bands): Standard RGB approximation\n",
    "        if D >= 150:\n",
    "            rgb_bands = [49, 26, 16]  # Red: ~630nm, Green: ~550nm, Blue: ~470nm\n",
    "            print(f\"  Using Indian Pines standard RGB bands: {[b+1 for b in rgb_bands]}\")\n",
    "        elif D >= 100:\n",
    "            rgb_bands = [int(D*0.22), int(D*0.12), int(D*0.07)]\n",
    "            print(f\"  Using scaled Indian bands: {[b+1 for b in rgb_bands]}\")\n",
    "        else:\n",
    "            rgb_bands = [min(D-1, 22), min(D-1, 12), min(D-1, 7)]\n",
    "            print(f\"  Using minimal Indian bands: {[b+1 for b in rgb_bands]}\")\n",
    "    \n",
    "    elif 'pavia' in filename_lower:\n",
    "        # Pavia University/Centre (103 bands)\n",
    "        if D >= 80:\n",
    "            rgb_bands = [55, 41, 12]\n",
    "        else:\n",
    "            rgb_bands = [int(D*0.7), int(D*0.5), int(D*0.15)]\n",
    "        print(f\"  Using Pavia RGB bands: {[b+1 for b in rgb_bands]}\")\n",
    "    \n",
    "    elif 'washington' in filename_lower or 'dc' in filename_lower:\n",
    "        # Washington DC Mall (191 bands)\n",
    "        if D >= 150:\n",
    "            rgb_bands = [56, 26, 16]\n",
    "        else:\n",
    "            rgb_bands = [int(D*0.35), int(D*0.15), int(D*0.08)]\n",
    "        print(f\"  Using Washington RGB bands: {[b+1 for b in rgb_bands]}\")\n",
    "    \n",
    "    else:\n",
    "        # Generic approach: use spread across spectrum\n",
    "        rgb_bands = [\n",
    "            min(D-1, int(D*0.7)),   # Near-infrared/Red\n",
    "            min(D-1, int(D*0.4)),   # Green/Yellow\n",
    "            min(D-1, int(D*0.15))   # Blue\n",
    "        ]\n",
    "        print(f\"  Using generic RGB bands: {[b+1 for b in rgb_bands]}\")\n",
    "\n",
    "    # Ensure bands are valid\n",
    "    # rgb_bands = [min(max(0, b), D-1) for b in rgb_bands]\n",
    "    rgb_bands = [22,13,5]\n",
    "    \n",
    "    # Create RGB composites with enhanced contrast\n",
    "    rgb_images = {}\n",
    "    for data_type, data in [('clean', clean), ('noisy', noisy), ('denoised', denoised)]:\n",
    "        rgb_image = np.zeros((H, W, 3))\n",
    "        for i, band in enumerate(rgb_bands):\n",
    "            band_data = data[band]\n",
    "            \n",
    "            rgb_image[:, :, i] = np.clip(band_data, 0, 1)\n",
    "        \n",
    "        # Slight gamma correction for visual appeal\n",
    "        rgb_image = np.power(rgb_image, 0.95)\n",
    "        rgb_images[data_type] = rgb_image\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(19, 8))\n",
    "\n",
    "    axes[0].imshow(rgb_images['clean'], interpolation='nearest')\n",
    "    axes[0].set_title(f'Clean (Bands {rgb_bands[0]+1}, {rgb_bands[1]+1}, {rgb_bands[2]+1})')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(rgb_images['noisy'], interpolation='nearest')\n",
    "    axes[1].set_title(f'Noisy (Bands {rgb_bands[0]+1}, {rgb_bands[1]+1}, {rgb_bands[2]+1})')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(rgb_images['denoised'], interpolation='nearest')\n",
    "    axes[2].set_title(f'Denoised (Bands {rgb_bands[0]+1}, {rgb_bands[1]+1}, {rgb_bands[2]+1})')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    orig_min, orig_max = result['original_range']\n",
    "    fig.suptitle(f'{title}\\n'\n",
    "                f'File: {result[\"filename\"]}, Noise: σ={result[\"noise_level\"]:.2f}, '\n",
    "                f'Original Range: [{orig_min:.2f}, {orig_max:.2f}]\\n'\n",
    "                f'PSNR: {result[\"input_psnr\"]:.2f}→{result[\"output_psnr\"]:.2f} dB (+{result[\"psnr_improvement\"]:.2f}), '\n",
    "                f'SSIM: {result[\"ssim\"]:.3f}, SAM: {result[\"sam\"]:.3f}',\n",
    "                fontsize=14, y=0.98)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88)\n",
    "    safe_title = title.replace(' ', '_').lower()\n",
    "    safe_filename = result['filename'].replace('.mat', '').replace(' ', '_')\n",
    "    plt.savefig(os.path.join(save_dir, f'sample_{safe_title}_{safe_filename}.png'),\n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"  Saved visualization: sample_{safe_title}_{safe_filename}.png\")\n",
    "\n",
    "def create_detailed_analysis_plots(results, save_dir):\n",
    "    \"\"\"Create detailed analysis plots\"\"\"\n",
    "    print(\"Creating detailed analysis plots...\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    input_psnrs = [r['input_psnr'] for r in results]\n",
    "    output_psnrs = [r['output_psnr'] for r in results]\n",
    "    psnr_improvements = [r['psnr_improvement'] for r in results]\n",
    "    ssims = [r['ssim'] for r in results]\n",
    "    sams = [r['sam'] for r in results]\n",
    "    losses = [r['loss'] for r in results]\n",
    "    noise_level = results[0]['noise_level']\n",
    "    image_sizes = [r['shape'][1] * r['shape'][2] for r in results]\n",
    "\n",
    "    # Correlation matrix\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        metrics_data = {\n",
    "            'Input_PSNR': input_psnrs,\n",
    "            'Output_PSNR': output_psnrs,\n",
    "            'PSNR_Improvement': psnr_improvements,\n",
    "            'SSIM': ssims,\n",
    "            'SAM': sams,\n",
    "            'Loss': losses,\n",
    "            'Image_Size': image_sizes\n",
    "        }\n",
    "        df = pd.DataFrame(metrics_data)\n",
    "        correlation_matrix = df.corr()\n",
    "\n",
    "        im = axes[0, 0].imshow(correlation_matrix.values, cmap='RdYlBu', vmin=-1, vmax=1)\n",
    "        axes[0, 0].set_xticks(range(len(correlation_matrix.columns)))\n",
    "        axes[0, 0].set_yticks(range(len(correlation_matrix.columns)))\n",
    "        axes[0, 0].set_xticklabels(correlation_matrix.columns, rotation=45, ha='right')\n",
    "        axes[0, 0].set_yticklabels(correlation_matrix.columns)\n",
    "        axes[0, 0].set_title('Metrics Correlation Matrix')\n",
    "\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(len(correlation_matrix.columns)):\n",
    "                axes[0, 0].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "        plt.colorbar(im, ax=axes[0, 0], shrink=0.8)\n",
    "    except ImportError:\n",
    "        axes[0, 0].text(0.5, 0.5, 'pandas not available\\nfor correlation matrix',\n",
    "                       ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "\n",
    "    # PSNR improvement distribution\n",
    "    axes[0, 1].hist(psnr_improvements, bins=15, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[0, 1].axvline(x=np.mean(psnr_improvements), color='red', linestyle='--', linewidth=2,\n",
    "                      label=f'Mean: {np.mean(psnr_improvements):.2f} dB')\n",
    "    axes[0, 1].axvline(x=np.median(psnr_improvements), color='blue', linestyle='--', linewidth=2,\n",
    "                      label=f'Median: {np.median(psnr_improvements):.2f} dB')\n",
    "    axes[0, 1].set_xlabel('PSNR Improvement (dB)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title(f'PSNR Improvement Distribution (σ={noise_level})')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Performance vs Image Size\n",
    "    scatter = axes[1, 0].scatter(image_sizes, output_psnrs, alpha=0.7, s=60,\n",
    "                                c=psnr_improvements, cmap='RdYlGn')\n",
    "    axes[1, 0].set_xlabel('Image Size (pixels)')\n",
    "    axes[1, 0].set_ylabel('Output PSNR (dB)')\n",
    "    axes[1, 0].set_title('PSNR vs Image Size (colored by improvement)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].axhline(y=40, color='red', linestyle='--', alpha=0.7, label='Target (40 dB)')\n",
    "    axes[1, 0].legend()\n",
    "    plt.colorbar(scatter, ax=axes[1, 0], label='PSNR Improvement (dB)', shrink=0.8)\n",
    "\n",
    "    # Performance summary\n",
    "    target_achieved = sum(1 for p in output_psnrs if p > 40)\n",
    "    success_rate = target_achieved / len(output_psnrs) * 100\n",
    "    consistency_metric = np.std(psnr_improvements) / np.mean(psnr_improvements) if np.mean(psnr_improvements) > 0 else 0\n",
    "    stats_text = f\"\"\"\n",
    "DETAILED PERFORMANCE ANALYSIS\n",
    "{'='*35}\n",
    "Static Noise Testing (σ={noise_level})\n",
    "\n",
    "PSNR PERFORMANCE:\n",
    "- Mean Output PSNR: {np.mean(output_psnrs):.2f} ± {np.std(output_psnrs):.2f} dB\n",
    "- Mean Input PSNR: {np.mean(input_psnrs):.2f} ± {np.std(input_psnrs):.2f} dB\n",
    "- Mean Improvement: {np.mean(psnr_improvements):.2f} ± {np.std(psnr_improvements):.2f} dB\n",
    "- Max Improvement: {max(psnr_improvements):.2f} dB\n",
    "- Min Improvement: {min(psnr_improvements):.2f} dB\n",
    "\n",
    "TARGET ACHIEVEMENT:\n",
    "- Success Rate: {success_rate:.1f}% ({target_achieved}/{len(results)})\n",
    "- Above 40 dB: {target_achieved} samples\n",
    "- Above 35 dB: {sum(1 for p in output_psnrs if p > 35)} samples\n",
    "- Above 30 dB: {sum(1 for p in output_psnrs if p > 30)} samples\n",
    "\n",
    "CONSISTENCY ANALYSIS:\n",
    "- Coefficient of Variation: {consistency_metric:.3f}\n",
    "- Performance Stability: {'High' if consistency_metric < 0.2 else 'Moderate' if consistency_metric < 0.5 else 'Low'}\n",
    "\n",
    "QUALITY METRICS:\n",
    "- SSIM: {np.mean(ssims):.4f} ± {np.std(ssims):.4f}\n",
    "- SAM: {np.mean(sams):.4f} ± {np.std(sams):.4f}\n",
    "\n",
    "IMAGE SIZE ANALYSIS:\n",
    "- Min Size: {min(image_sizes)} pixels\n",
    "- Max Size: {max(image_sizes)} pixels\n",
    "- Mean Size: {np.mean(image_sizes):.0f} pixels\n",
    "    \"\"\"\n",
    "\n",
    "    axes[1, 1].axis('off')\n",
    "    axes[1, 1].text(0.05, 0.95, stats_text, transform=axes[1, 1].transAxes,\n",
    "                    fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "    plt.suptitle(f'Detailed Analysis - Static Noise Level (σ={noise_level})',\n",
    "                 fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.94)\n",
    "    plt.savefig(os.path.join(save_dir, 'detailed_analysis_plots.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        'mean_psnr': np.mean(output_psnrs),\n",
    "        'std_psnr': np.std(output_psnrs),\n",
    "        'mean_improvement': np.mean(psnr_improvements),\n",
    "        'std_improvement': np.std(psnr_improvements),\n",
    "        'mean_ssim': np.mean(ssims),\n",
    "        'std_ssim': np.std(ssims),\n",
    "        'mean_sam': np.mean(sams),\n",
    "        'std_sam': np.std(sams),\n",
    "        'mean_loss': np.mean([r['loss'] for r in results]),\n",
    "        'std_loss': np.std([r['loss'] for r in results]),\n",
    "        'target_achievement_rate': success_rate / 100,\n",
    "        'max_psnr': max(output_psnrs),\n",
    "        'min_psnr': min(output_psnrs),\n",
    "        'noise_level': noise_level\n",
    "    }\n",
    "\n",
    "def create_spectral_analysis(results, save_dir, num_samples=3):\n",
    "    \"\"\"Create spectral signature analysis\"\"\"\n",
    "    print(\"Creating spectral analysis...\")\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x['output_psnr'])\n",
    "    sample_indices = [0, len(sorted_results)//2, len(sorted_results)-1]\n",
    "    selected_results = [sorted_results[i] for i in sample_indices]\n",
    "\n",
    "    fig, axes = plt.subplots(len(selected_results), 1, figsize=(15, 4*len(selected_results)))\n",
    "    if len(selected_results) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, result in enumerate(selected_results):\n",
    "        clean = result['clean']\n",
    "        noisy = result['noisy']\n",
    "        denoised = result['denoised']\n",
    "\n",
    "        D, H, W = clean.shape\n",
    "        center_h, center_w = H//2, W//2\n",
    "\n",
    "        clean_spectrum = clean[:, center_h, center_w]\n",
    "        noisy_spectrum = noisy[:, center_h, center_w]\n",
    "        denoised_spectrum = denoised[:, center_h, center_w]\n",
    "\n",
    "        bands = np.arange(D)\n",
    "\n",
    "        axes[idx].plot(bands, clean_spectrum, 'g-', linewidth=2, label='Clean', alpha=0.8)\n",
    "        axes[idx].plot(bands, noisy_spectrum, 'r--', linewidth=1.5, label='Noisy', alpha=0.7)\n",
    "        axes[idx].plot(bands, denoised_spectrum, 'b-', linewidth=2, label='Denoised', alpha=0.8)\n",
    "\n",
    "        axes[idx].set_xlabel('Spectral Band')\n",
    "        axes[idx].set_ylabel('Reflectance')\n",
    "        axes[idx].set_title(f'Spectral Signature - {result[\"filename\"]}\\n'\n",
    "                           f'PSNR: {result[\"output_psnr\"]:.2f} dB, SSIM: {result[\"ssim\"]:.3f}, '\n",
    "                           f'SAM: {result[\"sam\"]:.4f}')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "        ax2 = axes[idx].twinx()\n",
    "        denoising_improvement = np.abs(denoised_spectrum - clean_spectrum) - np.abs(noisy_spectrum - clean_spectrum)\n",
    "        ax2.fill_between(bands, 0, denoising_improvement, alpha=0.3, color='purple', label='Denoising Effect')\n",
    "        ax2.set_ylabel('Denoising Effect', color='purple')\n",
    "        ax2.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'spectral_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_detailed_results(results, summary_stats, save_dir, model_info=None):\n",
    "    \"\"\"Save detailed results as visualizations\"\"\"\n",
    "    print(\"Saving visualization results...\")\n",
    "\n",
    "    print(f\"Results saved to {save_dir}\")\n",
    "    print(f\"• Comprehensive plots: comprehensive_test_results.png\")\n",
    "    print(f\"• Detailed analysis: detailed_analysis_plots.png\")\n",
    "    print(f\"• Spectral analysis: spectral_analysis.png\")\n",
    "    print(f\"• Sample visualizations: sample_*.png\")\n",
    "    print(f\"• Testing Mode: Static noise (σ={summary_stats['noise_level']})\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main testing function with ICVL protocol\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"HSI DENOISING MODEL TESTING - ICVL Protocol\")\n",
    "    print(\"Test crops: 512×512×31 with training-matched normalization\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Configuration\n",
    "    TEST_DIR = '/workspace/icvl_part/test_gauss'\n",
    "    MODEL_PATH = './HSI_denoising_ICVL_resultsV15_noise30'\n",
    "    RESULTS_DIR = './HSI_denoising_ICVL_resultsV15_noise30/test_results'\n",
    "    TRAINING_NOISE_LEVEL = 10\n",
    "    \n",
    "    config = {\n",
    "        'base_dim': 64,\n",
    "        'target_bands': 31,  # ICVL standard\n",
    "        'patch_size': 64,    # For patch-based processing if needed\n",
    "        'test_crop_size': 512,  # ICVL test protocol\n",
    "        'noise_level': TRAINING_NOISE_LEVEL,\n",
    "    }\n",
    "\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    # Device setup\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Load model\n",
    "    print(\"\\nLoading trained model...\")\n",
    "    full_model_path = os.path.join(MODEL_PATH, 'enhanced_denoising_pipeline_full.pth')\n",
    "    best_model_path = os.path.join(MODEL_PATH, 'best_memory_optimized_model.pth')\n",
    "\n",
    "    model_info = None\n",
    "    if os.path.exists(full_model_path):\n",
    "        print(\"Loading full model with metadata...\")\n",
    "        checkpoint = torch.load(full_model_path, map_location=device, weights_only=False)\n",
    "        model_info = checkpoint\n",
    "        config = checkpoint.get('config', config)\n",
    "    elif os.path.exists(best_model_path):\n",
    "        print(\"Loading best model checkpoint...\")\n",
    "        checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
    "        config = checkpoint.get('config', config)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No model found in {MODEL_PATH}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = MemoryOptimizedUNet(\n",
    "        in_channels=1,\n",
    "        base_dim=config['base_dim'],\n",
    "        window_sizes=[4, 8, 16],\n",
    "        num_bands=config['target_bands']\n",
    "    ).to(device)\n",
    "\n",
    "    # Escape strict model parameter matching here\n",
    "    #model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    #model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "    # ===================================================================\n",
    "    # FIXED: Handle spectral_pos_embed size mismatches\n",
    "    # ===================================================================\n",
    "    # Get the saved state dict and current model's state dict\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model_dict = model.state_dict()\n",
    "    \n",
    "    # Prepare filtered state dict\n",
    "    filtered_state_dict = {}\n",
    "    adjusted_keys = []\n",
    "    skipped_keys = []\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        if k in model_dict:\n",
    "            if v.shape == model_dict[k].shape:\n",
    "                # Shapes match - load directly\n",
    "                filtered_state_dict[k] = v\n",
    "            else:\n",
    "                # Size mismatch detected\n",
    "                if 'spectral_pos_embed' in k:\n",
    "                    # Handle spectral positional embedding mismatches\n",
    "                    needed_bands = model_dict[k].shape[1]\n",
    "                    available_bands = v.shape[1]\n",
    "                    \n",
    "                    if needed_bands <= available_bands:\n",
    "                        # Checkpoint has more bands than we need - slice it\n",
    "                        filtered_state_dict[k] = v[:, :needed_bands, :].clone()\n",
    "                        adjusted_keys.append(f\"{k}: {v.shape} -> {filtered_state_dict[k].shape}\")\n",
    "                    else:\n",
    "                        # Checkpoint has fewer bands than we need - pad it\n",
    "                        padded = torch.zeros_like(model_dict[k])\n",
    "                        padded[:, :available_bands, :] = v\n",
    "                        filtered_state_dict[k] = padded\n",
    "                        adjusted_keys.append(f\"{k}: {v.shape} -> {filtered_state_dict[k].shape} (padded)\")\n",
    "                else:\n",
    "                    # Non-spectral parameter with mismatch - skip it\n",
    "                    skipped_keys.append(f\"{k}: checkpoint {v.shape} vs model {model_dict[k].shape}\")\n",
    "        else:\n",
    "            # Key not in model - likely from different architecture\n",
    "            skipped_keys.append(f\"{k}: not found in current model\")\n",
    "    \n",
    "    # Report what was adjusted\n",
    "    if adjusted_keys:\n",
    "        print(f\"\\n✓ Adjusted {len(adjusted_keys)} spectral_pos_embed parameters:\")\n",
    "        for key in adjusted_keys[:5]:  # Show first 5\n",
    "            print(f\"    {key}\")\n",
    "        if len(adjusted_keys) > 5:\n",
    "            print(f\"    ... and {len(adjusted_keys) - 5} more\")\n",
    "    \n",
    "    if skipped_keys:\n",
    "        print(f\"\\n⚠ Skipped {len(skipped_keys)} mismatched parameters:\")\n",
    "        for key in skipped_keys[:3]:  # Show first 3\n",
    "            print(f\"    {key}\")\n",
    "        if len(skipped_keys) > 3:\n",
    "            print(f\"    ... and {len(skipped_keys) - 3} more\")\n",
    "    \n",
    "    # Load the filtered state dict (strict=False allows missing keys)\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=True)\n",
    "    \n",
    "    if missing_keys:\n",
    "        print(f\"\\n⚠ Missing keys (will use random initialization): {len(missing_keys)}\")\n",
    "        if len(missing_keys) <= 5:\n",
    "            for key in missing_keys:\n",
    "                print(f\"    {key}\")\n",
    "    \n",
    "    if unexpected_keys:\n",
    "        print(f\"\\n⚠ Unexpected keys (ignored): {len(unexpected_keys)}\")\n",
    "    \n",
    "    print(\"\\n✓ Model weights loaded successfully!\")\n",
    "    # ===================================================================\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"Parameters: {total_params / 1e6:.2f}M\")\n",
    "    print(f\"Base dimension: {config['base_dim']}\")\n",
    "    print(f\"Target bands: {config['target_bands']}\")\n",
    "    print(f\"Training noise level: σ={config['noise_level']}\")\n",
    "\n",
    "    if model_info and 'best_psnr' in model_info:\n",
    "        print(f\"Training best PSNR: {model_info['best_psnr']:.2f} dB\")\n",
    "\n",
    "    # Load test data\n",
    "    test_data = load_test_data(\n",
    "        TEST_DIR, \n",
    "        target_bands=config['target_bands'],\n",
    "        test_crop_size=config.get('test_crop_size', 512)\n",
    "    )\n",
    "\n",
    "    if not test_data:\n",
    "        print(\"ERROR: No test data loaded!\")\n",
    "        return\n",
    "        \n",
    "\n",
    "    print(f\"Loaded {len(test_data)} test images (512×512×31)\")\n",
    "\n",
    "    # Add static noise\n",
    "    test_samples = add_static_noise_to_data(test_data, noise_level=config['noise_level'])\n",
    "\n",
    "    print(\"\\nNoise Analysis:\")\n",
    "    noise_types = {}\n",
    "    actual_noise_levels = []\n",
    "    for sample in test_samples:\n",
    "        noise_type = sample.get('noise_type', 'unknown')\n",
    "        noise_types[noise_type] = noise_types.get(noise_type, 0) + 1\n",
    "        actual_noise_levels.append(sample.get('actual_noise_level', 0))\n",
    "    \n",
    "    print(f\"Noise types distribution: {noise_types}\")\n",
    "    print(f\"Actual noise levels - Mean: {np.mean(actual_noise_levels):.4f}, \"\n",
    "          f\"Std: {np.std(actual_noise_levels):.4f}, \"\n",
    "          f\"Range: [{np.min(actual_noise_levels):.4f}, {np.max(actual_noise_levels):.4f}]\")\n",
    "    \n",
    "    # Verify noise is visible\n",
    "    sample_noise = test_samples[0]['noisy'] - test_samples[0]['clean']\n",
    "    print(f\"Sample noise verification - Mean: {np.mean(sample_noise):.4f}, \"\n",
    "          f\"Std: {np.std(sample_noise):.4f}, \"\n",
    "          f\"Max: {np.max(np.abs(sample_noise)):.4f}\")\n",
    "\n",
    "    print(f\"Created {len(test_samples)} test samples with static noise σ={config['noise_level']}\")\n",
    "\n",
    "    # Test model\n",
    "    results = test_model_comprehensive(model, test_samples, device, patch_size=config['patch_size'])\n",
    "\n",
    "    if not results:\n",
    "        print(\"No test results generated!\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nSuccessfully tested {len(results)} samples\")\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Number of test samples: {len(results)}\")\n",
    "    print(f\"Average Test Loss: {np.mean([r['loss'] for r in results]):.6f} ± {np.std([r['loss'] for r in results]):.6f}\")\n",
    "    print(f\"Average PSNR:      {np.mean([r['output_psnr'] for r in results]):.3f} ± {np.std([r['output_psnr'] for r in results]):.3f} dB\")\n",
    "    print(f\"Average SSIM:      {np.mean([r['ssim'] for r in results]):.4f} ± {np.std([r['ssim'] for r in results]):.4f}\")\n",
    "    print(f\"Average SAM:       {np.mean([r['sam'] for r in results]):.4f} ± {np.std([r['sam'] for r in results]):.4f}\")\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x['output_psnr'])\n",
    "    best_result = sorted_results[-1]\n",
    "    worst_result = sorted_results[0]\n",
    "    print(f\"Best PSNR:  {best_result['filename']} ({best_result['output_psnr']:.3f} dB)\")\n",
    "    print(f\"Worst PSNR: {worst_result['filename']} ({worst_result['output_psnr']:.3f} dB)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Visualizations\n",
    "    summary_stats = create_comprehensive_visualizations(results, RESULTS_DIR)\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x['output_psnr'])\n",
    "    best_result = sorted_results[-1]\n",
    "    worst_result = sorted_results[0]\n",
    "    median_result = sorted_results[len(sorted_results)//2]\n",
    "\n",
    "    for title, result in [('Best Performance', best_result),\n",
    "                         ('Median Performance', median_result),\n",
    "                         ('Challenging Case', worst_result)]:\n",
    "        create_sample_visualization(result, title, RESULTS_DIR)\n",
    "\n",
    "    create_spectral_analysis(results, RESULTS_DIR)\n",
    "    save_detailed_results(results, summary_stats, RESULTS_DIR, model_info)\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE TEST RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(f\"Test Configuration:\")\n",
    "    print(f\"  • Test Directory: {TEST_DIR}\")\n",
    "    print(f\"  • Model Path: {MODEL_PATH}\")\n",
    "    print(f\"  • Total Samples: {len(results)}\")\n",
    "    print(f\"  • Unique Images: {len(set([r['filename'] for r in results]))}\")\n",
    "    print(f\"  • Noise Level: σ={summary_stats['noise_level']}\")\n",
    "    print(f\"  • Device: {device}\")\n",
    "\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  • Mean PSNR: {summary_stats['mean_psnr']:.2f} ± {summary_stats['std_psnr']:.2f} dB\")\n",
    "    print(f\"  • Mean Improvement: {summary_stats['mean_improvement']:.2f} dB\")\n",
    "    print(f\"  • Max PSNR: {summary_stats['max_psnr']:.2f} dB\")\n",
    "    print(f\"  • Min PSNR: {summary_stats['min_psnr']:.2f} dB\")\n",
    "    print(f\"  • Mean SSIM: {summary_stats['mean_ssim']:.4f} ± {summary_stats['std_ssim']:.4f}\")\n",
    "    print(f\"  • Mean SAM: {summary_stats['mean_sam']:.4f} ± {summary_stats['std_sam']:.4f}\")\n",
    "\n",
    "    print(f\"\\nTarget Achievement (PSNR > 40 dB):\")\n",
    "    achievement_rate = summary_stats['target_achievement_rate'] * 100\n",
    "    target_count = int(achievement_rate * len(results) / 100)\n",
    "    print(f\"  • Success Rate: {achievement_rate:.1f}% ({target_count} / {len(results)} samples)\")\n",
    "    print(f\"  • Status: {'EXCELLENT' if achievement_rate > 80 else 'GOOD' if achievement_rate > 50 else 'NEEDS IMPROVEMENT'}\")\n",
    "\n",
    "    print(f\"\\nModel Architecture:\")\n",
    "    print(f\"  • Parameters: {total_params / 1e6:.2f}M\")\n",
    "    print(f\"  • Base Dimension: {config['base_dim']}\")\n",
    "    print(f\"  • Input Bands: {config['target_bands']}\")\n",
    "    print(f\"  • Patch Size: {config['patch_size']}\")\n",
    "\n",
    "    if model_info and 'best_psnr' in model_info:\n",
    "        training_psnr = model_info['best_psnr']\n",
    "        test_psnr = summary_stats['mean_psnr']\n",
    "        generalization = \"Good\" if abs(training_psnr - test_psnr) < 5 else \"Moderate\" if abs(training_psnr - test_psnr) < 10 else \"Poor\"\n",
    "        print(f\"\\nGeneralization Analysis:\")\n",
    "        print(f\"  • Training PSNR: {training_psnr:.2f} dB\")\n",
    "        print(f\"  • Test PSNR: {test_psnr:.2f} dB\")\n",
    "        print(f\"  • Difference: {abs(training_psnr - test_psnr):.2f} dB\")\n",
    "        print(f\"  • Generalization: {generalization}\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"Final Average PSNR: {summary_stats['mean_psnr']:.3f} ± {summary_stats['std_psnr']:.3f} dB\")\n",
    "    print(f\"Final Average SSIM: {summary_stats['mean_ssim']:.4f} ± {summary_stats['std_ssim']:.4f}\")\n",
    "    print(f\"Final Average SAM:  {summary_stats['mean_sam']:.4f} ± {summary_stats['std_sam']:.4f}\")\n",
    "    print(f\"\\nResults saved to: {RESULTS_DIR}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"TESTING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return results, summary_stats, model_info\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145aaa86-9893-474b-b4dd-dab9dedc9d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
